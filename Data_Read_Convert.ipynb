{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Analysis - PCA and KMeans Unsupervised Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "#from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '10g') \\\n",
    "        .config('spark.executor.cores', '10') \\\n",
    "        .config('spark.executor.instances', '1') \\\n",
    "        .config(\"spark.driver.memory\",'2g') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# import data manipulation methods\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#from pyspark.ml.clustering import KMeans\n",
    "#from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the file as an RDD. Since its CSV, the split is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = sc.textFile('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv')\n",
    "rdd = all_data.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For development purposes, take a subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd = sc.parallelize(all_data.take(1000))\n",
    "\n",
    "#rdd = all_data\n",
    "\n",
    "#all_data.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's an RDD, remove the header row. Convert to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = rdd.take(1)[0]\n",
    "rdd = rdd.filter(lambda x: x != header)\n",
    "final_DF = rdd.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract out just the trip id and start timestamp. Using pyspark sql functions, convert to a proper timestamp data type. Repeat for end timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53304688/spark-date-format-mmm-dd-yyyy-hhmmss-am-to-timestamp-in-df\n",
    "#https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "start_times = rdd.map(lambda x: (x[0],x[1]))\n",
    "start_times = start_times.toDF()\n",
    "st = start_times.withColumn(\"Trip_Start_Timestamp\",F.to_timestamp(F.col(\"_2\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  _1|                  _2|Trip_Start_Timestamp|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...| 2019-12-01 00:15:00|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...| 2019-12-01 00:15:00|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...| 2019-12-01 00:15:00|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...| 2019-12-01 00:15:00|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...| 2019-12-01 00:15:00|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "st.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_times = rdd.map(lambda x: (x[0],x[2]))\n",
    "end_times = end_times.toDF()\n",
    "et = end_times.withColumn(\"Trip_End_Timestamp\",F.to_timestamp(F.col(\"_2\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data back together with our timestamp fields. Cast all our fields to their proper data types and rename. The original timestamp fields we keep with suffix \"_str\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.join(st.select(\"_1\",\"Trip_Start_Timestamp\"),on=\"_1\").join(et.select(\"_1\",\"Trip_End_Timestamp\"),on=\"_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----+----+-----------+-----------+---+---+----+---+----+-----+-----+---+-------------+--------------+--------------------+-------------+--------------+--------------------+--------------------+-------------------+\n",
      "|                  _1|                  _2|                  _3|  _4|  _5|         _6|         _7| _8| _9| _10|_11| _12|  _13|  _14|_15|          _16|           _17|                 _18|          _19|           _20|                 _21|Trip_Start_Timestamp| Trip_End_Timestamp|\n",
      "+--------------------+--------------------+--------------------+----+----+-----------+-----------+---+---+----+---+----+-----+-----+---+-------------+--------------+--------------------+-------------+--------------+--------------------+--------------------+-------------------+\n",
      "|00002a2b4769d1834...|03/06/2020 12:45:...|03/06/2020 01:45:...|3117|22.9|           |           |   | 43|42.5|  0|3.54|46.04|false|  1|             |              |                    |41.7615779081|-87.5727819867|POINT (-87.572781...| 2020-03-06 12:45:00|2020-03-06 13:45:00|\n",
      "|00002dfb5764b08b9...|01/19/2020 02:45:...|01/19/2020 02:45:...| 516| 2.5|17031280100|17031081100| 28|  8|   5|  0|3.08| 8.08|false|  1|41.8853000224|-87.6428084655|POINT (-87.642808...|41.9002212967|-87.6291051864|POINT (-87.629105...| 2020-01-19 02:45:00|2020-01-19 02:45:00|\n",
      "|00003e35194c765e1...|07/09/2020 11:15:...|07/09/2020 11:30:...| 647| 3.5|           |           | 67| 38| 7.5|  0|3.08|10.58|false|  1|41.7759288274|-87.6665962653|POINT (-87.666596...|41.8129489392|-87.6178596758|POINT (-87.617859...| 2020-07-09 11:15:00|2020-07-09 11:30:00|\n",
      "|000050e521ab946cf...|02/14/2020 06:00:...|02/14/2020 06:15:...|1236| 2.8|17031070103|17031081700|  7|  8|  10|  0|4.83|14.83|false|  1|41.9268111821|-87.6426052473|POINT (-87.642605...|41.8920421365|-87.6318639497|POINT (-87.631863...| 2020-02-14 18:00:00|2020-02-14 18:15:00|\n",
      "|0000728a2193f648f...|12/08/2019 09:00:...|12/08/2019 09:30:...| 954| 4.7|           |           | 19| 23|  10|  0|2.55|12.55|false|  1|41.9272609555|-87.7655016086|POINT (-87.765501...|41.9000696026|-87.7209182385|POINT (-87.720918...| 2019-12-08 21:00:00|2019-12-08 21:30:00|\n",
      "+--------------------+--------------------+--------------------+----+----+-----------+-----------+---+---+----+---+----+-----+-----+---+-------------+--------------+--------------------+-------------+--------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.withColumn('Trip_Seconds',F.col('_4').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Trip_Miles',F.col('_5').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Pickup_Community_Area',F.col('_8').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Dropoff_Community_Area',F.col('_9').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Fare',F.col('_10').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Tip',F.col('_11').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Additional_Charges',F.col('_12').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Trip_Total',F.col('_13').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Shared_Trip_Authorized',F.col('_14').cast(\"boolean\"))\n",
    "final_DF = final_DF.withColumn('Trips_Pooled',F.col('_15').cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.withColumnRenamed(\"_1\",\"trip_id\") \\\n",
    "                    .withColumnRenamed(\"_2\",\"Trip_Start_Timestamp_str\") \\\n",
    "                    .withColumnRenamed(\"_3\",\"Trip_End_Timestamp_str\") \\\n",
    "                    .withColumnRenamed(\"_4\",\"Trip_Seconds_str\") \\\n",
    "                    .withColumnRenamed(\"_5\",\"Trip_Miles_str\") \\\n",
    "                    .withColumnRenamed(\"_6\",\"Pickup_Census_Tract\") \\\n",
    "                    .withColumnRenamed(\"_7\",\"Dropoff_Census_Tract\") \\\n",
    "                    .withColumnRenamed(\"_8\",\"Pickup_Community_Area_str\") \\\n",
    "                    .withColumnRenamed(\"_9\",\"Dropoff_Community_Area_str\") \\\n",
    "                    .withColumnRenamed(\"_10\",\"Fare_str\") \\\n",
    "                    .withColumnRenamed(\"_11\",\"Tip_str\") \\\n",
    "                    .withColumnRenamed(\"_12\",\"Additional_Charges_str\") \\\n",
    "                    .withColumnRenamed(\"_13\",\"Trip_Total_str\") \\\n",
    "                    .withColumnRenamed(\"_14\",\"Shared_Trip_Authorized_str\") \\\n",
    "                    .withColumnRenamed(\"_15\",\"Trips_Pooled_str\") \\\n",
    "                    .withColumnRenamed(\"_16\",\"Pickup_Centroid_Latitude\") \\\n",
    "                    .withColumnRenamed(\"_17\",\"Pickup_Centroid_Longitude\") \\\n",
    "                    .withColumnRenamed(\"_18\",\"Pickup_Centroid_Location\") \\\n",
    "                    .withColumnRenamed(\"_19\",\"Dropoff_Centroid_Latitude\") \\\n",
    "                    .withColumnRenamed(\"_20\",\"Dropoff_Centroid_Longitude\") \\\n",
    "                    .withColumnRenamed(\"_21\",\"Dropoff_Centroid_Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp_str: string (nullable = true)\n",
      " |-- Trip_End_Timestamp_str: string (nullable = true)\n",
      " |-- Trip_Seconds_str: string (nullable = true)\n",
      " |-- Trip_Miles_str: string (nullable = true)\n",
      " |-- Pickup_Census_Tract: string (nullable = true)\n",
      " |-- Dropoff_Census_Tract: string (nullable = true)\n",
      " |-- Pickup_Community_Area_str: string (nullable = true)\n",
      " |-- Dropoff_Community_Area_str: string (nullable = true)\n",
      " |-- Fare_str: string (nullable = true)\n",
      " |-- Tip_str: string (nullable = true)\n",
      " |-- Additional_Charges_str: string (nullable = true)\n",
      " |-- Trip_Total_str: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized_str: string (nullable = true)\n",
      " |-- Trips_Pooled_str: string (nullable = true)\n",
      " |-- Pickup_Centroid_Latitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Longitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Location: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Latitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Longitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Location: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trip_id: string, Trip_Start_Timestamp_str: string, Trip_End_Timestamp_str: string, Trip_Seconds_str: string, Trip_Miles_str: string, Pickup_Census_Tract: string, Dropoff_Census_Tract: string, Pickup_Community_Area_str: string, Dropoff_Community_Area_str: string, Fare_str: string, Tip_str: string, Additional_Charges_str: string, Trip_Total_str: string, Shared_Trip_Authorized_str: string, Trips_Pooled_str: string, Pickup_Centroid_Latitude: string, Pickup_Centroid_Longitude: string, Pickup_Centroid_Location: string, Dropoff_Centroid_Latitude: string, Dropoff_Centroid_Longitude: string, Dropoff_Centroid_Location: string, Trip_Start_Timestamp: timestamp, Trip_End_Timestamp: timestamp, Trip_Seconds: int, Trip_Miles: double, Pickup_Community_Area: int, Dropoff_Community_Area: int, Fare: double, Tip: double, Additional_Charges: double, Trip_Total: double, Shared_Trip_Authorized: boolean, Trips_Pooled: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_DF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in columns with various date and time values from our trip start and end. Arbitarily choose start time to pull year, month, week number, day of week, and date from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\n",
    "final_DF = final_DF.withColumn(\"Trip_Year\", F.year(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_Month\", F.month(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_WeekNumber\", F.weekofyear(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_DayofWeek\", F.dayofweek(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_Start_Hour\", F.hour(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_End_Hour\", F.hour(F.col(\"Trip_End_Timestamp\"))) \\\n",
    "                    .withColumn(\"Date\", F.to_date(F.col(\"Trip_Start_Timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp_str: string (nullable = true)\n",
      " |-- Trip_End_Timestamp_str: string (nullable = true)\n",
      " |-- Trip_Seconds_str: string (nullable = true)\n",
      " |-- Trip_Miles_str: string (nullable = true)\n",
      " |-- Pickup_Census_Tract: string (nullable = true)\n",
      " |-- Dropoff_Census_Tract: string (nullable = true)\n",
      " |-- Pickup_Community_Area_str: string (nullable = true)\n",
      " |-- Dropoff_Community_Area_str: string (nullable = true)\n",
      " |-- Fare_str: string (nullable = true)\n",
      " |-- Tip_str: string (nullable = true)\n",
      " |-- Additional_Charges_str: string (nullable = true)\n",
      " |-- Trip_Total_str: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized_str: string (nullable = true)\n",
      " |-- Trips_Pooled_str: string (nullable = true)\n",
      " |-- Pickup_Centroid_Latitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Longitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Location: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Latitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Longitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Location: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_End_Hour: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_DF.count()\n",
    "\n",
    "# result 49,108,003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable'\n",
    "\n",
    "df = final_DF.drop('Trip_Start_Timestamp_str',\n",
    "                   'Trip_End_Timestamp_str',\n",
    "                   'Trip_Seconds_str',\n",
    "                   'Trip_Miles_str',\n",
    "                   'Pickup_Community_Area_str',\n",
    "                   'Dropoff_Community_Area_str',\n",
    "                   'Fare_str',\n",
    "                   'Tip_str',\n",
    "                   'Additional_Charges_str',\n",
    "                   'Trip_Total_str',\n",
    "                   'Shared_Trip_Authorized_str',\n",
    "                   'Trips_Pooled_str', \n",
    "                   'Pickup_Census_Tract',\n",
    "                   'Dropoff_Census_Tract',\n",
    "                   'Pickup_Centroid_Latitude',\n",
    "                   'Pickup_Centroid_Longitude', \n",
    "                   'Pickup_Centroid_Location', \n",
    "                   'Dropoff_Centroid_Latitude', \n",
    "                   'Dropoff_Centroid_Longitude', \n",
    "                   'Dropoff_Centroid_Location')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_End_Hour: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                            Type            Data/Info\n",
      "-------------------------------------------------------------\n",
      "ArrayType                           type            <class 'pyspark.sql.types.ArrayType'>\n",
      "Binarizer                           type            <class 'pyspark.ml.feature.Binarizer'>\n",
      "BinaryClassificationEvaluator       type            <class 'pyspark.ml.evalua<...>ClassificationEvaluator'>\n",
      "BinaryClassificationMetrics         type            <class 'pyspark.mllib.eva<...>ryClassificationMetrics'>\n",
      "BinaryType                          type            <class 'pyspark.sql.types.BinaryType'>\n",
      "BooleanType                         type            <class 'pyspark.sql.types.BooleanType'>\n",
      "BucketedRandomProjectionLSH         type            <class 'pyspark.ml.featur<...>etedRandomProjectionLSH'>\n",
      "BucketedRandomProjectionLSHModel    type            <class 'pyspark.ml.featur<...>andomProjectionLSHModel'>\n",
      "Bucketizer                          type            <class 'pyspark.ml.feature.Bucketizer'>\n",
      "ByteType                            type            <class 'pyspark.sql.types.ByteType'>\n",
      "ChiSqSelector                       type            <class 'pyspark.ml.feature.ChiSqSelector'>\n",
      "ChiSqSelectorModel                  type            <class 'pyspark.ml.feature.ChiSqSelectorModel'>\n",
      "CountVectorizer                     type            <class 'pyspark.ml.feature.CountVectorizer'>\n",
      "CountVectorizerModel                type            <class 'pyspark.ml.feature.CountVectorizerModel'>\n",
      "DCT                                 type            <class 'pyspark.ml.feature.DCT'>\n",
      "DataType                            type            <class 'pyspark.sql.types.DataType'>\n",
      "DateType                            type            <class 'pyspark.sql.types.DateType'>\n",
      "DecimalType                         type            <class 'pyspark.sql.types.DecimalType'>\n",
      "DenseVector                         type            <class 'pyspark.ml.linalg.DenseVector'>\n",
      "DoubleType                          type            <class 'pyspark.sql.types.DoubleType'>\n",
      "ElementwiseProduct                  type            <class 'pyspark.ml.feature.ElementwiseProduct'>\n",
      "F                                   module          <module 'pyspark.sql.func<...>yspark/sql/functions.py'>\n",
      "FeatureHasher                       type            <class 'pyspark.ml.feature.FeatureHasher'>\n",
      "FloatType                           type            <class 'pyspark.sql.types.FloatType'>\n",
      "GBTClassifier                       type            <class 'pyspark.ml.classification.GBTClassifier'>\n",
      "HashingTF                           type            <class 'pyspark.ml.feature.HashingTF'>\n",
      "IDF                                 type            <class 'pyspark.ml.feature.IDF'>\n",
      "IDFModel                            type            <class 'pyspark.ml.feature.IDFModel'>\n",
      "Imputer                             type            <class 'pyspark.ml.feature.Imputer'>\n",
      "ImputerModel                        type            <class 'pyspark.ml.feature.ImputerModel'>\n",
      "IndexToString                       type            <class 'pyspark.ml.feature.IndexToString'>\n",
      "IntegerType                         type            <class 'pyspark.sql.types.IntegerType'>\n",
      "Interaction                         type            <class 'pyspark.ml.feature.Interaction'>\n",
      "LogisticRegression                  type            <class 'pyspark.ml.classi<...>tion.LogisticRegression'>\n",
      "LongType                            type            <class 'pyspark.sql.types.LongType'>\n",
      "MapType                             type            <class 'pyspark.sql.types.MapType'>\n",
      "MaxAbsScaler                        type            <class 'pyspark.ml.feature.MaxAbsScaler'>\n",
      "MaxAbsScalerModel                   type            <class 'pyspark.ml.feature.MaxAbsScalerModel'>\n",
      "MinHashLSH                          type            <class 'pyspark.ml.feature.MinHashLSH'>\n",
      "MinHashLSHModel                     type            <class 'pyspark.ml.feature.MinHashLSHModel'>\n",
      "MinMaxScaler                        type            <class 'pyspark.ml.feature.MinMaxScaler'>\n",
      "MinMaxScalerModel                   type            <class 'pyspark.ml.feature.MinMaxScalerModel'>\n",
      "MulticlassClassificationEvaluator   type            <class 'pyspark.ml.evalua<...>ClassificationEvaluator'>\n",
      "MulticlassMetrics                   type            <class 'pyspark.mllib.eva<...>ation.MulticlassMetrics'>\n",
      "NGram                               type            <class 'pyspark.ml.feature.NGram'>\n",
      "Normalizer                          type            <class 'pyspark.ml.feature.Normalizer'>\n",
      "NullType                            type            <class 'pyspark.sql.types.NullType'>\n",
      "OneHotEncoder                       type            <class 'pyspark.ml.feature.OneHotEncoder'>\n",
      "OneHotEncoderModel                  type            <class 'pyspark.ml.feature.OneHotEncoderModel'>\n",
      "PCA                                 type            <class 'pyspark.ml.feature.PCA'>\n",
      "PCAModel                            type            <class 'pyspark.ml.feature.PCAModel'>\n",
      "Pipeline                            type            <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "PolynomialExpansion                 type            <class 'pyspark.ml.feature.PolynomialExpansion'>\n",
      "QuantileDiscretizer                 type            <class 'pyspark.ml.feature.QuantileDiscretizer'>\n",
      "RFormula                            type            <class 'pyspark.ml.feature.RFormula'>\n",
      "RFormulaModel                       type            <class 'pyspark.ml.feature.RFormulaModel'>\n",
      "RandomForestClassifier              type            <class 'pyspark.ml.classi<...>.RandomForestClassifier'>\n",
      "RegexTokenizer                      type            <class 'pyspark.ml.feature.RegexTokenizer'>\n",
      "RobustScaler                        type            <class 'pyspark.ml.feature.RobustScaler'>\n",
      "RobustScalerModel                   type            <class 'pyspark.ml.feature.RobustScalerModel'>\n",
      "SQLContext                          type            <class 'pyspark.sql.context.SQLContext'>\n",
      "SQLTransformer                      type            <class 'pyspark.ml.feature.SQLTransformer'>\n",
      "ShortType                           type            <class 'pyspark.sql.types.ShortType'>\n",
      "SparkSession                        type            <class 'pyspark.sql.session.SparkSession'>\n",
      "StandardScaler                      type            <class 'pyspark.ml.feature.StandardScaler'>\n",
      "StandardScalerModel                 type            <class 'pyspark.ml.feature.StandardScalerModel'>\n",
      "StopWordsRemover                    type            <class 'pyspark.ml.feature.StopWordsRemover'>\n",
      "StringIndexer                       type            <class 'pyspark.ml.feature.StringIndexer'>\n",
      "StringIndexerModel                  type            <class 'pyspark.ml.feature.StringIndexerModel'>\n",
      "StringType                          type            <class 'pyspark.sql.types.StringType'>\n",
      "StructField                         type            <class 'pyspark.sql.types.StructField'>\n",
      "StructType                          type            <class 'pyspark.sql.types.StructType'>\n",
      "TimestampType                       type            <class 'pyspark.sql.types.TimestampType'>\n",
      "Tokenizer                           type            <class 'pyspark.ml.feature.Tokenizer'>\n",
      "VectorAssembler                     type            <class 'pyspark.ml.feature.VectorAssembler'>\n",
      "VectorIndexer                       type            <class 'pyspark.ml.feature.VectorIndexer'>\n",
      "VectorIndexerModel                  type            <class 'pyspark.ml.feature.VectorIndexerModel'>\n",
      "VectorSizeHint                      type            <class 'pyspark.ml.feature.VectorSizeHint'>\n",
      "VectorSlicer                        type            <class 'pyspark.ml.feature.VectorSlicer'>\n",
      "Word2Vec                            type            <class 'pyspark.ml.feature.Word2Vec'>\n",
      "Word2VecModel                       type            <class 'pyspark.ml.feature.Word2VecModel'>\n",
      "all_data                            RDD             /../../project/ds5559/Ali<...>MethodAccessorImpl.java:0\n",
      "df                                  DataFrame       DataFrame[trip_id: string<...>nd_Hour: int, Date: date]\n",
      "end_times                           DataFrame       DataFrame[_1: string, _2: string]\n",
      "et                                  DataFrame       DataFrame[_1: string, _2:<...>End_Timestamp: timestamp]\n",
      "final_DF                            DataFrame       DataFrame[trip_id: string<...>nd_Hour: int, Date: date]\n",
      "header                              list            n=21\n",
      "os                                  module          <module 'os' from '/opt/c<...>nda/lib/python3.8/os.py'>\n",
      "rdd                                 PipelinedRDD    PythonRDD[37] at RDD at PythonRDD.scala:53\n",
      "sc                                  SparkContext    <SparkContext master=loca<...>appName=mllib_classifier>\n",
      "spark                               SparkSession    <pyspark.sql.session.Spar<...>object at 0x7fad54275d30>\n",
      "sqlContext                          SQLContext      <pyspark.sql.context.SQLC<...>object at 0x7fad6426fc10>\n",
      "st                                  DataFrame       DataFrame[_1: string, _2:<...>art_Timestamp: timestamp]\n",
      "start_times                         DataFrame       DataFrame[_1: string, _2: string]\n",
      "typ                                 module          <module 'pyspark.sql.type<...>on/pyspark/sql/types.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('restructured_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (all_data)\n",
    "del (final_DF)\n",
    "del (end_times)\n",
    "del (et)\n",
    "del (header)\n",
    "del (st)\n",
    "del (start_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trip_id: string, Trip_Start_Timestamp: timestamp, Trip_End_Timestamp: timestamp, Trip_Seconds: int, Trip_Miles: double, Pickup_Community_Area: int, Dropoff_Community_Area: int, Fare: double, Tip: double, Additional_Charges: double, Trip_Total: double, Shared_Trip_Authorized: boolean, Trips_Pooled: int, Trip_Year: int, Trip_Month: int, Trip_WeekNumber: int, Trip_DayofWeek: int, Trip_Start_Hour: int, Trip_End_Hour: int, Date: date]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.sample(False, .0005, seed = 2021) #decreased our sample size\n",
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+\n",
      "|             trip_id|Trip_Start_Timestamp| Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_End_Hour|      Date|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+\n",
      "|03c8ea04dbf3be99f...| 2020-02-07 22:45:00|2020-02-07 23:00:00|         512|       1.2|                   28|                     8| 5.0|0.0|              3.08|      8.08|                 false|           1|     2020|         2|              6|             6|             22|           23|2020-02-07|\n",
      "|069f6a8266dac52c0...| 2020-02-27 08:15:00|2020-02-27 08:30:00|         840|       2.1|                    8|                    32|17.5|0.0|              5.13|     22.63|                 false|           1|     2020|         2|              9|             5|              8|            8|2020-02-27|\n",
      "|06bca6c918f06efb7...| 2020-05-16 23:45:00|2020-05-17 00:00:00|         877|       4.1|                   22|                    25|10.0|0.0|              3.08|     13.08|                 false|           1|     2020|         5|             20|             7|             23|            0|2020-05-16|\n",
      "|075e008fcc3dde8eb...| 2019-12-19 12:30:00|2019-12-19 12:45:00|        1189|      16.4|                 null|                    76|20.0|0.0|             10.95|     30.95|                 false|           1|     2019|        12|             51|             5|             12|           12|2019-12-19|\n",
      "|0940792dcc4e07e70...| 2020-01-28 19:30:00|2020-01-28 19:30:00|         115|       0.3|                    8|                     8| 2.5|0.0|              4.83|      7.33|                 false|           1|     2020|         1|              5|             3|             19|           19|2020-01-28|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill our NA community areas\n",
    "\n",
    "df2 = df2.na.fill(value=78,subset=['Pickup_Community_Area', 'Dropoff_Community_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a binary tip/no tip indicator\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer\n",
    "\n",
    "#binarized tip seems to be causing problems.  Change its name to label as that is that the packages are expecting\n",
    "\n",
    "binarizer = Binarizer(threshold=0, inputCol=\"Tip\", outputCol=\"label\")\n",
    "df2 = binarizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trip_id: string, Trip_Start_Timestamp: timestamp, Trip_End_Timestamp: timestamp, Trip_Seconds: int, Trip_Miles: double, Pickup_Community_Area: int, Dropoff_Community_Area: int, Fare: double, Tip: double, Additional_Charges: double, Trip_Total: double, Shared_Trip_Authorized: boolean, Trips_Pooled: int, Trip_Year: int, Trip_Month: int, Trip_WeekNumber: int, Trip_DayofWeek: int, Trip_Start_Hour: int, Trip_End_Hour: int, Date: date, label: double]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_End_Hour: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original No Tip Count:  16403\n",
      "Original Tip Count   :  3268\n",
      "\n",
      "Final No Tip Count   :  16403\n",
      "Final Tip Count      :  16459\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "\n",
    "# our model didn't work on the standard test train split.  Prof. Tashman recomended upscalling the help with the imbalanced dataset.\n",
    "#from https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\n",
    "\n",
    "train_inital, test = df2.randomSplit([0.8, 0.2], seed=2021)\n",
    "\n",
    "# cahce our test values for later speed\n",
    "test.cache()\n",
    "\n",
    "# oversampleing code sample\n",
    "# https://stackoverflow.com/questions/53273133/how-to-perform-up-sampling-using-sample-functionpy-spark\n",
    "\n",
    "df_a = train_inital.filter(train_inital['label'] == 0)\n",
    "df_b = train_inital.filter(train_inital['label'] == 1)\n",
    "\n",
    "org_a_count = df_a.count()\n",
    "org_b_count = df_b.count() \n",
    "\n",
    "\n",
    "ratio = df_a.count() / df_b.count()\n",
    "# print(ratio)\n",
    "\n",
    "df_b_overampled = df_b.sample(withReplacement=True, fraction=ratio, seed=2021)\n",
    "\n",
    "# cahce our train values for later speed\n",
    "train = df_a.unionAll(df_b_overampled).cache()\n",
    "\n",
    "df_af = train.filter(train_inital['label'] == 0)\n",
    "df_bf = train.filter(train_inital['label'] == 1)\n",
    "fin_a_count = df_af.count()\n",
    "fin_b_count = df_bf.count() \n",
    "\n",
    "print(\"Original No Tip Count: \", org_a_count)\n",
    "print(\"Original Tip Count   : \", org_b_count)\n",
    "print(\"\")\n",
    "print(\"Final No Tip Count   : \", fin_a_count)\n",
    "print(\"Final Tip Count      : \", fin_b_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "|             trip_id|Trip_Start_Timestamp| Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_End_Hour|      Date|label|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "|075e008fcc3dde8eb...| 2019-12-19 12:30:00|2019-12-19 12:45:00|        1189|      16.4|                   78|                    76|20.0|0.0|             10.95|     30.95|                 false|           1|     2019|        12|             51|             5|             12|           12|2019-12-19|  0.0|\n",
      "|0d263270466738061...| 2019-12-26 21:45:00|2019-12-26 22:15:00|        1614|      18.5|                   56|                     3|25.0|0.0|              8.69|     33.69|                 false|           1|     2019|        12|             52|             5|             21|           22|2019-12-26|  0.0|\n",
      "|0ffde954e09b54da8...| 2020-03-13 06:00:00|2020-03-13 06:30:00|        1566|      17.5|                   42|                     8|15.0|0.0|               0.0|      15.0|                  true|           2|     2020|         3|             11|             6|              6|            6|2020-03-13|  0.0|\n",
      "|39c2b503c951268e7...| 2020-09-04 10:00:00|2020-09-04 10:30:00|        1875|      23.1|                   40|                    78|30.0|0.0|              3.18|     33.18|                 false|           1|     2020|         9|             36|             6|             10|           10|2020-09-04|  0.0|\n",
      "|4efa52d3765458577...| 2020-01-25 09:30:00|2020-01-25 10:00:00|        1579|       8.2|                   66|                    24|15.0|0.0|              3.08|     18.08|                 false|           1|     2020|         1|              4|             7|              9|           10|2020-01-25|  0.0|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "|             trip_id|Trip_Start_Timestamp| Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_End_Hour|      Date|label|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "|03c8ea04dbf3be99f...| 2020-02-07 22:45:00|2020-02-07 23:00:00|         512|       1.2|                   28|                     8| 5.0|0.0|              3.08|      8.08|                 false|           1|     2020|         2|              6|             6|             22|           23|2020-02-07|  0.0|\n",
      "|069f6a8266dac52c0...| 2020-02-27 08:15:00|2020-02-27 08:30:00|         840|       2.1|                    8|                    32|17.5|0.0|              5.13|     22.63|                 false|           1|     2020|         2|              9|             5|              8|            8|2020-02-27|  0.0|\n",
      "|06bca6c918f06efb7...| 2020-05-16 23:45:00|2020-05-17 00:00:00|         877|       4.1|                   22|                    25|10.0|0.0|              3.08|     13.08|                 false|           1|     2020|         5|             20|             7|             23|            0|2020-05-16|  0.0|\n",
      "|0940792dcc4e07e70...| 2020-01-28 19:30:00|2020-01-28 19:30:00|         115|       0.3|                    8|                     8| 2.5|0.0|              4.83|      7.33|                 false|           1|     2020|         1|              5|             3|             19|           19|2020-01-28|  0.0|\n",
      "|09de848c9e68948c8...| 2020-03-27 21:45:00|2020-03-27 22:00:00|        1092|       5.3|                   29|                    26|12.5|0.0|              3.08|     15.58|                 false|           1|     2020|         3|             13|             6|             21|           22|2020-03-27|  0.0|\n",
      "+--------------------+--------------------+-------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+---------+----------+---------------+--------------+---------------+-------------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training size has increased.  This is to be expected in upscaling.\n",
    "\n",
    "Good reference: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4188\n",
      "827\n"
     ]
    }
   ],
   "source": [
    "#just as a reminder what was the truth in our test data?\n",
    "\n",
    "dft_a = test.filter(train_inital['label'] == 0)\n",
    "dft_b = test.filter(train_inital['label'] == 1)\n",
    "count_test_a = dft_a.count()\n",
    "count_test_b = dft_b.count()\n",
    "print(count_test_a)\n",
    "print(count_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    # McM accuracy\n",
    "    \n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print()\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()     \n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc2(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    #McM accuracy\n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    #precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "       \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()\n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()\n",
    "    print(\"Area Under the PR Curve\", prc)\n",
    "   \n",
    "    print(\"Summary Stats\")\n",
    "    #print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    \n",
    "    # Weighted stats\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_End_Hour: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to endHour\n",
    "ohe_eh = OneHotEncoder(inputCol=\"Trip_End_Hour\", outputCol=\"Trip_End_Hour_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_End_Hour_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(maxIter=10,\n",
    "                        regParam=0.1, #org 0.1\n",
    "                        elasticNetParam=0.3, #org 0.3\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"label\")\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_eh, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "lr_prediction = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3165. 1023.]\n",
      " [ 505.  322.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6953140578265204\n",
      "\n",
      "Accuracy from MulticlassMetrics:  0.6953140578265204\n",
      "\n",
      "Area Under the ROC 0.5725448942045506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "recall() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a549c950343f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcmacc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-09581920a5cb>\u001b[0m in \u001b[0;36mcmacc2\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#precision = metrics.precision()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mf1Score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfMeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: recall() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms the results of the first pass of the grid search from tuning below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.380857\n",
      "Accuracy:  0.6191425722831505\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_628354a6ca93, numTrees=10, numClasses=2, numFeatures=269\n"
     ]
    }
   ],
   "source": [
    "# pipeline steps for Random Forest:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to endHour\n",
    "ohe_eh = OneHotEncoder(inputCol=\"Trip_End_Hour\", outputCol=\"Trip_End_Hour_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf =  ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_End_Hour_vec']\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=10)\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_eh, rf_va, rf])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "rf_prediction = rf_model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "#rf_prediction.select(\"prediction\", \"label\",\"features\").show(5) \n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#metric options f1|accuracy|weightedPrecision|weightedRecall\n",
    "\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_prediction)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - rf_accuracy))\n",
    "print(\"Accuracy: \" , rf_accuracy)\n",
    "\n",
    "rfModel2 = rf_model.stages[7]\n",
    "print(rfModel2)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[2631. 1557.]\n",
      " [ 353.  474.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6191425722831505\n",
      "\n",
      "Accuracy from MulticlassMetrics:  0.6191425722831505\n",
      "\n",
      "Area Under the ROC 0.6006897405958639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmacc(rf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a GBT \n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  0.0|(269,[0,1,2,3,5,1...|\n",
      "|       1.0|  0.0|(269,[0,1,2,3,5,6...|\n",
      "|       0.0|  0.0|(269,[0,1,2,4,5,4...|\n",
      "|       0.0|  0.0|(269,[0,1,2,3,5,4...|\n",
      "|       0.0|  0.0|(269,[0,1,2,3,5,7...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.358923\n",
      "gbt accuracy =  0.6410767696909272\n",
      "GBTClassificationModel: uid = GBTClassifier_ea71d901a054, numTrees=5, numClasses=2, numFeatures=269\n"
     ]
    }
   ],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to endHour\n",
    "ohe_eh = OneHotEncoder(inputCol=\"Trip_End_Hour\", outputCol=\"Trip_End_Hour_vec\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "# labelIndexer = StringIndexer(inputCol=\"binarized_tip\", outputCol=\"indexedLabel\")#.fit(data)\n",
    "\n",
    "#assemble the vector or GBT\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_End_Hour_vec']\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_gbt, outputCol=\"features\") \n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)#.fit(data)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=5)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_eh, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = gbt_pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "gbt_prediction = gbt_model.transform(test)\n",
    "\n",
    "#gbt_prediction.show(3)\n",
    "\n",
    "# Select example rows to display.\n",
    "gbt_prediction.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "gbt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "gbt_accuracy = gbt_evaluator.evaluate(gbt_prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - gbt_accuracy))\n",
    "print('gbt accuracy = ', gbt_accuracy)\n",
    "gbtModel = gbt_model.stages[7]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[2791. 1397.]\n",
      " [ 403.  424.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6410767696909272\n",
      "\n",
      "Accuracy from MulticlassMetrics:  0.6410767696909272\n",
      "\n",
      "Area Under the ROC 0.5895621912783574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmacc(gbt_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline LR with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\",\n",
    "                        labelCol=\"label\") # regParam=0.1, elasticNetParam=0.3, maxIter=10,\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(lr_paramGrid): {}'.format(len(lr_paramGrid)))\n",
    "'''\n",
    "25k set\n",
    "best from tuning inital (accuracy):\n",
    "addGrid(lr.regParam, [0.03, 0.05, 0.07]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.15, 0.2, 0.25]) \\\n",
    ".addGrid(lr.maxIter, [8, 9, 10, 11, 12]) \n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 12\n",
    " \n",
    "Confusion Matrix\n",
    "[[1842. 2203.]\n",
    " [ 193.  595.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5042416718394372\n",
    "Accuracy from MulticlassMetrics:  0.5042416718394372\n",
    "\n",
    "Area Under the ROC 0.6052265753923187\n",
    "\n",
    "Area Under the PR Curve 0.2065770273222743\n",
    "Summary Stats\n",
    "Precision = 0.5042416718394372\n",
    "Recall = 0.5042416718394372\n",
    "F1 Score = 0.5042416718394372\n",
    "Weighted recall = 0.5042416718394371\n",
    "Weighted precision = 0.7922492654683645\n",
    "Weighted F(1) Score = 0.5612342974368173\n",
    "Weighted F(0.5) Score = 0.6730989071456968\n",
    "Weighted false positive rate = 0.2937885210547999\n",
    "\n",
    "2nd round:\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.05, 0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [11, 12, 13, 15]) \\\n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 11\n",
    "\n",
    "Confusion Matrix\n",
    "[[1815. 2230.]\n",
    " [ 183.  605.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5007241878750258\n",
    "Accuracy from MulticlassMetrics:  0.5007241878750258\n",
    "\n",
    "Area Under the ROC 0.6082342994108162\n",
    "\n",
    "Area Under the PR Curve 0.2075564549699384\n",
    "Summary Stats\n",
    "Precision = 0.5007241878750258\n",
    "Recall = 0.5007241878750258\n",
    "F1 Score = 0.5007241878750258\n",
    "Weighted recall = 0.5007241878750258\n",
    "Weighted precision = 0.7950908896146499\n",
    "Weighted F(1) Score = 0.5572078454446675\n",
    "Weighted F(0.5) Score = 0.6716684076809212\n",
    "Weighted false positive rate = 0.28425558905339354\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "lr_cvModel = lr_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is... apparently it is RMSE https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf\n",
    "#lr_cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "# lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "lr_cvModel.getEstimatorParamMaps()[ np.argmin(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "lr_prediction = lr_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline RF with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pipeline steps for RF:\n",
    "\n",
    "# # Index labels, adding metadata to the label column.\n",
    "# # Fit on whole dataset to include all labels in index.\n",
    "# labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "# #onehotencoder to pickup\n",
    "# ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "# #onehotencoder to dropoff\n",
    "# ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# #assemble the vector or LR\n",
    "\n",
    "# # our colulms for rf\n",
    "# predictor_col_for_rf = ['Trip_Seconds',\n",
    "#                         'Trip_Miles',\n",
    "#                         'Fare',\n",
    "#                         'Additional_Charges',\n",
    "#                         'Shared_Trip_Authorized',\n",
    "#                         'Trips_Pooled',\n",
    "#                         'Pickup_Community_Area_vec',\n",
    "#                         'Dropoff_Community_Area_vec']\n",
    "\n",
    "# #assemble the vector or RF\n",
    "                             \n",
    "# rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4) #what if 10 like in other basic version\n",
    "\n",
    "# #what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "# #we learned that this week.  May also need to add in cv step\n",
    "\n",
    "# rf = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "#                             featuresCol=\"indexedFeatures\") #numTrees=10\n",
    "\n",
    "# # Build the pipeline\n",
    "# rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, labelIndexer, rf_va, featureIndexer, rf])\n",
    "\n",
    "# pipeline steps for Random Forest:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\")\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, rf_va, rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "    #.addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    \n",
    "\n",
    "print('len(rf_paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "#https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n",
    "#maxDepth, maxBins, impurity, auto and seed \n",
    "#.addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "#name='featureSubsetStrategy', auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 10\n",
    "impurity = gini\n",
    " \n",
    "Confusion Matrix\n",
    "[[2901. 1144.]\n",
    " [ 456.  332.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6689426857024623\n",
    "Accuracy from MulticlassMetrics:  0.6689426857024623\n",
    "\n",
    "Area Under the ROC 0.5692507513819781\n",
    "\n",
    "Area Under the PR Curve 0.2070259967551608\n",
    "Summary Stats\n",
    "Precision = 0.6689426857024623\n",
    "Recall = 0.6689426857024623\n",
    "F1 Score = 0.6689426857024623\n",
    "Weighted recall = 0.6689426857024622\n",
    "Weighted precision = 0.7599403563099746\n",
    "Weighted F(1) Score = 0.7038591473392332\n",
    "Weighted F(0.5) Score = 0.735232181263078\n",
    "Weighted false positive rate = 0.530441182938506\n",
    "\n",
    "2nd attempt:\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 15\n",
    "impurity = gini\n",
    "featureSubsetStrategy = auto\n",
    "\n",
    "Confusion Matrix\n",
    "[[2663. 1382.]\n",
    " [ 388.  400.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6337678460583489\n",
    "Accuracy from MulticlassMetrics:  0.6337678460583489\n",
    "\n",
    "Area Under the ROC 0.5829789236570811\n",
    "\n",
    "Area Under the PR Curve 0.209345437091233\n",
    "Summary Stats\n",
    "Precision = 0.6337678460583489\n",
    "Recall = 0.6337678460583489\n",
    "F1 Score = 0.6337678460583489\n",
    "Weighted recall = 0.6337678460583488\n",
    "Weighted precision = 0.7671159775546591\n",
    "Weighted F(1) Score = 0.6789410276493224\n",
    "Weighted F(0.5) Score = 0.7270236282319229\n",
    "Weighted false positive rate = 0.46780999874418644\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_rf = rf_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is... rmse\n",
    "cvModel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_rf = cvModel_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmin(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline GBT with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for RF:\n",
    "\n",
    "# # Index labels, adding metadata to the label column.\n",
    "# # Fit on whole dataset to include all labels in index.\n",
    "# labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "\n",
    "# #onehotencoder to pickup\n",
    "# ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "# #onehotencoder to dropoff\n",
    "# ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# #assemble the vector or LR\n",
    "\n",
    "# # our colulms for rf\n",
    "# predictor_col_for_gbt = ['Trip_Seconds',\n",
    "#                         'Trip_Miles',\n",
    "#                         'Fare',\n",
    "#                         'Additional_Charges',\n",
    "#                         'Shared_Trip_Authorized',\n",
    "#                         'Trips_Pooled',\n",
    "#                         'Pickup_Community_Area_vec',\n",
    "#                         'Dropoff_Community_Area_vec']\n",
    "\n",
    "# #assemble the vector or RF\n",
    "                             \n",
    "# gbt_va = VectorAssembler(inputCols=predictor_col_for_gbt, outputCol=\"features\") \n",
    "\n",
    "# # Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)\n",
    "\n",
    "# #what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "# #we learned that this week.  May also need to add in cv step\n",
    "\n",
    "# # Train a GBT model.\n",
    "# gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")#maxIter=5\n",
    "\n",
    "# # Build the pipeline\n",
    "# gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, labelIndexer, gbt_va, featureIndexer, gbt])\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "# labelIndexer = StringIndexer(inputCol=\"binarized_tip\", outputCol=\"indexedLabel\")#.fit(data)\n",
    "\n",
    "#assemble the vector or GBT\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\") #, maxIter=5\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Set up the parameter grid\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "print('len(gbt_paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "maxIter = 20\n",
    "\n",
    "Confusion Matrix\n",
    "[[2798. 1247.]\n",
    " [ 423.  365.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.654458928201945\n",
    "Accuracy from MulticlassMetrics:  0.654458928201945\n",
    "\n",
    "Area Under the ROC 0.5774580700620556\n",
    "\n",
    "Area Under the PR Curve 0.2094152550126291\n",
    "Summary Stats\n",
    "Precision = 0.654458928201945\n",
    "Recall = 0.654458928201945\n",
    "F1 Score = 0.654458928201945\n",
    "Weighted recall = 0.654458928201945\n",
    "Weighted precision = 0.7639586098089827\n",
    "Weighted F(1) Score = 0.6941837869282138\n",
    "Weighted F(0.5) Score = 0.7327747544723259\n",
    "Weighted false positive rate = 0.4995427880778336\n",
    "\n",
    "\n",
    "maxIter = 40\n",
    "\n",
    "Confusion Matrix\n",
    "[[2571. 1474.]\n",
    " [ 382.  406.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6159735154148562\n",
    "Accuracy from MulticlassMetrics:  0.6159735154148562\n",
    "\n",
    "Area Under the ROC 0.5754139659791808\n",
    "\n",
    "Area Under the PR Curve 0.20313239804234073\n",
    "Summary Stats\n",
    "Precision = 0.6159735154148562\n",
    "Recall = 0.6159735154148562\n",
    "F1 Score = 0.6159735154148562\n",
    "Weighted recall = 0.6159735154148562\n",
    "Weighted precision = 0.7638968296438198\n",
    "Weighted F(1) Score = 0.6646010165217533\n",
    "Weighted F(0.5) Score = 0.7183436330713325\n",
    "Weighted false positive rate = 0.4651455834564943\n",
    "\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "    \n",
    "maxIter = 5\n",
    "maxDepth = 5\n",
    "\n",
    "Confusion Matrix\n",
    "[[2863. 1182.]\n",
    " [ 469.  319.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6583902338092282\n",
    "Accuracy from MulticlassMetrics:  0.6583902338092282\n",
    "\n",
    "Area Under the ROC 0.5563048634335804\n",
    "\n",
    "Area Under the PR Curve 0.19780050930331394\n",
    "Summary Stats\n",
    "Precision = 0.6583902338092282\n",
    "Recall = 0.6583902338092282\n",
    "F1 Score = 0.6583902338092282\n",
    "Weighted recall = 0.6583902338092282\n",
    "Weighted precision = 0.7537989743798752\n",
    "Weighted F(1) Score = 0.6950856095348379\n",
    "Weighted F(0.5) Score = 0.7279222226014918\n",
    "Weighted false positive rate = 0.5457805069420676\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "gbt_crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_gbt = gbt_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is...\n",
    "cvModel_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "#cvModel_gbt.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]\n",
    "\n",
    "cvModel_gbt.getEstimatorParamMaps()[np.argmax(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_gbt.getEstimatorParamMaps()[np.argmin(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_gbt = cvModel_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(prediction_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
