{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DS5559 Final Project Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '21g') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.instances', '3') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# import data manipulation methods\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "#from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Schema.  \n",
    "This schema was been primarly determined by using a much smaller dataset and letting spark infer the schema.  We encountered an issue with spark reading in the ENTIRE dataset as NULL when there was a type mismatch.  Only the data we are likely to use later has been assigned to a specific type, otherwise it is left as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Census_Tract|Dropoff_Census_Tract|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Pickup_Centroid_Latitude|Pickup_Centroid_Longitude|Pickup_Centroid_Location|Dropoff_Centroid_Latitude|Dropoff_Centroid_Longitude|Dropoff_Centroid_Location|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...|12/01/2019 01:00:...|      2675.0|      32.5|        17031980000|                null|                 76.0|                  null|70.0|0.0|              9.06|     79.06|                 false|         1.0|           41.9790708201|           -87.9030396611|    POINT (-87.903039...|                     null|                      null|                     null|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...|12/01/2019 12:15:...|       550.0|       2.8|        17031242100|         17031081700|                 24.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8996701799|           -87.6698377982|    POINT (-87.669837...|            41.8920421365|            -87.6318639497|     POINT (-87.631863...|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...|12/01/2019 12:30:...|       922.0|       2.8|        17031080300|         17031281900|                  8.0|                  28.0|10.0|0.0|              3.11|     13.11|                 false|         1.0|           41.9074919303|           -87.6357600901|    POINT (-87.635760...|            41.8792550844|             -87.642648998|     POINT (-87.642648...|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...|12/01/2019 12:30:...|      1475.0|      12.5|        17031310600|         17031063302|                 31.0|                   6.0|17.5|4.0|              2.55|     24.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|            41.9347624564|            -87.6398538587|     POINT (-87.639853...|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...|12/01/2019 12:30:...|       594.0|       2.5|        17031310600|         17031330100|                 31.0|                  33.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|             41.859349715|            -87.6173580061|     POINT (-87.617358...|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a custom schema.  \n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField('Trip_ID', StringType(), True),        \n",
    "    StructField('Trip_Start_Timestamp', StringType(), True),\n",
    "    StructField('Trip_End_Timestamp', StringType(), True),\n",
    "    StructField('Trip_Seconds', DoubleType(), True),\n",
    "    StructField('Trip_Miles', DoubleType(), True),\n",
    "    StructField('Pickup_Census_Tract', StringType(), True),\n",
    "    StructField('Dropoff_Census_Tract', StringType(), True),\n",
    "    StructField('Pickup_Community_Area', DoubleType(), True),\n",
    "    StructField('Dropoff_Community_Area', DoubleType(), True),\n",
    "    StructField(\"Fare\", DoubleType(), True),\n",
    "    StructField(\"Tip\", DoubleType(), True),\n",
    "    StructField(\"Additional_Charges\", DoubleType(), True),\n",
    "    StructField(\"Trip_Total\", StringType(), True),\n",
    "    StructField(\"Shared_Trip_Authorized\", BooleanType(), True),\n",
    "    StructField(\"Trips_Pooled\", DoubleType(), True),\n",
    "    StructField('Pickup_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Location', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Location', StringType(), True)])\n",
    "\n",
    "#old readin.  Infer is slow for large dataset\n",
    "#df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, inferSchema=True)\n",
    "\n",
    "#read in the data to a dataframe\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, schema=customSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable\n",
    "\n",
    "df = df.drop('Pickup_Census_Tract',\n",
    "             'Dropoff_Census_Tract',\n",
    "             'Pickup_Centroid_Latitude',\n",
    "             'Pickup_Centroid_Longitude', \n",
    "             'Pickup_Centroid_Location', \n",
    "             'Dropoff_Centroid_Latitude', \n",
    "             'Dropoff_Centroid_Longitude', \n",
    "             'Dropoff_Centroid_Location')\n",
    "\n",
    "#'Trip_End_Timestamp' keep for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(False, .05, seed = 2021) #decreased our sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2454879"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill our NA community areas\n",
    "\n",
    "df2 = df2.na.fill(value=78,subset=['Pickup_Community_Area', 'Dropoff_Community_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a binary tip/no tip indicator\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer\n",
    "\n",
    "#binarized tip seems to be causing problems.  Change its name to label as that is that the packages are expecting\n",
    "\n",
    "binarizer = Binarizer(threshold=0, inputCol=\"Tip\", outputCol=\"label\")\n",
    "df2 = binarizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"Trip_Start_TS\", F.to_timestamp(F.col(\"Trip_Start_Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- Trip_Start_TS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|Trip_ID                                 |Trip_Start_Timestamp  |Trip_End_Timestamp    |Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|Trip_Start_TS      |Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|Date      |\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|0bdddabc391ef52fc63e30d35808dead269a905f|12/01/2019 12:15:00 AM|12/01/2019 12:15:00 AM|739.0       |3.5       |4.0                  |1.0                   |7.5 |0.0|2.55              |10.05     |true                  |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|0cb70321eae7e4a0b36c2d3281fa100c34ac04ae|12/01/2019 12:15:00 AM|12/01/2019 12:15:00 AM|303.0       |1.4       |23.0                 |27.0                  |5.0 |0.0|2.55              |7.55      |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|119b6f96668219f181fb4014e71a806e5b679e76|12/01/2019 12:15:00 AM|12/01/2019 12:30:00 AM|709.0       |4.7       |28.0                 |7.0                   |12.5|0.0|2.85              |15.35     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|144e04da0eaeccd9c3921a535318c592efc8bc1f|12/01/2019 12:15:00 AM|12/01/2019 12:45:00 AM|1022.0      |3.5       |33.0                 |8.0                   |10.0|0.0|2.55              |12.55     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|146042694c82251a322183a28b27440f661a5188|12/01/2019 12:15:00 AM|12/01/2019 12:30:00 AM|892.0       |6.2       |6.0                  |77.0                  |10.0|0.0|2.55              |12.55     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('Trip_Year',F.year(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Month',F.month(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_WeekNumber',F.weekofyear(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_DayofWeek', F.dayofweek(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Start_Hour', F.hour(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Start_Minute', F.minute(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Date', F.to_date(F.col('Trip_Start_TS')))\n",
    "         \n",
    "df2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- Trip_Start_TS: timestamp (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_Start_Minute: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original No Tip Count:  1634450\n",
      "Original Tip Count   :  329363\n",
      "\n",
      "Final No Tip Count   :  1634450\n",
      "Final Tip Count      :  1635935\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "\n",
    "# our model didn't work on the standard test train split.  Prof. Tashman recomended upscalling the help with the imbalanced dataset.\n",
    "#from https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\n",
    "\n",
    "train_inital, test = df2.randomSplit([0.8, 0.2], seed=2021)\n",
    "\n",
    "# cahce our test values for later speed\n",
    "test.cache()\n",
    "\n",
    "# oversampleing code sample\n",
    "# https://stackoverflow.com/questions/53273133/how-to-perform-up-sampling-using-sample-functionpy-spark\n",
    "\n",
    "df_a = train_inital.filter(train_inital['label'] == 0)\n",
    "df_b = train_inital.filter(train_inital['label'] == 1)\n",
    "\n",
    "org_a_count = df_a.count()\n",
    "org_b_count = df_b.count() \n",
    "\n",
    "\n",
    "ratio = df_a.count() / df_b.count()\n",
    "# print(ratio)\n",
    "\n",
    "df_b_overampled = df_b.sample(withReplacement=True, fraction=ratio, seed=2021)\n",
    "\n",
    "# cahce our train values for later speed\n",
    "train = df_a.unionAll(df_b_overampled).cache()\n",
    "\n",
    "df_af = train.filter(train_inital['label'] == 0)\n",
    "df_bf = train.filter(train_inital['label'] == 1)\n",
    "fin_a_count = df_af.count()\n",
    "fin_b_count = df_bf.count() \n",
    "\n",
    "print(\"Original No Tip Count: \", org_a_count)\n",
    "print(\"Original Tip Count   : \", org_b_count)\n",
    "print(\"\")\n",
    "print(\"Final No Tip Count   : \", fin_a_count)\n",
    "print(\"Final Tip Count      : \", fin_b_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|      Trip_Start_TS|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|      Date|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|0007e8f94f041a834...|12/01/2019 11:00:...|12/01/2019 11:00:...|       157.0|       0.4|                 77.0|                  77.0| 5.0|2.0|              2.55|      9.55|                 false|         1.0|  1.0|2019-12-01 23:00:00|     2019|        12|             48|             1|             23|                0|2019-12-01|\n",
      "|001ee0bc4c768af12...|12/01/2019 09:00:...|12/01/2019 09:15:...|       544.0|       3.3|                  8.0|                  28.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|  0.0|2019-12-01 21:00:00|     2019|        12|             48|             1|             21|                0|2019-12-01|\n",
      "|00285f26e1afb57f3...|12/01/2019 11:00:...|12/01/2019 11:15:...|       889.0|       9.3|                 24.0|                  39.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|  0.0|2019-12-01 11:00:00|     2019|        12|             48|             1|             11|                0|2019-12-01|\n",
      "|0056b922746f0028a...|12/01/2019 01:15:...|12/01/2019 01:15:...|       343.0|       1.3|                  8.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|  0.0|2019-12-01 01:15:00|     2019|        12|             48|             1|              1|               15|2019-12-01|\n",
      "|005a69bf68d2e56bd...|12/01/2019 09:30:...|12/01/2019 09:45:...|      1463.0|      23.8|                 76.0|                   7.0|25.0|0.0|               0.0|        25|                  true|         2.0|  0.0|2019-12-01 09:30:00|     2019|        12|             48|             1|              9|               30|2019-12-01|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|      Trip_Start_TS|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|      Date|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|0001d47ac70fccd44...|12/02/2019 09:45:...|12/02/2019 09:45:...|       401.0|       1.4|                  8.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|  0.0|2019-12-02 21:45:00|     2019|        12|             49|             2|             21|               45|2019-12-02|\n",
      "|0002fbd17ca414e95...|12/01/2019 08:45:...|12/01/2019 09:00:...|       284.0|       1.2|                 77.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|  0.0|2019-12-01 20:45:00|     2019|        12|             48|             1|             20|               45|2019-12-01|\n",
      "|000456646a9928609...|12/01/2019 03:00:...|12/01/2019 03:15:...|       355.0|       0.8|                  8.0|                   8.0| 7.5|0.0|              3.11|     10.61|                 false|         1.0|  0.0|2019-12-01 15:00:00|     2019|        12|             48|             1|             15|                0|2019-12-01|\n",
      "|000e879212e519b17...|12/02/2019 01:45:...|12/02/2019 02:15:...|      2376.0|      14.6|                 24.0|                  56.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|  0.0|2019-12-02 13:45:00|     2019|        12|             49|             2|             13|               45|2019-12-02|\n",
      "|001470a7635bd1b84...|12/02/2019 09:15:...|12/02/2019 09:30:...|       797.0|       2.5|                  6.0|                   8.0| 7.5|0.0|              2.55|     10.05|                  true|         1.0|  0.0|2019-12-02 09:15:00|     2019|        12|             49|             2|              9|               15|2019-12-02|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training size has increased.  This is to be expected in upscaling.\n",
    "\n",
    "Good reference: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409253\n",
      "81813\n"
     ]
    }
   ],
   "source": [
    "#just as a reminder what was the truth in our test data?\n",
    "\n",
    "dft_a = test.filter(train_inital['label'] == 0)\n",
    "dft_b = test.filter(train_inital['label'] == 1)\n",
    "count_test_a = dft_a.count()\n",
    "count_test_b = dft_b.count()\n",
    "print(count_test_a)\n",
    "print(count_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    # McM accuracy\n",
    "    \n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print()\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()     \n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()\n",
    "    print(\"Area Under the PR Curve\", prc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc2(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    #McM accuracy\n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "       \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()\n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()\n",
    "    print(\"Area Under the PR Curve\", prc)\n",
    "   \n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    \n",
    "    # Weighted stats\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(maxIter=10,\n",
    "                        regParam=0.1, #org 0.1\n",
    "                        elasticNetParam=0.3, #org 0.3\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"label\")\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "lr_prediction = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[312836.  96417.]\n",
      " [ 51813.  30000.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6981464813283754\n",
      "\n",
      "Accuracy from MulticlassMetrics:  0.6981464813283754\n",
      "\n",
      "Area Under the ROC 0.565548618702108\n",
      "\n",
      "Area Under the PR Curve 0.21492012791750828\n"
     ]
    }
   ],
   "source": [
    "cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[312836.  96417.]\n",
      " [ 51813.  30000.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6981464813283754\n",
      "Accuracy from MulticlassMetrics:  0.6981464813283754\n",
      "\n",
      "Area Under the ROC 0.565548618702108\n",
      "\n",
      "Area Under the PR Curve 0.21492012791750828\n",
      "Summary Stats\n",
      "Precision = 0.6981464813283754\n",
      "Recall = 0.6981464813283754\n",
      "F1 Score = 0.6981464813283754\n",
      "Weighted recall = 0.6981464813283754\n",
      "Weighted precision = 0.7545161871534753\n",
      "Weighted F(1) Score = 0.7217771204779072\n",
      "Weighted F(0.5) Score = 0.7404443373565394\n",
      "Weighted false positive rate = 0.5670492439241593\n"
     ]
    }
   ],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms the results of the first pass of the grid search from tuning below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.334784\n",
      "Accuracy:  0.6652160809341311\n",
      "OneHotEncoder_5b106eae9ee4\n"
     ]
    }
   ],
   "source": [
    "# pipeline steps for Random Forest:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=10)\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, rf_va, rf])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "rf_prediction = rf_model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "#rf_prediction.select(\"prediction\", \"label\",\"features\").show(5) \n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#metric options f1|accuracy|weightedPrecision|weightedRecall\n",
    "\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_prediction)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - rf_accuracy))\n",
    "print(\"Accuracy: \" , rf_accuracy)\n",
    "\n",
    "rfModel2 = rf_model.stages[3]\n",
    "print(rfModel2)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[290152. 119101.]\n",
      " [ 45300.  36513.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6652160809341311\n",
      "Accuracy from MulticlassMetrics:  0.6652160809341311\n",
      "\n",
      "Area Under the ROC 0.5776388982779902\n",
      "\n",
      "Area Under the PR Curve 0.2158026088843178\n",
      "Summary Stats\n",
      "Precision = 0.6652160809341311\n",
      "Recall = 0.6652160809341311\n",
      "F1 Score = 0.6652160809341311\n",
      "Weighted recall = 0.665216080934131\n",
      "Weighted precision = 0.7599452013601018\n",
      "Weighted F(1) Score = 0.7006589192079372\n",
      "Weighted F(0.5) Score = 0.7336601996843543\n",
      "Weighted false positive rate = 0.5099382843781506\n"
     ]
    }
   ],
   "source": [
    "cmacc2(rf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a GBT \n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)#.fit(data)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=5)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = gbt_pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "gbt_prediction = gbt_model.transform(test)\n",
    "\n",
    "#gbt_prediction.show(3)\n",
    "\n",
    "# Select example rows to display.\n",
    "gbt_prediction.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "gbt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "gbt_accuracy = gbt_evaluator.evaluate(gbt_prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - gbt_accuracy))\n",
    "print('gbt accuracy = ', gbt_accuracy)\n",
    "gbtModel = gbt_model.stages[3]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(gbt_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline LR with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\",\n",
    "                        labelCol=\"label\") # regParam=0.1, elasticNetParam=0.3, maxIter=10,\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(lr_paramGrid): {}'.format(len(lr_paramGrid)))\n",
    "'''\n",
    "25k set\n",
    "best from tuning inital (accuracy):\n",
    "addGrid(lr.regParam, [0.03, 0.05, 0.07]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.15, 0.2, 0.25]) \\\n",
    ".addGrid(lr.maxIter, [8, 9, 10, 11, 12]) \n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 12\n",
    " \n",
    "Confusion Matrix\n",
    "[[1842. 2203.]\n",
    " [ 193.  595.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5042416718394372\n",
    "Accuracy from MulticlassMetrics:  0.5042416718394372\n",
    "\n",
    "Area Under the ROC 0.6052265753923187\n",
    "\n",
    "Area Under the PR Curve 0.2065770273222743\n",
    "Summary Stats\n",
    "Precision = 0.5042416718394372\n",
    "Recall = 0.5042416718394372\n",
    "F1 Score = 0.5042416718394372\n",
    "Weighted recall = 0.5042416718394371\n",
    "Weighted precision = 0.7922492654683645\n",
    "Weighted F(1) Score = 0.5612342974368173\n",
    "Weighted F(0.5) Score = 0.6730989071456968\n",
    "Weighted false positive rate = 0.2937885210547999\n",
    "\n",
    "2nd round:\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.05, 0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [11, 12, 13, 15]) \\\n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 11\n",
    "\n",
    "Confusion Matrix\n",
    "[[1815. 2230.]\n",
    " [ 183.  605.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5007241878750258\n",
    "Accuracy from MulticlassMetrics:  0.5007241878750258\n",
    "\n",
    "Area Under the ROC 0.6082342994108162\n",
    "\n",
    "Area Under the PR Curve 0.2075564549699384\n",
    "Summary Stats\n",
    "Precision = 0.5007241878750258\n",
    "Recall = 0.5007241878750258\n",
    "F1 Score = 0.5007241878750258\n",
    "Weighted recall = 0.5007241878750258\n",
    "Weighted precision = 0.7950908896146499\n",
    "Weighted F(1) Score = 0.5572078454446675\n",
    "Weighted F(0.5) Score = 0.6716684076809212\n",
    "Weighted false positive rate = 0.28425558905339354\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "lr_cvModel = lr_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is... apparently it is RMSE https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf\n",
    "#lr_cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "# lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "lr_cvModel.getEstimatorParamMaps()[ np.argmin(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "lr_prediction = lr_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline RF with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for RF:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "               \n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\")\n",
    "    \n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, rf_va, rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\", \"entropy\"])\\\n",
    "    .build()\n",
    "    #.addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    \n",
    "print('len(rf_paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "#https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n",
    "#maxDepth, maxBins, impurity, auto and seed \n",
    "#.addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "#name='featureSubsetStrategy', auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 10\n",
    "impurity = gini\n",
    " \n",
    "Confusion Matrix\n",
    "[[2901. 1144.]\n",
    " [ 456.  332.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6689426857024623\n",
    "Accuracy from MulticlassMetrics:  0.6689426857024623\n",
    "\n",
    "Area Under the ROC 0.5692507513819781\n",
    "\n",
    "Area Under the PR Curve 0.2070259967551608\n",
    "Summary Stats\n",
    "Precision = 0.6689426857024623\n",
    "Recall = 0.6689426857024623\n",
    "F1 Score = 0.6689426857024623\n",
    "Weighted recall = 0.6689426857024622\n",
    "Weighted precision = 0.7599403563099746\n",
    "Weighted F(1) Score = 0.7038591473392332\n",
    "Weighted F(0.5) Score = 0.735232181263078\n",
    "Weighted false positive rate = 0.530441182938506\n",
    "\n",
    "2nd attempt:\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 15\n",
    "impurity = gini\n",
    "featureSubsetStrategy = auto\n",
    "\n",
    "Confusion Matrix\n",
    "[[2663. 1382.]\n",
    " [ 388.  400.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6337678460583489\n",
    "Accuracy from MulticlassMetrics:  0.6337678460583489\n",
    "\n",
    "Area Under the ROC 0.5829789236570811\n",
    "\n",
    "Area Under the PR Curve 0.209345437091233\n",
    "Summary Stats\n",
    "Precision = 0.6337678460583489\n",
    "Recall = 0.6337678460583489\n",
    "F1 Score = 0.6337678460583489\n",
    "Weighted recall = 0.6337678460583488\n",
    "Weighted precision = 0.7671159775546591\n",
    "Weighted F(1) Score = 0.6789410276493224\n",
    "Weighted F(0.5) Score = 0.7270236282319229\n",
    "Weighted false positive rate = 0.46780999874418644\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_rf = rf_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is... rmse\n",
    "cvModel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_rf = cvModel_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmin(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline GBT with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for gbt:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_gbt, outputCol=\"features\") \n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\") #, maxIter=5\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Set up the parameter grid\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "print('len(gbt_paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "maxIter = 20\n",
    "\n",
    "Confusion Matrix\n",
    "[[2798. 1247.]\n",
    " [ 423.  365.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.654458928201945\n",
    "Accuracy from MulticlassMetrics:  0.654458928201945\n",
    "\n",
    "Area Under the ROC 0.5774580700620556\n",
    "\n",
    "Area Under the PR Curve 0.2094152550126291\n",
    "Summary Stats\n",
    "Precision = 0.654458928201945\n",
    "Recall = 0.654458928201945\n",
    "F1 Score = 0.654458928201945\n",
    "Weighted recall = 0.654458928201945\n",
    "Weighted precision = 0.7639586098089827\n",
    "Weighted F(1) Score = 0.6941837869282138\n",
    "Weighted F(0.5) Score = 0.7327747544723259\n",
    "Weighted false positive rate = 0.4995427880778336\n",
    "\n",
    "\n",
    "maxIter = 40\n",
    "\n",
    "Confusion Matrix\n",
    "[[2571. 1474.]\n",
    " [ 382.  406.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6159735154148562\n",
    "Accuracy from MulticlassMetrics:  0.6159735154148562\n",
    "\n",
    "Area Under the ROC 0.5754139659791808\n",
    "\n",
    "Area Under the PR Curve 0.20313239804234073\n",
    "Summary Stats\n",
    "Precision = 0.6159735154148562\n",
    "Recall = 0.6159735154148562\n",
    "F1 Score = 0.6159735154148562\n",
    "Weighted recall = 0.6159735154148562\n",
    "Weighted precision = 0.7638968296438198\n",
    "Weighted F(1) Score = 0.6646010165217533\n",
    "Weighted F(0.5) Score = 0.7183436330713325\n",
    "Weighted false positive rate = 0.4651455834564943\n",
    "\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "    \n",
    "maxIter = 5\n",
    "maxDepth = 5\n",
    "\n",
    "Confusion Matrix\n",
    "[[2863. 1182.]\n",
    " [ 469.  319.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6583902338092282\n",
    "Accuracy from MulticlassMetrics:  0.6583902338092282\n",
    "\n",
    "Area Under the ROC 0.5563048634335804\n",
    "\n",
    "Area Under the PR Curve 0.19780050930331394\n",
    "Summary Stats\n",
    "Precision = 0.6583902338092282\n",
    "Recall = 0.6583902338092282\n",
    "F1 Score = 0.6583902338092282\n",
    "Weighted recall = 0.6583902338092282\n",
    "Weighted precision = 0.7537989743798752\n",
    "Weighted F(1) Score = 0.6950856095348379\n",
    "Weighted F(0.5) Score = 0.7279222226014918\n",
    "Weighted false positive rate = 0.5457805069420676\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "gbt_crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_gbt = gbt_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is...\n",
    "cvModel_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "#cvModel_gbt.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]\n",
    "\n",
    "cvModel_gbt.getEstimatorParamMaps()[np.argmax(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_gbt.getEstimatorParamMaps()[np.argmin(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_gbt = cvModel_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmacc2(prediction_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline LR with CV and no Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lr_paramGrid): 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlr_paramGrid = ParamGridBuilder()     .addGrid(lr.regParam, [0.01])     .addGrid(lr.elasticNetParam, [0.3])     .addGrid(lr.maxIter, [10])     .build()\\n\\ntrain time: 140.95259499549866\\nConfusion Matrix\\n[[205206. 204047.]\\n [ 21926.  59887.]]\\n\\nAccuracy from Confusion Matrix:  0.5398317130487551\\nAccuracy from MulticlassMetrics:  0.5398317130487551\\n\\nArea Under the ROC 0.6167072883197446\\n\\nArea Under the PR Curve 0.2188213722126973\\nSummary Stats\\nPrecision = 0.5398317130487551\\nRecall = 0.5398317130487551\\nF1 Score = 0.5398317130487551\\nWeighted recall = 0.5398317130487552\\nWeighted precision = 0.7907482614736813\\nWeighted F(1) Score = 0.5951821122400512\\nWeighted F(0.5) Score = 0.6927412010642031\\nWeighted false positive rate = 0.3064171364092659\\n\\n\\nlr_paramGrid = ParamGridBuilder()     .addGrid(lr.regParam, [0.01])     .addGrid(lr.elasticNetParam, [0.3])     .addGrid(lr.maxIter, [10])     .build()\\n\\ntrain time: 103.50798845291138\\nConfusion Matrix\\n[[190906. 218347.]\\n [ 19609.  62204.]]\\n\\nAccuracy from Confusion Matrix:  0.5154296978410234\\nAccuracy from MulticlassMetrics:  0.5154296978410234\\n\\nArea Under the ROC 0.61339677414919\\n\\nArea Under the PR Curve 0.21511547038697104\\nSummary Stats\\nPrecision = 0.5154296978410234\\nRecall = 0.5154296978410234\\nF1 Score = 0.5154296978410234\\nWeighted recall = 0.5154296978410234\\nWeighted precision = 0.792707390100297\\nWeighted F(1) Score = 0.5706182258743312\\nWeighted F(0.5) Score = 0.678770901909123\\nWeighted false positive rate = 0.2886361495426433\\n\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\",\n",
    "                        labelCol=\"label\") # regParam=0.1, elasticNetParam=0.3, maxIter=10,\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.005, 0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.2]) \\\n",
    "    .addGrid(lr.maxIter, [10]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(lr_paramGrid): {}'.format(len(lr_paramGrid)))\n",
    "'''\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.3]) \\\n",
    "    .addGrid(lr.maxIter, [10]) \\\n",
    "    .build()\n",
    "\n",
    "train time: 140.95259499549866\n",
    "Confusion Matrix\n",
    "[[205206. 204047.]\n",
    " [ 21926.  59887.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5398317130487551\n",
    "Accuracy from MulticlassMetrics:  0.5398317130487551\n",
    "\n",
    "Area Under the ROC 0.6167072883197446\n",
    "\n",
    "Area Under the PR Curve 0.2188213722126973\n",
    "Summary Stats\n",
    "Precision = 0.5398317130487551\n",
    "Recall = 0.5398317130487551\n",
    "F1 Score = 0.5398317130487551\n",
    "Weighted recall = 0.5398317130487552\n",
    "Weighted precision = 0.7907482614736813\n",
    "Weighted F(1) Score = 0.5951821122400512\n",
    "Weighted F(0.5) Score = 0.6927412010642031\n",
    "Weighted false positive rate = 0.3064171364092659\n",
    "\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.3]) \\\n",
    "    .addGrid(lr.maxIter, [10]) \\\n",
    "    .build()\n",
    "\n",
    "train time: 103.50798845291138\n",
    "Confusion Matrix\n",
    "[[190906. 218347.]\n",
    " [ 19609.  62204.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5154296978410234\n",
    "Accuracy from MulticlassMetrics:  0.5154296978410234\n",
    "\n",
    "Area Under the ROC 0.61339677414919\n",
    "\n",
    "Area Under the PR Curve 0.21511547038697104\n",
    "Summary Stats\n",
    "Precision = 0.5154296978410234\n",
    "Recall = 0.5154296978410234\n",
    "F1 Score = 0.5154296978410234\n",
    "Weighted recall = 0.5154296978410234\n",
    "Weighted precision = 0.792707390100297\n",
    "Weighted F(1) Score = 0.5706182258743312\n",
    "Weighted F(0.5) Score = 0.678770901909123\n",
    "Weighted false positive rate = 0.2886361495426433\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1003.2818486690521\n",
      "Confusion Matrix\n",
      "[[219819. 189434.]\n",
      " [ 24523.  57290.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.5643009289993606\n",
      "Accuracy from MulticlassMetrics:  0.5643009289993606\n",
      "\n",
      "Area Under the ROC 0.6186889870405314\n",
      "\n",
      "Area Under the PR Curve 0.22237117558780095\n",
      "Summary Stats\n",
      "Precision = 0.5643009289993606\n",
      "Recall = 0.5643009289993606\n",
      "F1 Score = 0.5643009289993606\n",
      "Weighted recall = 0.5643009289993606\n",
      "Weighted precision = 0.7884401968118694\n",
      "Weighted F(1) Score = 0.6186853005221369\n",
      "Weighted F(0.5) Score = 0.7052413279806726\n",
      "Weighted false positive rate = 0.3269229549182977\n"
     ]
    }
   ],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "lr_cvModel = lr_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "lr_prediction = lr_cvModel.transform(test)\n",
    "\n",
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "[[312836.  96417.]\n",
    " [ 51813.  30000.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6981464813283754\n",
    "Accuracy from MulticlassMetrics:  0.6981464813283754\n",
    "\n",
    "Area Under the ROC 0.565548618702108\n",
    "\n",
    "Area Under the PR Curve 0.21492012791750828\n",
    "Summary Stats\n",
    "Precision = 0.6981464813283754\n",
    "Recall = 0.6981464813283754\n",
    "F1 Score = 0.6981464813283754\n",
    "Weighted recall = 0.6981464813283754\n",
    "Weighted precision = 0.7545161871534753\n",
    "Weighted F(1) Score = 0.7217771204779072\n",
    "Weighted F(0.5) Score = 0.7404443373565394\n",
    "Weighted false positive rate = 0.5670492439241593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline RF with CV and no Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rf_paramGrid): 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [5])     .addGrid(rf.maxDepth, [10])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n\\nConfusion Matrix\\n[[310741.  98512.]\\n [ 48819.  32994.]]\\n\\nAccuracy from Confusion Matrix:  0.6999771924751459\\nAccuracy from MulticlassMetrics:  0.6999771924751459\\n\\nArea Under the ROC 0.5812869028226872\\n\\nArea Under the PR Curve 0.22574477490483819\\nSummary Stats\\nPrecision = 0.6999771924751459\\nRecall = 0.6999771924751459\\nF1 Score = 0.6999771924751459\\nWeighted recall = 0.6999771924751459\\nWeighted precision = 0.7620428175754068\\nWeighted F(1) Score = 0.725226449674273\\nWeighted F(0.5) Score = 0.746087327758354\\nWeighted false positive rate = 0.5374033868297713\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [5, 10])     .addGrid(rf.maxDepth, [10])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n    #.addGrid(rf.featureSubsetStrategy, [\\'auto\\', \\'sqrt\\'])\\nConfusion Matrix\\n[[314738.  94515.]\\n [ 49387.  32426.]]\\n\\nAccuracy from Confusion Matrix:  0.7069599605755642\\nAccuracy from MulticlassMetrics:  0.7069599605755642\\n\\nArea Under the ROC 0.5826988592158698\\n\\nArea Under the PR Curve 0.22862746418070495\\nSummary Stats\\nPrecision = 0.7069599605755642\\nRecall = 0.7069599605755642\\nF1 Score = 0.7069599605755642\\nWeighted recall = 0.7069599605755641\\nWeighted precision = 0.7629191089280811\\nWeighted F(1) Score = 0.7300846426136949\\nWeighted F(0.5) Score = 0.7487527818041555\\nWeighted false positive rate = 0.5415622421438244\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [10])     .addGrid(rf.maxDepth, [5, 10])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n    \\nConfusion Matrix\\n[[314738.  94515.]\\n [ 49387.  32426.]]\\n\\nAccuracy from Confusion Matrix:  0.7069599605755642\\nAccuracy from MulticlassMetrics:  0.7069599605755642\\n\\nArea Under the ROC 0.5826988592158698\\n\\nArea Under the PR Curve 0.22862746418070495\\nSummary Stats\\nPrecision = 0.7069599605755642\\nRecall = 0.7069599605755642\\nF1 Score = 0.7069599605755642\\nWeighted recall = 0.7069599605755641\\nWeighted precision = 0.7629191089280811\\nWeighted F(1) Score = 0.7300846426136949\\nWeighted F(0.5) Score = 0.7487527818041555\\nWeighted false positive rate = 0.5415622421438244\\n\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [15])     .addGrid(rf.maxDepth, [10])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n\\nConfusion Matrix\\n[[314741.  94512.]\\n [ 49347.  32466.]]\\n\\nAccuracy from Confusion Matrix:  0.7070475251799148\\nAccuracy from MulticlassMetrics:  0.7070475251799148\\n\\nArea Under the ROC 0.5829469843572036\\n\\nArea Under the PR Curve 0.22881720820050683\\nSummary Stats\\nPrecision = 0.7070475251799148\\nRecall = 0.7070475251799148\\nF1 Score = 0.7070475251799148\\nWeighted recall = 0.7070475251799148\\nWeighted precision = 0.7630392635253729\\nWeighted F(1) Score = 0.7301782260281282\\nWeighted F(0.5) Score = 0.7488602512258918\\nWeighted false positive rate = 0.5411535564655076\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [15])     .addGrid(rf.maxDepth, [15])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n\\nConfusion Matrix\\n[[283025. 126228.]\\n [ 40908.  40905.]]\\n\\nAccuracy from Confusion Matrix:  0.6596465648202074\\nAccuracy from MulticlassMetrics:  0.6596465648202074\\n\\nArea Under the ROC 0.5957732705112914\\n\\nArea Under the PR Curve 0.22520890081666428\\nSummary Stats\\nPrecision = 0.6596465648202074\\nRecall = 0.6596465648202074\\nF1 Score = 0.6596465648202074\\nWeighted recall = 0.6596465648202074\\nWeighted precision = 0.76892652146907\\nWeighted F(1) Score = 0.6981671019368805\\nWeighted F(0.5) Score = 0.7371252023470279\\nWeighted false positive rate = 0.4681000237976247\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [15])     .addGrid(rf.maxDepth, [5])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n\\nConfusion Matrix\\n[[309640.  99613.]\\n [ 49083.  32730.]]\\n\\nAccuracy from Confusion Matrix:  0.6971975253835533\\nAccuracy from MulticlassMetrics:  0.6971975253835533\\n\\nArea Under the ROC 0.5783283336102997\\n\\nArea Under the PR Curve 0.22310156202954437\\nSummary Stats\\nPrecision = 0.6971975253835533\\nRecall = 0.6971975253835533\\nF1 Score = 0.6971975253835533\\nWeighted recall = 0.6971975253835534\\nWeighted precision = 0.7605687622026887\\nWeighted F(1) Score = 0.7229589255192178\\nWeighted F(0.5) Score = 0.7442644365033869\\nWeighted false positive rate = 0.540540858162954\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [20])     .addGrid(rf.maxDepth, [10])     .addGrid(rf.impurity, [\"gini\"])    .build()\\n\\nConfusion Matrix\\n[[287361. 121892.]\\n [ 43803.  38010.]]\\n\\nAccuracy from Confusion Matrix:  0.6625809972590242\\nAccuracy from MulticlassMetrics:  0.6625809972590242\\n\\nArea Under the ROC 0.5833779398869698\\n\\nArea Under the PR Curve 0.21867308547708184\\nSummary Stats\\nPrecision = 0.6625809972590242\\nRecall = 0.6625809972590242\\nF1 Score = 0.6625809972590242\\nWeighted recall = 0.6625809972590243\\nWeighted precision = 0.7627667057602532\\nWeighted F(1) Score = 0.6992915166799253\\nWeighted F(0.5) Score = 0.7344847606612861\\nWeighted false positive rate = 0.4958251174850846\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline steps for Random Forest:\n",
    "\n",
    "# #onehotencoder to pickup\n",
    "# ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "# #onehotencoder to dropoff\n",
    "# ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# # our colulms for rf\n",
    "# predictor_col_for_rf = ['Trip_Seconds',\n",
    "#                         'Trip_Miles',\n",
    "#                         'Fare',\n",
    "#                         'Additional_Charges',\n",
    "#                         'Shared_Trip_Authorized',\n",
    "#                         'Trips_Pooled',\n",
    "#                         'Pickup_Community_Area_vec',\n",
    "#                         'Dropoff_Community_Area_vec']\n",
    "\n",
    "# pipeline steps for gbt:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ]\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\")\n",
    "\n",
    "# # Build the pipeline\n",
    "# rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, rf_va, rf])\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, rf_va, rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10]) \\\n",
    "    .addGrid(rf.maxDepth, [10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "    #.addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    \n",
    "\n",
    "print('len(rf_paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "#https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n",
    "#maxDepth, maxBins, impurity, auto and seed \n",
    "#.addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "#name='featureSubsetStrategy', auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]\n",
    "\n",
    "'''\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5]) \\\n",
    "    .addGrid(rf.maxDepth, [10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "Confusion Matrix\n",
    "[[310741.  98512.]\n",
    " [ 48819.  32994.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6999771924751459\n",
    "Accuracy from MulticlassMetrics:  0.6999771924751459\n",
    "\n",
    "Area Under the ROC 0.5812869028226872\n",
    "\n",
    "Area Under the PR Curve 0.22574477490483819\n",
    "Summary Stats\n",
    "Precision = 0.6999771924751459\n",
    "Recall = 0.6999771924751459\n",
    "F1 Score = 0.6999771924751459\n",
    "Weighted recall = 0.6999771924751459\n",
    "Weighted precision = 0.7620428175754068\n",
    "Weighted F(1) Score = 0.725226449674273\n",
    "Weighted F(0.5) Score = 0.746087327758354\n",
    "Weighted false positive rate = 0.5374033868297713\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "    #.addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "\n",
    "Confusion Matrix\n",
    "[[314738.  94515.]\n",
    " [ 49387.  32426.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.7069599605755642\n",
    "Accuracy from MulticlassMetrics:  0.7069599605755642\n",
    "\n",
    "Area Under the ROC 0.5826988592158698\n",
    "\n",
    "Area Under the PR Curve 0.22862746418070495\n",
    "Summary Stats\n",
    "Precision = 0.7069599605755642\n",
    "Recall = 0.7069599605755642\n",
    "F1 Score = 0.7069599605755642\n",
    "Weighted recall = 0.7069599605755641\n",
    "Weighted precision = 0.7629191089280811\n",
    "Weighted F(1) Score = 0.7300846426136949\n",
    "Weighted F(0.5) Score = 0.7487527818041555\n",
    "Weighted false positive rate = 0.5415622421438244\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "    \n",
    "Confusion Matrix\n",
    "[[314738.  94515.]\n",
    " [ 49387.  32426.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.7069599605755642\n",
    "Accuracy from MulticlassMetrics:  0.7069599605755642\n",
    "\n",
    "Area Under the ROC 0.5826988592158698\n",
    "\n",
    "Area Under the PR Curve 0.22862746418070495\n",
    "Summary Stats\n",
    "Precision = 0.7069599605755642\n",
    "Recall = 0.7069599605755642\n",
    "F1 Score = 0.7069599605755642\n",
    "Weighted recall = 0.7069599605755641\n",
    "Weighted precision = 0.7629191089280811\n",
    "Weighted F(1) Score = 0.7300846426136949\n",
    "Weighted F(0.5) Score = 0.7487527818041555\n",
    "Weighted false positive rate = 0.5415622421438244\n",
    "\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [15]) \\\n",
    "    .addGrid(rf.maxDepth, [10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "Confusion Matrix\n",
    "[[314741.  94512.]\n",
    " [ 49347.  32466.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.7070475251799148\n",
    "Accuracy from MulticlassMetrics:  0.7070475251799148\n",
    "\n",
    "Area Under the ROC 0.5829469843572036\n",
    "\n",
    "Area Under the PR Curve 0.22881720820050683\n",
    "Summary Stats\n",
    "Precision = 0.7070475251799148\n",
    "Recall = 0.7070475251799148\n",
    "F1 Score = 0.7070475251799148\n",
    "Weighted recall = 0.7070475251799148\n",
    "Weighted precision = 0.7630392635253729\n",
    "Weighted F(1) Score = 0.7301782260281282\n",
    "Weighted F(0.5) Score = 0.7488602512258918\n",
    "Weighted false positive rate = 0.5411535564655076\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [15]) \\\n",
    "    .addGrid(rf.maxDepth, [15]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "Confusion Matrix\n",
    "[[283025. 126228.]\n",
    " [ 40908.  40905.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6596465648202074\n",
    "Accuracy from MulticlassMetrics:  0.6596465648202074\n",
    "\n",
    "Area Under the ROC 0.5957732705112914\n",
    "\n",
    "Area Under the PR Curve 0.22520890081666428\n",
    "Summary Stats\n",
    "Precision = 0.6596465648202074\n",
    "Recall = 0.6596465648202074\n",
    "F1 Score = 0.6596465648202074\n",
    "Weighted recall = 0.6596465648202074\n",
    "Weighted precision = 0.76892652146907\n",
    "Weighted F(1) Score = 0.6981671019368805\n",
    "Weighted F(0.5) Score = 0.7371252023470279\n",
    "Weighted false positive rate = 0.4681000237976247\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [15]) \\\n",
    "    .addGrid(rf.maxDepth, [5]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "Confusion Matrix\n",
    "[[309640.  99613.]\n",
    " [ 49083.  32730.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6971975253835533\n",
    "Accuracy from MulticlassMetrics:  0.6971975253835533\n",
    "\n",
    "Area Under the ROC 0.5783283336102997\n",
    "\n",
    "Area Under the PR Curve 0.22310156202954437\n",
    "Summary Stats\n",
    "Precision = 0.6971975253835533\n",
    "Recall = 0.6971975253835533\n",
    "F1 Score = 0.6971975253835533\n",
    "Weighted recall = 0.6971975253835534\n",
    "Weighted precision = 0.7605687622026887\n",
    "Weighted F(1) Score = 0.7229589255192178\n",
    "Weighted F(0.5) Score = 0.7442644365033869\n",
    "Weighted false positive rate = 0.540540858162954\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20]) \\\n",
    "    .addGrid(rf.maxDepth, [10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "Confusion Matrix\n",
    "[[287361. 121892.]\n",
    " [ 43803.  38010.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6625809972590242\n",
    "Accuracy from MulticlassMetrics:  0.6625809972590242\n",
    "\n",
    "Area Under the ROC 0.5833779398869698\n",
    "\n",
    "Area Under the PR Curve 0.21867308547708184\n",
    "Summary Stats\n",
    "Precision = 0.6625809972590242\n",
    "Recall = 0.6625809972590242\n",
    "F1 Score = 0.6625809972590242\n",
    "Weighted recall = 0.6625809972590243\n",
    "Weighted precision = 0.7627667057602532\n",
    "Weighted F(1) Score = 0.6992915166799253\n",
    "Weighted F(0.5) Score = 0.7344847606612861\n",
    "Weighted false positive rate = 0.4958251174850846\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 329.7941288948059\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_rf = rf_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.588006874360152]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not sure what this metric is... rmse\n",
    "cvModel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_9615177f3ecf', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_rf = cvModel_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[264123. 145130.]\n",
      " [ 37774.  44039.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6275368280434809\n",
      "Accuracy from MulticlassMetrics:  0.6275368280434809\n",
      "\n",
      "Area Under the ROC 0.5918334114163827\n",
      "\n",
      "Area Under the PR Curve 0.21751986807580723\n",
      "Summary Stats\n",
      "Precision = 0.6275368280434809\n",
      "Recall = 0.6275368280434809\n",
      "F1 Score = 0.6275368280434809\n",
      "Weighted recall = 0.627536828043481\n",
      "Weighted precision = 0.7679062509631377\n",
      "Weighted F(1) Score = 0.6732031960000059\n",
      "Weighted F(0.5) Score = 0.7244595579778885\n",
      "Weighted false positive rate = 0.44387000521071557\n"
     ]
    }
   ],
   "source": [
    "cmacc2(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_9615177f3ecf', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmin(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_9615177f3ecf', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_9615177f3ecf', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
