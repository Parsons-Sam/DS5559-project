{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '20g') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.instances', '3') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark DF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = spark.read.csv(\"/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: string (nullable = true)\n",
      " |-- Trip Miles: string (nullable = true)\n",
      " |-- Pickup Census Tract: string (nullable = true)\n",
      " |-- Dropoff Census Tract: string (nullable = true)\n",
      " |-- Pickup Community Area: string (nullable = true)\n",
      " |-- Dropoff Community Area: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Tip: string (nullable = true)\n",
      " |-- Additional Charges: string (nullable = true)\n",
      " |-- Trip Total: string (nullable = true)\n",
      " |-- Shared Trip Authorized: string (nullable = true)\n",
      " |-- Trips Pooled: string (nullable = true)\n",
      " |-- Pickup Centroid Latitude: string (nullable = true)\n",
      " |-- Pickup Centroid Longitude: string (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: string (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: string (nullable = true)\n",
      " |-- Dropoff Centroid Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data file is ~12 gigabytes. Take a 25% sample, stratified by sample, to get a 3 gigabyte file. Commented out line is for testing smaller subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip ID: string, Trip Start Timestamp: string, Trip End Timestamp: string, Trip Seconds: string, Trip Miles: string, Pickup Census Tract: string, Dropoff Census Tract: string, Pickup Community Area: string, Dropoff Community Area: string, Fare: string, Tip: string, Additional Charges: string, Trip Total: string, Shared Trip Authorized: string, Trips Pooled: string, Pickup Centroid Latitude: string, Pickup Centroid Longitude: string, Pickup Centroid Location: string, Dropoff Centroid Latitude: string, Dropoff Centroid Longitude: string, Dropoff Centroid Location: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_DF = data_file.sample(0.25).sample(0.0001)\n",
    "final_DF = data_file\n",
    "del(data_file)\n",
    "final_DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53304688/spark-date-format-mmm-dd-yyyy-hhmmss-am-to-timestamp-in-df\n",
    "#https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "#start_times = final_DF.select(\"Trip ID\",\"Trip Start Timestamp\")\n",
    "#st = start_times.withColumn(\"Trip_Start_Timestamp\",F.to_timestamp(F.col(\"Trip Start Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "#end_times = final_DF.select(\"Trip ID\",\"Trip End Timestamp\")\n",
    "#et = end_times.withColumn(\"Trip_End_Timestamp\",F.to_timestamp(F.col(\"Trip End Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "final_DF = final_DF.withColumn(\"Trip_Start_Timestamp\",F.to_timestamp(F.col(\"Trip Start Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "final_DF = final_DF.withColumn(\"Trip_End_Timestamp\",F.to_timestamp(F.col(\"Trip End Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "#https://stackoverflow.com/questions/29600673/how-to-delete-columns-in-pyspark-dataframe\n",
    "final_DF = final_DF.drop(*['Trip Start Timestamp','Trip End Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF.createOrReplaceTempView(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = spark.sql(\"\"\"SELECT *, (CASE WHEN Trip_Start_Timestamp >= CAST('2020-03-11' AS DATE)\n",
    "                                                        THEN 1\n",
    "                                                        ELSE 0\n",
    "                                                END) as PostShutdownFlag\n",
    "                                     FROM dataset\n",
    "                                    WHERE Trip_Start_Timestamp BETWEEN CAST('2020-02-05' AS DATE) AND CAST('2020-04-15' AS DATE)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_DF = final_DF.join(st.select(\"Trip ID\",\"Trip_Start_Timestamp\"),on=\"Trip ID\").join(et.select(\"Trip ID\",\"Trip_End_Timestamp\"),on=\"Trip ID\")\n",
    "#https://stackoverflow.com/questions/49397966/in-pyspark-how-do-you-add-concat-a-string-to-a-column\n",
    "final_DF = final_DF.withColumn(\"Day_Month_str\", F.concat(F.dayofmonth(F.col(\"Trip_Start_Timestamp\")),F.lit(\"-\"),F.month(F.col(\"Trip_Start_Timestamp\"))).cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.withColumn('Trip Seconds',F.col('Trip Seconds').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Trip Miles',F.col('Trip Miles').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Pickup Community Area',F.col('Pickup Community Area').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Dropoff Community Area',F.col('Dropoff Community Area').cast(\"integer\"))\n",
    "final_DF = final_DF.withColumn('Fare',F.col('Fare').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Tip',F.col('Tip').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Additional Charges',F.col('Additional Charges').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Trip Total',F.col('Trip Total').cast(\"double\"))\n",
    "final_DF = final_DF.withColumn('Shared Trip Authorized',F.col('Shared Trip Authorized').cast(\"boolean\"))\n",
    "final_DF = final_DF.withColumn('Trips Pooled',F.col('Trips Pooled').cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.withColumn(\"label\", F.when(F.col(\"Tip\") > 0,1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\n",
    "final_DF = final_DF.withColumn(\"Trip_Year\", F.year(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_Month\", F.month(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_WeekNumber\", F.weekofyear(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_DayofWeek\", F.dayofweek(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_Start_Hour\", F.hour(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_Start_Minute\", F.minute(F.col(\"Trip_Start_Timestamp\"))) \\\n",
    "                    .withColumn(\"Trip_End_Hour\", F.hour(F.col(\"Trip_End_Timestamp\"))) \\\n",
    "                    .withColumn(\"Date\", F.to_date(F.col(\"Trip_Start_Timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF = final_DF.withColumnRenamed(\"Trip ID\",\"Trip_ID\") \\\n",
    "                    .withColumnRenamed(\"Trip Seconds\",\"Trip_Seconds\") \\\n",
    "                    .withColumnRenamed(\"Trip Miles\",\"Trip_Miles\") \\\n",
    "                    .withColumnRenamed(\"Pickup Census Tract\",\"Pickup_Census_Tract\") \\\n",
    "                    .withColumnRenamed(\"Dropoff Census Tract\",\"Dropoff_Census_Tract\") \\\n",
    "                    .withColumnRenamed(\"Pickup Community Area\",\"Pickup_Community_Area\") \\\n",
    "                    .withColumnRenamed(\"Dropoff Community Area\",\"Dropoff_Community_Area\") \\\n",
    "                    .withColumnRenamed(\"Additional Charges\",\"Additional_Charges_str\") \\\n",
    "                    .withColumnRenamed(\"Trip Total\",\"Trip_Total\") \\\n",
    "                    .withColumnRenamed(\"Shared Trip Authorized\",\"Shared_Trip_Authorized\") \\\n",
    "                    .withColumnRenamed(\"Trips Pooled\",\"Trips_Pooled\") \\\n",
    "                    .withColumnRenamed(\"Pickup Centroid Latitude\",\"Pickup_Centroid_Latitude\") \\\n",
    "                    .withColumnRenamed(\"Pickup Centroid Longitude\",\"Pickup_Centroid_Longitude\") \\\n",
    "                    .withColumnRenamed(\"Pickup Centroid Location\",\"Pickup_Centroid_Location\") \\\n",
    "                    .withColumnRenamed(\"Dropoff Centroid Latitude\",\"Dropoff_Centroid_Latitude\") \\\n",
    "                    .withColumnRenamed(\"Dropoff Centroid Longitude\",\"Dropoff_Centroid_Longitude\") \\\n",
    "                    .withColumnRenamed(\"Dropoff Centroid Location\",\"Dropoff_Centroid_Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.13 ms, sys: 4.68 ms, total: 8.81 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_records_count = final_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Seconds: integer (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Census_Tract: string (nullable = true)\n",
      " |-- Dropoff_Census_Tract: string (nullable = true)\n",
      " |-- Pickup_Community_Area: integer (nullable = true)\n",
      " |-- Dropoff_Community_Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges_str: double (nullable = true)\n",
      " |-- Trip_Total: double (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: integer (nullable = true)\n",
      " |-- Pickup_Centroid_Latitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Longitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Location: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Latitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Longitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Location: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: timestamp (nullable = true)\n",
      " |-- Trip_End_Timestamp: timestamp (nullable = true)\n",
      " |-- PostShutdownFlag: integer (nullable = false)\n",
      " |-- Day_Month_str: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_Start_Minute: integer (nullable = true)\n",
      " |-- Trip_End_Hour: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_record_count = final_DF.filter(F.col('label') == 1).count()\n",
    "final_DF = final_DF.filter(F.col('label') == 1) \\\n",
    "                .union(final_DF.filter(F.col('label') == 0)\n",
    "                       .sample(False, (tip_record_count/(all_records_count-tip_record_count)), 1221))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|2375183|\n",
      "|    0|2375686|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_DF.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 ms, sys: 10.6 ms, total: 23.7 ms\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_DF.write.parquet(\"/../../project/ds5559/Alice_Ed_Michael_Sam_project/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.5 ms, sys: 364 µs, total: 1.87 ms\n",
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = spark.read.parquet(\"/../../project/ds5559/Alice_Ed_Michael_Sam_project/final_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750869"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Subselection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a few helpful aggregated and sampled datasets to feed into our EDA notebook. First a 1% sample for scatterplots and boxplots. This will be approximately 47,500 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sample = final_DF.sample(False,.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46950"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+--------------+\n",
      "|Trip_Start_Timestamp|sum(label)|sum(Fare)|count(trip_id)|\n",
      "+--------------------+----------+---------+--------------+\n",
      "| 2020-02-07 23:00:00|       832|  17307.5|          1697|\n",
      "| 2020-02-13 10:00:00|       762|  16567.5|          1363|\n",
      "| 2020-02-13 19:15:00|      1297|  27317.5|          2334|\n",
      "| 2020-02-16 10:15:00|       675|  16420.0|          1261|\n",
      "| 2020-02-17 07:15:00|       425|  13635.0|           888|\n",
      "+--------------------+----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_summary = final_DF.groupBy('Trip_Start_Timestamp').agg(F.sum(\"label\"),F.sum(\"Fare\"), F.count(\"trip_id\"))\n",
    "daily_summary.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559 Spark 3",
   "language": "python",
   "name": "ds5559_spark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
