{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DS5559 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '21g') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.instances', '3') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# import data manipulation methods\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "#from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                            Type            Data/Info\n",
      "-------------------------------------------------------------\n",
      "ArrayType                           type            <class 'pyspark.sql.types.ArrayType'>\n",
      "Binarizer                           type            <class 'pyspark.ml.feature.Binarizer'>\n",
      "BinaryClassificationEvaluator       type            <class 'pyspark.ml.evalua<...>ClassificationEvaluator'>\n",
      "BinaryClassificationMetrics         type            <class 'pyspark.mllib.eva<...>ryClassificationMetrics'>\n",
      "BinaryType                          type            <class 'pyspark.sql.types.BinaryType'>\n",
      "BooleanType                         type            <class 'pyspark.sql.types.BooleanType'>\n",
      "BucketedRandomProjectionLSH         type            <class 'pyspark.ml.featur<...>etedRandomProjectionLSH'>\n",
      "BucketedRandomProjectionLSHModel    type            <class 'pyspark.ml.featur<...>andomProjectionLSHModel'>\n",
      "Bucketizer                          type            <class 'pyspark.ml.feature.Bucketizer'>\n",
      "ByteType                            type            <class 'pyspark.sql.types.ByteType'>\n",
      "ChiSqSelector                       type            <class 'pyspark.ml.feature.ChiSqSelector'>\n",
      "ChiSqSelectorModel                  type            <class 'pyspark.ml.feature.ChiSqSelectorModel'>\n",
      "CountVectorizer                     type            <class 'pyspark.ml.feature.CountVectorizer'>\n",
      "CountVectorizerModel                type            <class 'pyspark.ml.feature.CountVectorizerModel'>\n",
      "CrossValidator                      type            <class 'pyspark.ml.tuning.CrossValidator'>\n",
      "DCT                                 type            <class 'pyspark.ml.feature.DCT'>\n",
      "DataType                            type            <class 'pyspark.sql.types.DataType'>\n",
      "DateType                            type            <class 'pyspark.sql.types.DateType'>\n",
      "DecimalType                         type            <class 'pyspark.sql.types.DecimalType'>\n",
      "DenseVector                         type            <class 'pyspark.ml.linalg.DenseVector'>\n",
      "DoubleType                          type            <class 'pyspark.sql.types.DoubleType'>\n",
      "ElementwiseProduct                  type            <class 'pyspark.ml.feature.ElementwiseProduct'>\n",
      "F                                   module          <module 'pyspark.sql.func<...>yspark/sql/functions.py'>\n",
      "FeatureHasher                       type            <class 'pyspark.ml.feature.FeatureHasher'>\n",
      "FloatType                           type            <class 'pyspark.sql.types.FloatType'>\n",
      "GBTClassifier                       type            <class 'pyspark.ml.classification.GBTClassifier'>\n",
      "HashingTF                           type            <class 'pyspark.ml.feature.HashingTF'>\n",
      "IDF                                 type            <class 'pyspark.ml.feature.IDF'>\n",
      "IDFModel                            type            <class 'pyspark.ml.feature.IDFModel'>\n",
      "Imputer                             type            <class 'pyspark.ml.feature.Imputer'>\n",
      "ImputerModel                        type            <class 'pyspark.ml.feature.ImputerModel'>\n",
      "IndexToString                       type            <class 'pyspark.ml.feature.IndexToString'>\n",
      "IntegerType                         type            <class 'pyspark.sql.types.IntegerType'>\n",
      "LogisticRegression                  type            <class 'pyspark.ml.classi<...>tion.LogisticRegression'>\n",
      "LongType                            type            <class 'pyspark.sql.types.LongType'>\n",
      "MapType                             type            <class 'pyspark.sql.types.MapType'>\n",
      "MaxAbsScaler                        type            <class 'pyspark.ml.feature.MaxAbsScaler'>\n",
      "MaxAbsScalerModel                   type            <class 'pyspark.ml.feature.MaxAbsScalerModel'>\n",
      "MinHashLSH                          type            <class 'pyspark.ml.feature.MinHashLSH'>\n",
      "MinHashLSHModel                     type            <class 'pyspark.ml.feature.MinHashLSHModel'>\n",
      "MinMaxScaler                        type            <class 'pyspark.ml.feature.MinMaxScaler'>\n",
      "MinMaxScalerModel                   type            <class 'pyspark.ml.feature.MinMaxScalerModel'>\n",
      "MulticlassClassificationEvaluator   type            <class 'pyspark.ml.evalua<...>ClassificationEvaluator'>\n",
      "MulticlassMetrics                   type            <class 'pyspark.mllib.eva<...>ation.MulticlassMetrics'>\n",
      "NGram                               type            <class 'pyspark.ml.feature.NGram'>\n",
      "Normalizer                          type            <class 'pyspark.ml.feature.Normalizer'>\n",
      "NullType                            type            <class 'pyspark.sql.types.NullType'>\n",
      "OneHotEncoder                       type            <class 'pyspark.ml.feature.OneHotEncoder'>\n",
      "OneHotEncoderEstimator              type            <class 'pyspark.ml.featur<...>.OneHotEncoderEstimator'>\n",
      "OneHotEncoderModel                  type            <class 'pyspark.ml.feature.OneHotEncoderModel'>\n",
      "PCA                                 type            <class 'pyspark.ml.feature.PCA'>\n",
      "PCAModel                            type            <class 'pyspark.ml.feature.PCAModel'>\n",
      "ParamGridBuilder                    type            <class 'pyspark.ml.tuning.ParamGridBuilder'>\n",
      "Pipeline                            type            <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "PolynomialExpansion                 type            <class 'pyspark.ml.feature.PolynomialExpansion'>\n",
      "QuantileDiscretizer                 type            <class 'pyspark.ml.feature.QuantileDiscretizer'>\n",
      "RFormula                            type            <class 'pyspark.ml.feature.RFormula'>\n",
      "RFormulaModel                       type            <class 'pyspark.ml.feature.RFormulaModel'>\n",
      "RandomForestClassifier              type            <class 'pyspark.ml.classi<...>.RandomForestClassifier'>\n",
      "RegexTokenizer                      type            <class 'pyspark.ml.feature.RegexTokenizer'>\n",
      "SQLTransformer                      type            <class 'pyspark.ml.feature.SQLTransformer'>\n",
      "ShortType                           type            <class 'pyspark.sql.types.ShortType'>\n",
      "SparkSession                        type            <class 'pyspark.sql.session.SparkSession'>\n",
      "StandardScaler                      type            <class 'pyspark.ml.feature.StandardScaler'>\n",
      "StandardScalerModel                 type            <class 'pyspark.ml.feature.StandardScalerModel'>\n",
      "StopWordsRemover                    type            <class 'pyspark.ml.feature.StopWordsRemover'>\n",
      "StringIndexer                       type            <class 'pyspark.ml.feature.StringIndexer'>\n",
      "StringIndexerModel                  type            <class 'pyspark.ml.feature.StringIndexerModel'>\n",
      "StringType                          type            <class 'pyspark.sql.types.StringType'>\n",
      "StructField                         type            <class 'pyspark.sql.types.StructField'>\n",
      "StructType                          type            <class 'pyspark.sql.types.StructType'>\n",
      "TimestampType                       type            <class 'pyspark.sql.types.TimestampType'>\n",
      "Tokenizer                           type            <class 'pyspark.ml.feature.Tokenizer'>\n",
      "VectorAssembler                     type            <class 'pyspark.ml.feature.VectorAssembler'>\n",
      "VectorIndexer                       type            <class 'pyspark.ml.feature.VectorIndexer'>\n",
      "VectorIndexerModel                  type            <class 'pyspark.ml.feature.VectorIndexerModel'>\n",
      "VectorSizeHint                      type            <class 'pyspark.ml.feature.VectorSizeHint'>\n",
      "VectorSlicer                        type            <class 'pyspark.ml.feature.VectorSlicer'>\n",
      "Vectors                             type            <class 'pyspark.mllib.linalg.Vectors'>\n",
      "Word2Vec                            type            <class 'pyspark.ml.feature.Word2Vec'>\n",
      "Word2VecModel                       type            <class 'pyspark.ml.feature.Word2VecModel'>\n",
      "np                                  module          <module 'numpy' from '/op<...>kages/numpy/__init__.py'>\n",
      "os                                  module          <module 'os' from '/opt/c<...>nda/lib/python3.7/os.py'>\n",
      "sc                                  SparkContext    <SparkContext master=loca<...>appName=mllib_classifier>\n",
      "spark                               SparkSession    <pyspark.sql.session.Spar<...>object at 0x7f33be6b8290>\n",
      "time                                module          <module 'time' (built-in)>\n",
      "typ                                 module          <module 'pyspark.sql.type<...>on/pyspark/sql/types.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear old df\n",
    "#del (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Schema.  \n",
    "This schema was been primarly determined by using a much smaller dataset and letting spark infer the schema.  We encountered an issue with spark reading in the ENTIRE dataset as NULL when there was a type mismatch.  Only the data we are likely to use later has been assigned to a specific type, otherwise it is left as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Census_Tract|Dropoff_Census_Tract|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Pickup_Centroid_Latitude|Pickup_Centroid_Longitude|Pickup_Centroid_Location|Dropoff_Centroid_Latitude|Dropoff_Centroid_Longitude|Dropoff_Centroid_Location|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...|12/01/2019 01:00:...|      2675.0|      32.5|        17031980000|                null|                 76.0|                  null|70.0|0.0|              9.06|     79.06|                 false|         1.0|           41.9790708201|           -87.9030396611|    POINT (-87.903039...|                     null|                      null|                     null|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...|12/01/2019 12:15:...|       550.0|       2.8|        17031242100|         17031081700|                 24.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8996701799|           -87.6698377982|    POINT (-87.669837...|            41.8920421365|            -87.6318639497|     POINT (-87.631863...|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...|12/01/2019 12:30:...|       922.0|       2.8|        17031080300|         17031281900|                  8.0|                  28.0|10.0|0.0|              3.11|     13.11|                 false|         1.0|           41.9074919303|           -87.6357600901|    POINT (-87.635760...|            41.8792550844|             -87.642648998|     POINT (-87.642648...|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...|12/01/2019 12:30:...|      1475.0|      12.5|        17031310600|         17031063302|                 31.0|                   6.0|17.5|4.0|              2.55|     24.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|            41.9347624564|            -87.6398538587|     POINT (-87.639853...|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...|12/01/2019 12:30:...|       594.0|       2.5|        17031310600|         17031330100|                 31.0|                  33.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|             41.859349715|            -87.6173580061|     POINT (-87.617358...|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a custom schema.  \n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField('Trip_ID', StringType(), True),        \n",
    "    StructField('Trip_Start_Timestamp', StringType(), True),\n",
    "    StructField('Trip_End_Timestamp', StringType(), True),\n",
    "    StructField('Trip_Seconds', DoubleType(), True),\n",
    "    StructField('Trip_Miles', DoubleType(), True),\n",
    "    StructField('Pickup_Census_Tract', StringType(), True),\n",
    "    StructField('Dropoff_Census_Tract', StringType(), True),\n",
    "    StructField('Pickup_Community_Area', DoubleType(), True),\n",
    "    StructField('Dropoff_Community_Area', DoubleType(), True),\n",
    "    StructField(\"Fare\", DoubleType(), True),\n",
    "    StructField(\"Tip\", DoubleType(), True),\n",
    "    StructField(\"Additional_Charges\", DoubleType(), True),\n",
    "    StructField(\"Trip_Total\", StringType(), True),\n",
    "    StructField(\"Shared_Trip_Authorized\", BooleanType(), True),\n",
    "    StructField(\"Trips_Pooled\", DoubleType(), True),\n",
    "    StructField('Pickup_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Location', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Location', StringType(), True)])\n",
    "\n",
    "#old readin.  Infer is slow for large dataset\n",
    "#df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, inferSchema=True)\n",
    "\n",
    "#read in the data to a dataframe\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, schema=customSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable\n",
    "\n",
    "df = df.drop('Pickup_Census_Tract',\n",
    "             'Dropoff_Census_Tract',\n",
    "             'Pickup_Centroid_Latitude',\n",
    "             'Pickup_Centroid_Longitude', \n",
    "             'Pickup_Centroid_Location', \n",
    "             'Dropoff_Centroid_Latitude', \n",
    "             'Dropoff_Centroid_Longitude', \n",
    "             'Dropoff_Centroid_Location')\n",
    "\n",
    "#'Trip_End_Timestamp' keep for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(False, .0005, seed = 2021) #decreased our sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill our NA community areas\n",
    "\n",
    "df2 = df2.na.fill(value=78,subset=['Pickup_Community_Area', 'Dropoff_Community_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a binary tip/no tip indicator\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer\n",
    "\n",
    "#binarized tip seems to be causing problems.  Change its name to label as that is that the packages are expecting\n",
    "\n",
    "binarizer = Binarizer(threshold=0, inputCol=\"Tip\", outputCol=\"label\")\n",
    "df2 = binarizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"Trip_Start_TS\", F.to_timestamp(F.col(\"Trip_Start_Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- Trip_Start_TS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|Trip_ID                                 |Trip_Start_Timestamp  |Trip_End_Timestamp    |Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|Trip_Start_TS      |Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|Date      |\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|387c40aa5f66f8c85f02b8741aa9c4854f99fe33|12/01/2019 12:15:00 AM|12/01/2019 12:30:00 AM|664.0       |4.1       |24.0                 |5.0                   |7.5 |0.0|2.55              |10.05     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|4530f5f266da1fa88450aad9634bb87a6c6904a9|12/01/2019 12:15:00 AM|12/01/2019 12:45:00 AM|1116.0      |13.7      |76.0                 |22.0                  |27.5|3.0|0.0               |30.5      |true                  |1.0         |1.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|5ca498521f0284ae1665a34606c3163bedbca428|12/01/2019 12:15:00 AM|12/01/2019 12:15:00 AM|298.0       |0.9       |32.0                 |8.0                   |7.5 |4.0|2.85              |14.35     |false                 |1.0         |1.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|649b42138fca921ba56b2bd34bc74018f86feb7d|12/01/2019 12:15:00 AM|12/01/2019 12:15:00 AM|481.0       |1.8       |7.0                  |6.0                   |7.5 |0.0|2.55              |10.05     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "|b7c41becbaae0747d8396e32bdf786ab361647cb|12/01/2019 12:15:00 AM|12/01/2019 12:30:00 AM|1004.0      |7.5       |3.0                  |8.0                   |15.0|0.0|2.55              |17.55     |false                 |1.0         |0.0  |2019-12-01 00:15:00|2019     |12        |48             |1             |0              |15               |2019-12-01|\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn('Trip_Year',F.year(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Month',F.month(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_WeekNumber',F.weekofyear(F.to_timestamp('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_DayofWeek', F.dayofweek(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Start_Hour', F.hour(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Trip_Start_Minute', F.minute(F.col('Trip_Start_TS'))) \\\n",
    "         .withColumn('Date', F.to_date(F.col('Trip_Start_TS')))\n",
    "         \n",
    "df2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- Trip_Start_TS: timestamp (nullable = true)\n",
      " |-- Trip_Year: integer (nullable = true)\n",
      " |-- Trip_Month: integer (nullable = true)\n",
      " |-- Trip_WeekNumber: integer (nullable = true)\n",
      " |-- Trip_DayofWeek: integer (nullable = true)\n",
      " |-- Trip_Start_Hour: integer (nullable = true)\n",
      " |-- Trip_Start_Minute: integer (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original No Tip Count:  16370\n",
      "Original Tip Count   :  3275\n",
      "\n",
      "Final No Tip Count   :  16370\n",
      "Final Tip Count      :  16425\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "\n",
    "# our model didn't work on the standard test train split.  Prof. Tashman recomended upscalling the help with the imbalanced dataset.\n",
    "#from https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\n",
    "\n",
    "train_inital, test = df2.randomSplit([0.8, 0.2], seed=2021)\n",
    "\n",
    "# cahce our test values for later speed\n",
    "test.cache()\n",
    "\n",
    "# oversampleing code sample\n",
    "# https://stackoverflow.com/questions/53273133/how-to-perform-up-sampling-using-sample-functionpy-spark\n",
    "\n",
    "df_a = train_inital.filter(train_inital['label'] == 0)\n",
    "df_b = train_inital.filter(train_inital['label'] == 1)\n",
    "\n",
    "org_a_count = df_a.count()\n",
    "org_b_count = df_b.count() \n",
    "\n",
    "\n",
    "ratio = df_a.count() / df_b.count()\n",
    "# print(ratio)\n",
    "\n",
    "df_b_overampled = df_b.sample(withReplacement=True, fraction=ratio, seed=2021)\n",
    "\n",
    "# cahce our train values for later speed\n",
    "train = df_a.unionAll(df_b_overampled).cache()\n",
    "\n",
    "df_af = train.filter(train_inital['label'] == 0)\n",
    "df_bf = train.filter(train_inital['label'] == 1)\n",
    "fin_a_count = df_af.count()\n",
    "fin_b_count = df_bf.count() \n",
    "\n",
    "print(\"Original No Tip Count: \", org_a_count)\n",
    "print(\"Original Tip Count   : \", org_b_count)\n",
    "print(\"\")\n",
    "print(\"Final No Tip Count   : \", fin_a_count)\n",
    "print(\"Final Tip Count      : \", fin_b_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|      Trip_Start_TS|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|      Date|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|01ba2ca16a9c5d6b7...|12/01/2019 04:30:...|12/01/2019 04:30:...|       575.0|      14.4|                 23.0|                  22.0| 7.5|0.0|               0.0|       7.5|                  true|         3.0|  0.0|2019-12-01 04:30:00|     2019|        12|             48|             1|              4|               30|2019-12-01|\n",
      "|0617812ed64374b3a...|12/01/2019 08:00:...|12/01/2019 08:15:...|       821.0|       5.3|                  5.0|                   7.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|  1.0|2019-12-01 20:00:00|     2019|        12|             48|             1|             20|                0|2019-12-01|\n",
      "|0957f4242b84cfb83...|12/01/2019 08:00:...|12/01/2019 08:30:...|      1004.0|       8.1|                  8.0|                  77.0|12.5|1.0|              2.55|     16.05|                 false|         1.0|  1.0|2019-12-01 20:00:00|     2019|        12|             48|             1|             20|                0|2019-12-01|\n",
      "|18ab602877942f36d...|12/02/2019 03:30:...|12/02/2019 03:45:...|       690.0|       2.2|                 28.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|  0.0|2019-12-02 15:30:00|     2019|        12|             49|             2|             15|               30|2019-12-02|\n",
      "|1b6d204d58edb00c1...|12/02/2019 07:30:...|12/02/2019 07:30:...|       378.0|       0.9|                 22.0|                  24.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|  0.0|2019-12-02 19:30:00|     2019|        12|             49|             2|             19|               30|2019-12-02|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|label|      Trip_Start_TS|Trip_Year|Trip_Month|Trip_WeekNumber|Trip_DayofWeek|Trip_Start_Hour|Trip_Start_Minute|      Date|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "|000e879212e519b17...|12/02/2019 01:45:...|12/02/2019 02:15:...|      2376.0|      14.6|                 24.0|                  56.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|  0.0|2019-12-02 13:45:00|     2019|        12|             49|             2|             13|               45|2019-12-02|\n",
      "|00bcab98504c1e0c9...|12/01/2019 08:00:...|12/01/2019 08:30:...|      1414.0|       6.7|                  6.0|                  32.0|15.0|0.0|              2.55|     17.55|                 false|         1.0|  0.0|2019-12-01 20:00:00|     2019|        12|             48|             1|             20|                0|2019-12-01|\n",
      "|0117b3a3eeb5b268d...|12/02/2019 06:45:...|12/02/2019 07:00:...|       792.0|       2.1|                  8.0|                  28.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|  0.0|2019-12-02 18:45:00|     2019|        12|             49|             2|             18|               45|2019-12-02|\n",
      "|026810744e62e4554...|12/02/2019 08:15:...|12/02/2019 08:30:...|      1328.0|       4.6|                 21.0|                   7.0| 5.0|0.0|              2.55|      7.55|                 false|         2.0|  0.0|2019-12-02 08:15:00|     2019|        12|             49|             2|              8|               15|2019-12-02|\n",
      "|057df74c802dea47c...|12/01/2019 01:00:...|12/01/2019 01:30:...|      1838.0|      17.4|                 77.0|                  32.0|10.0|0.0|               0.0|        10|                  true|         2.0|  0.0|2019-12-01 13:00:00|     2019|        12|             48|             1|             13|                0|2019-12-01|\n",
      "+--------------------+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-----+-------------------+---------+----------+---------------+--------------+---------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training size has increased.  This is to be expected in upscaling.\n",
    "\n",
    "Good reference: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4045\n",
      "788\n"
     ]
    }
   ],
   "source": [
    "#just as a reminder what was the truth in our test data?\n",
    "\n",
    "dft_a = test.filter(train_inital['label'] == 0)\n",
    "dft_b = test.filter(train_inital['label'] == 1)\n",
    "count_test_a = dft_a.count()\n",
    "count_test_b = dft_b.count()\n",
    "print(count_test_a)\n",
    "print(count_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    # McM accuracy\n",
    "    \n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print()\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()     \n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()\n",
    "    print(\"Area Under the PR Curve\", prc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc2(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('label').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    metrics2 = BinaryClassificationMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    #McM accuracy\n",
    "    acc2 = metrics.accuracy\n",
    "    \n",
    "    #calc area under curve\n",
    "    auc = metrics2.areaUnderROC\n",
    "    \n",
    "    prc = metrics2.areaUnderPR\n",
    "    \n",
    "    precision = metrics.precision()\n",
    "    recall = metrics.recall()\n",
    "    f1Score = metrics.fMeasure()\n",
    "       \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    print(\"Accuracy from Confusion Matrix: \", acc)\n",
    "    print(\"Accuracy from MulticlassMetrics: \", acc2)\n",
    "    print()\n",
    "    print(\"Area Under the ROC\", auc)\n",
    "    print()\n",
    "    print(\"Area Under the PR Curve\", prc)\n",
    "   \n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    \n",
    "    # Weighted stats\n",
    "    print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "    print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "    print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "    print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "    print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(maxIter=10,\n",
    "                        regParam=0.1, #org 0.1\n",
    "                        elasticNetParam=0.3, #org 0.3\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"label\")\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "lr_prediction = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3060.  985.]\n",
      " [ 483.  305.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6962549141320091\n",
      "\n",
      "Accuracy from MulticlassMetrics:  0.6962549141320091\n",
      "\n",
      "Area Under the ROC 0.5717726653824675\n",
      "\n",
      "Area Under the PR Curve 0.21394261859261904\n"
     ]
    }
   ],
   "source": [
    "cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3060.  985.]\n",
      " [ 483.  305.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6962549141320091\n",
      "Accuracy from MulticlassMetrics:  0.6962549141320091\n",
      "\n",
      "Area Under the ROC 0.5717726653824675\n",
      "\n",
      "Area Under the PR Curve 0.21394261859261904\n",
      "Summary Stats\n",
      "Precision = 0.6962549141320091\n",
      "Recall = 0.6962549141320091\n",
      "F1 Score = 0.6962549141320091\n",
      "Weighted recall = 0.6962549141320091\n",
      "Weighted precision = 0.7614059286433409\n",
      "Weighted F(1) Score = 0.7228966007425919\n",
      "Weighted F(0.5) Score = 0.7447399201039544\n",
      "Weighted false positive rate = 0.5527095833670741\n"
     ]
    }
   ],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms the results of the first pass of the grid search from tuning below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.271467\n",
      "Accuracy:  0.728533002276019\n",
      "OneHotEncoder_0d071120045e\n"
     ]
    }
   ],
   "source": [
    "# pipeline steps for Random Forest:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\", \n",
    "                            numTrees=10)\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, rf_va, rf])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "rf_prediction = rf_model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "#rf_prediction.select(\"prediction\", \"label\",\"features\").show(5) \n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#metric options f1|accuracy|weightedPrecision|weightedRecall\n",
    "\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_prediction)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - rf_accuracy))\n",
    "print(\"Accuracy: \" , rf_accuracy)\n",
    "\n",
    "rfModel2 = rf_model.stages[3]\n",
    "print(rfModel2)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3244.  801.]\n",
      " [ 511.  277.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.728533002276019\n",
      "Accuracy from MulticlassMetrics:  0.728533002276019\n",
      "\n",
      "Area Under the ROC 0.5767502964743088\n",
      "\n",
      "Area Under the PR Curve 0.2265075643254815\n",
      "Summary Stats\n",
      "Precision = 0.728533002276019\n",
      "Recall = 0.728533002276019\n",
      "F1 Score = 0.728533002276019\n",
      "Weighted recall = 0.728533002276019\n",
      "Weighted precision = 0.7649529611117407\n",
      "Weighted F(1) Score = 0.7445812027907388\n",
      "Weighted F(0.5) Score = 0.7563367617723235\n",
      "Weighted false positive rate = 0.5750324093274016\n"
     ]
    }
   ],
   "source": [
    "cmacc2(rf_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a GBT \n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(291,[0,1,2,4,5,2...|\n",
      "|       0.0|  1.0|(291,[0,1,2,3,5,1...|\n",
      "|       0.0|  1.0|(291,[0,1,2,3,5,1...|\n",
      "|       1.0|  0.0|(291,[0,1,2,3,5,3...|\n",
      "|       0.0|  0.0|(291,[0,1,2,3,5,2...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.282847\n",
      "gbt accuracy =  0.7171529070970412\n",
      "OneHotEncoder_c485f9d45656\n"
     ]
    }
   ],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)#.fit(data)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=5)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = gbt_pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "gbt_prediction = gbt_model.transform(test)\n",
    "\n",
    "#gbt_prediction.show(3)\n",
    "\n",
    "# Select example rows to display.\n",
    "gbt_prediction.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "gbt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "gbt_accuracy = gbt_evaluator.evaluate(gbt_prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - gbt_accuracy))\n",
    "print('gbt accuracy = ', gbt_accuracy)\n",
    "gbtModel = gbt_model.stages[3]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3245.  800.]\n",
      " [ 567.  221.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.7171529070970412\n",
      "Accuracy from MulticlassMetrics:  0.7171529070970412\n",
      "\n",
      "Area Under the ROC 0.5413409109447648\n",
      "\n",
      "Area Under the PR Curve 0.19723951389423752\n",
      "Summary Stats\n",
      "Precision = 0.7171529070970412\n",
      "Recall = 0.7171529070970412\n",
      "F1 Score = 0.7171529070970412\n",
      "Weighted recall = 0.7171529070970413\n",
      "Weighted precision = 0.7477569834372434\n",
      "Weighted F(1) Score = 0.7311743951823794\n",
      "Weighted F(0.5) Score = 0.7408404152077188\n",
      "Weighted false positive rate = 0.6344710852075116\n"
     ]
    }
   ],
   "source": [
    "cmacc2(gbt_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline LR with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lr_paramGrid): 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n25k set\\nbest from tuning inital (accuracy):\\naddGrid(lr.regParam, [0.03, 0.05, 0.07]) .addGrid(lr.elasticNetParam, [0.15, 0.2, 0.25]) .addGrid(lr.maxIter, [8, 9, 10, 11, 12]) \\n\\nregParam= 0.03\\nelasticNetParam = 0.15\\nmaxIter = 12\\n \\nConfusion Matrix\\n[[1842. 2203.]\\n [ 193.  595.]]\\n\\nAccuracy from Confusion Matrix:  0.5042416718394372\\nAccuracy from MulticlassMetrics:  0.5042416718394372\\n\\nArea Under the ROC 0.6052265753923187\\n\\nArea Under the PR Curve 0.2065770273222743\\nSummary Stats\\nPrecision = 0.5042416718394372\\nRecall = 0.5042416718394372\\nF1 Score = 0.5042416718394372\\nWeighted recall = 0.5042416718394371\\nWeighted precision = 0.7922492654683645\\nWeighted F(1) Score = 0.5612342974368173\\nWeighted F(0.5) Score = 0.6730989071456968\\nWeighted false positive rate = 0.2937885210547999\\n\\n2nd round:\\n\\nlr_paramGrid = ParamGridBuilder()     .addGrid(lr.regParam, [0.01, 0.02, 0.03])     .addGrid(lr.elasticNetParam, [0.05, 0.1, 0.15])     .addGrid(lr.maxIter, [11, 12, 13, 15]) \\nregParam= 0.03\\nelasticNetParam = 0.15\\nmaxIter = 11\\n\\nConfusion Matrix\\n[[1815. 2230.]\\n [ 183.  605.]]\\n\\nAccuracy from Confusion Matrix:  0.5007241878750258\\nAccuracy from MulticlassMetrics:  0.5007241878750258\\n\\nArea Under the ROC 0.6082342994108162\\n\\nArea Under the PR Curve 0.2075564549699384\\nSummary Stats\\nPrecision = 0.5007241878750258\\nRecall = 0.5007241878750258\\nF1 Score = 0.5007241878750258\\nWeighted recall = 0.5007241878750258\\nWeighted precision = 0.7950908896146499\\nWeighted F(1) Score = 0.5572078454446675\\nWeighted F(0.5) Score = 0.6716684076809212\\nWeighted false positive rate = 0.28425558905339354\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\",\n",
    "                        labelCol=\"label\") # regParam=0.1, elasticNetParam=0.3, maxIter=10,\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Set up the parameter grid\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "print('len(lr_paramGrid): {}'.format(len(lr_paramGrid)))\n",
    "'''\n",
    "25k set\n",
    "best from tuning inital (accuracy):\n",
    "addGrid(lr.regParam, [0.03, 0.05, 0.07]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.15, 0.2, 0.25]) \\\n",
    ".addGrid(lr.maxIter, [8, 9, 10, 11, 12]) \n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 12\n",
    " \n",
    "Confusion Matrix\n",
    "[[1842. 2203.]\n",
    " [ 193.  595.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5042416718394372\n",
    "Accuracy from MulticlassMetrics:  0.5042416718394372\n",
    "\n",
    "Area Under the ROC 0.6052265753923187\n",
    "\n",
    "Area Under the PR Curve 0.2065770273222743\n",
    "Summary Stats\n",
    "Precision = 0.5042416718394372\n",
    "Recall = 0.5042416718394372\n",
    "F1 Score = 0.5042416718394372\n",
    "Weighted recall = 0.5042416718394371\n",
    "Weighted precision = 0.7922492654683645\n",
    "Weighted F(1) Score = 0.5612342974368173\n",
    "Weighted F(0.5) Score = 0.6730989071456968\n",
    "Weighted false positive rate = 0.2937885210547999\n",
    "\n",
    "2nd round:\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.02, 0.03]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.05, 0.1, 0.15]) \\\n",
    "    .addGrid(lr.maxIter, [11, 12, 13, 15]) \\\n",
    "\n",
    "regParam= 0.03\n",
    "elasticNetParam = 0.15\n",
    "maxIter = 11\n",
    "\n",
    "Confusion Matrix\n",
    "[[1815. 2230.]\n",
    " [ 183.  605.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.5007241878750258\n",
    "Accuracy from MulticlassMetrics:  0.5007241878750258\n",
    "\n",
    "Area Under the ROC 0.6082342994108162\n",
    "\n",
    "Area Under the PR Curve 0.2075564549699384\n",
    "Summary Stats\n",
    "Precision = 0.5007241878750258\n",
    "Recall = 0.5007241878750258\n",
    "F1 Score = 0.5007241878750258\n",
    "Weighted recall = 0.5007241878750258\n",
    "Weighted precision = 0.7950908896146499\n",
    "Weighted F(1) Score = 0.5572078454446675\n",
    "Weighted F(0.5) Score = 0.6716684076809212\n",
    "Weighted false positive rate = 0.28425558905339354\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "lr_crossval = CrossValidator(estimator=lr_pipeline,\n",
    "                          estimatorParamMaps=lr_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 270.67600870132446\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "lr_cvModel = lr_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure what this metric is... apparently it is RMSE https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf\n",
    "#lr_cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "# lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_fb4451652a14', name='regParam', doc='regularization parameter (>= 0).'): 0.03,\n",
       " Param(parent='LogisticRegression_fb4451652a14', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.15,\n",
       " Param(parent='LogisticRegression_fb4451652a14', name='maxIter', doc='max number of iterations (>= 0).'): 5}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "lr_cvModel.getEstimatorParamMaps()[ np.argmin(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_fb4451652a14', name='regParam', doc='regularization parameter (>= 0).'): 0.02,\n",
       " Param(parent='LogisticRegression_fb4451652a14', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1,\n",
       " Param(parent='LogisticRegression_fb4451652a14', name='maxIter', doc='max number of iterations (>= 0).'): 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cvModel.getEstimatorParamMaps()[ np.argmax(lr_cvModel.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "lr_prediction = lr_cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[2056. 1989.]\n",
      " [ 244.  544.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.5379681357334989\n",
      "Accuracy from MulticlassMetrics:  0.5379681357334989\n",
      "\n",
      "Area Under the ROC 0.5993185796841372\n",
      "\n",
      "Area Under the PR Curve 0.20675778651846122\n",
      "Summary Stats\n",
      "Precision = 0.5379681357334989\n",
      "Recall = 0.5379681357334989\n",
      "F1 Score = 0.5379681357334989\n",
      "Weighted recall = 0.5379681357334989\n",
      "Weighted precision = 0.7831808732047225\n",
      "Weighted F(1) Score = 0.5958201718109717\n",
      "Weighted F(0.5) Score = 0.690207435757037\n",
      "Weighted false positive rate = 0.3393309763652244\n"
     ]
    }
   ],
   "source": [
    "cmacc2(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline RF with Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rf_paramGrid): 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nbest from tuning inital (accuracy):\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [5, 10])     .addGrid(rf.maxDepth, [5, 10])     .addGrid(rf.impurity, [\"entropy\", \"gini\"])    .build()\\n\\nnumTrees = 10\\nmaxDepth = 10\\nimpurity = gini\\n \\nConfusion Matrix\\n[[2901. 1144.]\\n [ 456.  332.]]\\n\\nAccuracy from Confusion Matrix:  0.6689426857024623\\nAccuracy from MulticlassMetrics:  0.6689426857024623\\n\\nArea Under the ROC 0.5692507513819781\\n\\nArea Under the PR Curve 0.2070259967551608\\nSummary Stats\\nPrecision = 0.6689426857024623\\nRecall = 0.6689426857024623\\nF1 Score = 0.6689426857024623\\nWeighted recall = 0.6689426857024622\\nWeighted precision = 0.7599403563099746\\nWeighted F(1) Score = 0.7038591473392332\\nWeighted F(0.5) Score = 0.735232181263078\\nWeighted false positive rate = 0.530441182938506\\n\\n2nd attempt:\\n\\nrf_paramGrid = ParamGridBuilder()     .addGrid(rf.numTrees, [5, 10, 15])     .addGrid(rf.maxDepth, [5, 10, 15])     .addGrid(rf.impurity, [\"entropy\", \"gini\"])    .addGrid(rf.featureSubsetStrategy, [\\'auto\\', \\'sqrt\\'])    .build()\\n\\n\\nnumTrees = 10\\nmaxDepth = 15\\nimpurity = gini\\nfeatureSubsetStrategy = auto\\n\\nConfusion Matrix\\n[[2663. 1382.]\\n [ 388.  400.]]\\n\\nAccuracy from Confusion Matrix:  0.6337678460583489\\nAccuracy from MulticlassMetrics:  0.6337678460583489\\n\\nArea Under the ROC 0.5829789236570811\\n\\nArea Under the PR Curve 0.209345437091233\\nSummary Stats\\nPrecision = 0.6337678460583489\\nRecall = 0.6337678460583489\\nF1 Score = 0.6337678460583489\\nWeighted recall = 0.6337678460583488\\nWeighted precision = 0.7671159775546591\\nWeighted F(1) Score = 0.6789410276493224\\nWeighted F(0.5) Score = 0.7270236282319229\\nWeighted false positive rate = 0.46780999874418644\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline steps for RF:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "# assemble feature vector\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "               \n",
    "# set classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \n",
    "                            featuresCol=\"features\")\n",
    "    \n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, rf_va, rf])\n",
    "\n",
    "# Set up the parameter grid\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\"])\\\n",
    "    .build()\n",
    "    #.addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    \n",
    "print('len(rf_paramGrid): {}'.format(len(rf_paramGrid)))\n",
    "\n",
    "#https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n",
    "#maxDepth, maxBins, impurity, auto and seed \n",
    "#.addGrid(randomForestClassifier.impurity, Array(\"entropy\", \"gini\"))\n",
    "#name='featureSubsetStrategy', auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .build()\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 10\n",
    "impurity = gini\n",
    " \n",
    "Confusion Matrix\n",
    "[[2901. 1144.]\n",
    " [ 456.  332.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6689426857024623\n",
    "Accuracy from MulticlassMetrics:  0.6689426857024623\n",
    "\n",
    "Area Under the ROC 0.5692507513819781\n",
    "\n",
    "Area Under the PR Curve 0.2070259967551608\n",
    "Summary Stats\n",
    "Precision = 0.6689426857024623\n",
    "Recall = 0.6689426857024623\n",
    "F1 Score = 0.6689426857024623\n",
    "Weighted recall = 0.6689426857024622\n",
    "Weighted precision = 0.7599403563099746\n",
    "Weighted F(1) Score = 0.7038591473392332\n",
    "Weighted F(0.5) Score = 0.735232181263078\n",
    "Weighted false positive rate = 0.530441182938506\n",
    "\n",
    "2nd attempt:\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 15]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n",
    "    .addGrid(rf.featureSubsetStrategy, ['auto', 'sqrt'])\\\n",
    "    .build()\n",
    "\n",
    "\n",
    "numTrees = 10\n",
    "maxDepth = 15\n",
    "impurity = gini\n",
    "featureSubsetStrategy = auto\n",
    "\n",
    "Confusion Matrix\n",
    "[[2663. 1382.]\n",
    " [ 388.  400.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6337678460583489\n",
    "Accuracy from MulticlassMetrics:  0.6337678460583489\n",
    "\n",
    "Area Under the ROC 0.5829789236570811\n",
    "\n",
    "Area Under the PR Curve 0.209345437091233\n",
    "Summary Stats\n",
    "Precision = 0.6337678460583489\n",
    "Recall = 0.6337678460583489\n",
    "F1 Score = 0.6337678460583489\n",
    "Weighted recall = 0.6337678460583488\n",
    "Weighted precision = 0.7671159775546591\n",
    "Weighted F(1) Score = 0.6789410276493224\n",
    "Weighted F(0.5) Score = 0.7270236282319229\n",
    "Weighted false positive rate = 0.46780999874418644\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "rf_crossval = CrossValidator(estimator=rf_pipeline,\n",
    "                          estimatorParamMaps=rf_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 262.9041373729706\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_rf = rf_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5679094903079406, 0.5971460644767181, 0.580450048927935, 0.6125191936476635]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not sure what this metric is... rmse\n",
    "cvModel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_0072ed8d2b6b', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_rf = cvModel_rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[2781. 1264.]\n",
      " [ 453.  335.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6447341195944548\n",
      "Accuracy from MulticlassMetrics:  0.6447341195944548\n",
      "\n",
      "Area Under the ROC 0.5563211773637944\n",
      "\n",
      "Area Under the PR Curve 0.19615157769388036\n",
      "Summary Stats\n",
      "Precision = 0.6447341195944548\n",
      "Recall = 0.6447341195944548\n",
      "F1 Score = 0.6447341195944548\n",
      "Weighted recall = 0.6447341195944548\n",
      "Weighted precision = 0.7538776114519533\n",
      "Weighted F(1) Score = 0.6852949341957646\n",
      "Weighted F(0.5) Score = 0.7233605918510828\n",
      "Weighted false positive rate = 0.532091764866866\n"
     ]
    }
   ],
   "source": [
    "cmacc2(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_0072ed8d2b6b', name='numTrees', doc='Number of trees to train (>= 1).'): 5,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://dsharpc.github.io/SparkMLFlights/\n",
    "#best?\n",
    "#cvModel.getEstimatorParamMaps()[ np.argmin(cvModel.avgMetrics) ]\n",
    "\n",
    "cvModel_rf.getEstimatorParamMaps()[ np.argmin(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_0072ed8d2b6b', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_0072ed8d2b6b', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel_rf.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline GBT with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gbt_paramGrid): 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nbest from tuning inital (accuracy):\\ngbt_paramGrid = ParamGridBuilder()     .addGrid(gbt.maxIter, [5, 10, 20])     .build()\\n\\nmaxIter = 20\\n\\nConfusion Matrix\\n[[2798. 1247.]\\n [ 423.  365.]]\\n\\nAccuracy from Confusion Matrix:  0.654458928201945\\nAccuracy from MulticlassMetrics:  0.654458928201945\\n\\nArea Under the ROC 0.5774580700620556\\n\\nArea Under the PR Curve 0.2094152550126291\\nSummary Stats\\nPrecision = 0.654458928201945\\nRecall = 0.654458928201945\\nF1 Score = 0.654458928201945\\nWeighted recall = 0.654458928201945\\nWeighted precision = 0.7639586098089827\\nWeighted F(1) Score = 0.6941837869282138\\nWeighted F(0.5) Score = 0.7327747544723259\\nWeighted false positive rate = 0.4995427880778336\\n\\n\\nmaxIter = 40\\n\\nConfusion Matrix\\n[[2571. 1474.]\\n [ 382.  406.]]\\n\\nAccuracy from Confusion Matrix:  0.6159735154148562\\nAccuracy from MulticlassMetrics:  0.6159735154148562\\n\\nArea Under the ROC 0.5754139659791808\\n\\nArea Under the PR Curve 0.20313239804234073\\nSummary Stats\\nPrecision = 0.6159735154148562\\nRecall = 0.6159735154148562\\nF1 Score = 0.6159735154148562\\nWeighted recall = 0.6159735154148562\\nWeighted precision = 0.7638968296438198\\nWeighted F(1) Score = 0.6646010165217533\\nWeighted F(0.5) Score = 0.7183436330713325\\nWeighted false positive rate = 0.4651455834564943\\n\\n\\ngbt_paramGrid = ParamGridBuilder()     .addGrid(gbt.maxIter, [5, 10])    .addGrid(gbt.maxDepth, [5, 10])    .build()\\n    \\nmaxIter = 5\\nmaxDepth = 5\\n\\nConfusion Matrix\\n[[2863. 1182.]\\n [ 469.  319.]]\\n\\nAccuracy from Confusion Matrix:  0.6583902338092282\\nAccuracy from MulticlassMetrics:  0.6583902338092282\\n\\nArea Under the ROC 0.5563048634335804\\n\\nArea Under the PR Curve 0.19780050930331394\\nSummary Stats\\nPrecision = 0.6583902338092282\\nRecall = 0.6583902338092282\\nF1 Score = 0.6583902338092282\\nWeighted recall = 0.6583902338092282\\nWeighted precision = 0.7537989743798752\\nWeighted F(1) Score = 0.6950856095348379\\nWeighted F(0.5) Score = 0.7279222226014918\\nWeighted false positive rate = 0.5457805069420676\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline steps for gbt:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to weekNumber\n",
    "ohe_twn = OneHotEncoder(inputCol=\"Trip_WeekNumber\", outputCol=\"Trip_WeekNumber_vec\")\n",
    "\n",
    "#onehotencoder to dayOfWeek\n",
    "ohe_dw = OneHotEncoder(inputCol=\"Trip_DayofWeek\", outputCol=\"Trip_DayofWeek_vec\")\n",
    "\n",
    "#onehotencoder to startHour\n",
    "ohe_sh = OneHotEncoder(inputCol=\"Trip_Start_Hour\", outputCol=\"Trip_Start_Hour_vec\")\n",
    "\n",
    "#onehotencoder to startMinute\n",
    "ohe_sm = OneHotEncoder(inputCol=\"Trip_Start_Minute\", outputCol=\"Trip_Start_Minute_vec\")\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Trip_Year', \n",
    "                        'Trip_Month',\n",
    "                        'Trip_WeekNumber_vec', \n",
    "                        'Trip_DayofWeek_vec', \n",
    "                        'Trip_Start_Hour_vec',\n",
    "                        'Trip_Start_Minute_vec'\n",
    "                        ] # 'Date' not supported datatype\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_gbt, outputCol=\"features\") \n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\") #, maxIter=5\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, ohe_twn, ohe_dw, ohe_sh, ohe_sm, gbt_va, gbt]) #labelIndexer, featureIndexer\n",
    "\n",
    "# Set up the parameter grid\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "print('len(gbt_paramGrid): {}'.format(len(gbt_paramGrid)))\n",
    "\n",
    "\n",
    "'''\n",
    "best from tuning inital (accuracy):\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "maxIter = 20\n",
    "\n",
    "Confusion Matrix\n",
    "[[2798. 1247.]\n",
    " [ 423.  365.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.654458928201945\n",
    "Accuracy from MulticlassMetrics:  0.654458928201945\n",
    "\n",
    "Area Under the ROC 0.5774580700620556\n",
    "\n",
    "Area Under the PR Curve 0.2094152550126291\n",
    "Summary Stats\n",
    "Precision = 0.654458928201945\n",
    "Recall = 0.654458928201945\n",
    "F1 Score = 0.654458928201945\n",
    "Weighted recall = 0.654458928201945\n",
    "Weighted precision = 0.7639586098089827\n",
    "Weighted F(1) Score = 0.6941837869282138\n",
    "Weighted F(0.5) Score = 0.7327747544723259\n",
    "Weighted false positive rate = 0.4995427880778336\n",
    "\n",
    "\n",
    "maxIter = 40\n",
    "\n",
    "Confusion Matrix\n",
    "[[2571. 1474.]\n",
    " [ 382.  406.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6159735154148562\n",
    "Accuracy from MulticlassMetrics:  0.6159735154148562\n",
    "\n",
    "Area Under the ROC 0.5754139659791808\n",
    "\n",
    "Area Under the PR Curve 0.20313239804234073\n",
    "Summary Stats\n",
    "Precision = 0.6159735154148562\n",
    "Recall = 0.6159735154148562\n",
    "F1 Score = 0.6159735154148562\n",
    "Weighted recall = 0.6159735154148562\n",
    "Weighted precision = 0.7638968296438198\n",
    "Weighted F(1) Score = 0.6646010165217533\n",
    "Weighted F(0.5) Score = 0.7183436330713325\n",
    "Weighted false positive rate = 0.4651455834564943\n",
    "\n",
    "\n",
    "gbt_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [5, 10])\\\n",
    "    .addGrid(gbt.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "    \n",
    "maxIter = 5\n",
    "maxDepth = 5\n",
    "\n",
    "Confusion Matrix\n",
    "[[2863. 1182.]\n",
    " [ 469.  319.]]\n",
    "\n",
    "Accuracy from Confusion Matrix:  0.6583902338092282\n",
    "Accuracy from MulticlassMetrics:  0.6583902338092282\n",
    "\n",
    "Area Under the ROC 0.5563048634335804\n",
    "\n",
    "Area Under the PR Curve 0.19780050930331394\n",
    "Summary Stats\n",
    "Precision = 0.6583902338092282\n",
    "Recall = 0.6583902338092282\n",
    "F1 Score = 0.6583902338092282\n",
    "Weighted recall = 0.6583902338092282\n",
    "Weighted precision = 0.7537989743798752\n",
    "Weighted F(1) Score = 0.6950856095348379\n",
    "Weighted F(0.5) Score = 0.7279222226014918\n",
    "Weighted false positive rate = 0.5457805069420676\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "gbt_crossval = CrossValidator(estimator=gbt_pipeline,\n",
    "                          estimatorParamMaps=gbt_paramGrid,\n",
    "                          #evaluator=BinaryClassificationEvaluator(metricName='areaUnderROC'), #we can pass in our own function if necessary\n",
    "                          evaluator= MulticlassClassificationEvaluator(metricName='accuracy'),\n",
    "                          numFolds=5)\n",
    "\n",
    "# you can do a custom evaluator, but it seems to be a lot of work.  https://stackoverflow.com/questions/51404344/custom-evaluator-in-pyspark\n",
    "# we can use either areaUnderROC or areaUnderPR as defaults for binary.\n",
    "# f1|accuracy|weightedPrecision|weightedRecall for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to find all our items we can call\n",
    "#dir(crossval.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 836.6635949611664\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters. Print the training time.\n",
    "\n",
    "t0 = time.time()\n",
    "cvModel_gbt = gbt_crossval.setParallelism(6).fit(train) # train 6 models in parallel\n",
    "print(\"train time:\", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.596076567228641, 0.64811843790031, 0.6117758066421406, 0.6769369131046441]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not sure what this metric is...\n",
    "cvModel_gbt.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_6ec4c793ba20', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='GBTClassifier_6ec4c793ba20', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# magic code from https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark\n",
    "\n",
    "#cvModel_gbt.getEstimatorParamMaps()[ np.argmax(cvModel_rf.avgMetrics) ]\n",
    "\n",
    "cvModel_gbt.getEstimatorParamMaps()[np.argmax(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='GBTClassifier_6ec4c793ba20', name='maxIter', doc='max number of iterations (>= 0).'): 5,\n",
       " Param(parent='GBTClassifier_6ec4c793ba20', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel_gbt.getEstimatorParamMaps()[np.argmin(cvModel_gbt.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction_gbt = cvModel_gbt.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[2618. 1427.]\n",
      " [ 389.  399.]]\n",
      "\n",
      "Accuracy from Confusion Matrix:  0.6242499482722946\n",
      "Accuracy from MulticlassMetrics:  0.6242499482722946\n",
      "\n",
      "Area Under the ROC 0.5767819831464551\n",
      "\n",
      "Area Under the PR Curve 0.20482020238384122\n",
      "Summary Stats\n",
      "Precision = 0.6242499482722946\n",
      "Recall = 0.6242499482722946\n",
      "F1 Score = 0.6242499482722946\n",
      "Weighted recall = 0.6242499482722946\n",
      "Weighted precision = 0.7643090256415888\n",
      "Weighted F(1) Score = 0.6711999721980451\n",
      "Weighted F(0.5) Score = 0.7218205677729161\n",
      "Weighted false positive rate = 0.47068598197938427\n"
     ]
    }
   ],
   "source": [
    "cmacc2(prediction_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
