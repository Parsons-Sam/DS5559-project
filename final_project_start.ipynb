{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DS5559 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable        Type            Data/Info\n",
      "-----------------------------------------\n",
      "ArrayType       type            <class 'pyspark.sql.types.ArrayType'>\n",
      "BinaryType      type            <class 'pyspark.sql.types.BinaryType'>\n",
      "BooleanType     type            <class 'pyspark.sql.types.BooleanType'>\n",
      "ByteType        type            <class 'pyspark.sql.types.ByteType'>\n",
      "DataType        type            <class 'pyspark.sql.types.DataType'>\n",
      "DateType        type            <class 'pyspark.sql.types.DateType'>\n",
      "DecimalType     type            <class 'pyspark.sql.types.DecimalType'>\n",
      "DoubleType      type            <class 'pyspark.sql.types.DoubleType'>\n",
      "F               module          <module 'pyspark.sql.func<...>yspark/sql/functions.py'>\n",
      "FloatType       type            <class 'pyspark.sql.types.FloatType'>\n",
      "IntegerType     type            <class 'pyspark.sql.types.IntegerType'>\n",
      "LongType        type            <class 'pyspark.sql.types.LongType'>\n",
      "MapType         type            <class 'pyspark.sql.types.MapType'>\n",
      "NullType        type            <class 'pyspark.sql.types.NullType'>\n",
      "ShortType       type            <class 'pyspark.sql.types.ShortType'>\n",
      "SparkSession    type            <class 'pyspark.sql.session.SparkSession'>\n",
      "StringType      type            <class 'pyspark.sql.types.StringType'>\n",
      "StructField     type            <class 'pyspark.sql.types.StructField'>\n",
      "StructType      type            <class 'pyspark.sql.types.StructType'>\n",
      "TimestampType   type            <class 'pyspark.sql.types.TimestampType'>\n",
      "os              module          <module 'os' from '/opt/c<...>nda/lib/python3.7/os.py'>\n",
      "sc              SparkContext    <SparkContext master=loca<...>appName=mllib_classifier>\n",
      "spark           SparkSession    <pyspark.sql.session.Spar<...>object at 0x7f31f8d909d0>\n",
      "typ             module          <module 'pyspark.sql.type<...>on/pyspark/sql/types.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear old df\n",
    "#del (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Schema.  \n",
    "This schema was been primarly determined by using a much smaller dataset and letting spark infer the schema.  We encountered an issue with spark reading in the ENTIRE dataset as NULL when there was a type mismatch.  Only the data we are likely to use later has been assigned to a specific type, otherwise it is left as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Census_Tract|Dropoff_Census_Tract|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Pickup_Centroid_Latitude|Pickup_Centroid_Longitude|Pickup_Centroid_Location|Dropoff_Centroid_Latitude|Dropoff_Centroid_Longitude|Dropoff_Centroid_Location|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...|12/01/2019 01:00:...|      2675.0|      32.5|        17031980000|                null|                 76.0|                  null|70.0|0.0|              9.06|     79.06|                 false|         1.0|           41.9790708201|           -87.9030396611|    POINT (-87.903039...|                     null|                      null|                     null|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...|12/01/2019 12:15:...|       550.0|       2.8|        17031242100|         17031081700|                 24.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8996701799|           -87.6698377982|    POINT (-87.669837...|            41.8920421365|            -87.6318639497|     POINT (-87.631863...|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...|12/01/2019 12:30:...|       922.0|       2.8|        17031080300|         17031281900|                  8.0|                  28.0|10.0|0.0|              3.11|     13.11|                 false|         1.0|           41.9074919303|           -87.6357600901|    POINT (-87.635760...|            41.8792550844|             -87.642648998|     POINT (-87.642648...|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...|12/01/2019 12:30:...|      1475.0|      12.5|        17031310600|         17031063302|                 31.0|                   6.0|17.5|4.0|              2.55|     24.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|            41.9347624564|            -87.6398538587|     POINT (-87.639853...|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...|12/01/2019 12:30:...|       594.0|       2.5|        17031310600|         17031330100|                 31.0|                  33.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|             41.859349715|            -87.6173580061|     POINT (-87.617358...|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a custom schema.  \n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField('Trip_ID', StringType(), True),        \n",
    "    StructField('Trip_Start_Timestamp', StringType(), True),\n",
    "    StructField('Trip_End_Timestamp', StringType(), True),\n",
    "    StructField('Trip_Seconds', DoubleType(), True),\n",
    "    StructField('Trip_Miles', DoubleType(), True),\n",
    "    StructField('Pickup_Census_Tract', StringType(), True),\n",
    "    StructField('Dropoff_Census_Tract', StringType(), True),\n",
    "    StructField('Pickup_Community_Area', DoubleType(), True),\n",
    "    StructField('Dropoff_Community_Area', DoubleType(), True),\n",
    "    StructField(\"Fare\", DoubleType(), True),\n",
    "    StructField(\"Tip\", DoubleType(), True),\n",
    "    StructField(\"Additional_Charges\", DoubleType(), True),\n",
    "    StructField(\"Trip_Total\", StringType(), True),\n",
    "    StructField(\"Shared_Trip_Authorized\", BooleanType(), True),\n",
    "    StructField(\"Trips_Pooled\", DoubleType(), True),\n",
    "    StructField('Pickup_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Location', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Location', StringType(), True)\n",
    "])\n",
    "\n",
    "#old readin.  Infer is slow for large dataset\n",
    "#df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, inferSchema=True)\n",
    "\n",
    "#read in the data to a dataframe\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, schema=customSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_End_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Census_Tract: string (nullable = true)\n",
      " |-- Dropoff_Census_Tract: string (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- Pickup_Centroid_Latitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Longitude: string (nullable = true)\n",
      " |-- Pickup_Centroid_Location: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Latitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Longitude: string (nullable = true)\n",
      " |-- Dropoff_Centroid_Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trip_ID',\n",
       " 'Trip_Start_Timestamp',\n",
       " 'Trip_End_Timestamp',\n",
       " 'Trip_Seconds',\n",
       " 'Trip_Miles',\n",
       " 'Pickup_Census_Tract',\n",
       " 'Dropoff_Census_Tract',\n",
       " 'Pickup_Community_Area',\n",
       " 'Dropoff_Community_Area',\n",
       " 'Fare',\n",
       " 'Tip',\n",
       " 'Additional_Charges',\n",
       " 'Trip_Total',\n",
       " 'Shared_Trip_Authorized',\n",
       " 'Trips_Pooled',\n",
       " 'Pickup_Centroid_Latitude',\n",
       " 'Pickup_Centroid_Longitude',\n",
       " 'Pickup_Centroid_Location',\n",
       " 'Dropoff_Centroid_Latitude',\n",
       " 'Dropoff_Centroid_Longitude',\n",
       " 'Dropoff_Centroid_Location']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable\n",
    "\n",
    "df = df.drop('Trip_End_Timestamp', \n",
    "             'Pickup_Census_Tract',\n",
    "             'Dropoff_Census_Tract',\n",
    "             'Pickup_Centroid_Latitude',\n",
    "             'Pickup_Centroid_Longitude', \n",
    "             'Pickup_Centroid_Location', \n",
    "             'Dropoff_Centroid_Latitude', \n",
    "             'Dropoff_Centroid_Longitude', \n",
    "             'Dropoff_Centroid_Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49108003"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a sampled dataframe for faster work while developing all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(False, .0005, 1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24414"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Trip_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From reading the data dictionary it appears that there are multiple ways that pickup and drop off locations are being reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do histograms of the pickup and drop off areas?  To see if there are desinations that are popular (Airports, downtown, ball parks, museum row... etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|Pickup_Community_Area|count|\n",
      "+---------------------+-----+\n",
      "|                  8.0| 3350|\n",
      "|                 70.0|   87|\n",
      "|                 67.0|  128|\n",
      "|                 69.0|  215|\n",
      "|                  7.0| 1199|\n",
      "|                 49.0|  153|\n",
      "|                 29.0|  221|\n",
      "|                 64.0|   29|\n",
      "|                 75.0|   76|\n",
      "|                 47.0|   14|\n",
      "|                 42.0|  174|\n",
      "|                 44.0|  211|\n",
      "|                 null| 1773|\n",
      "|                 35.0|  168|\n",
      "|                 62.0|   39|\n",
      "|                 18.0|   25|\n",
      "|                  1.0|  308|\n",
      "|                 39.0|  121|\n",
      "|                 34.0|  104|\n",
      "|                 37.0|   28|\n",
      "|                 25.0|  427|\n",
      "|                 36.0|   34|\n",
      "|                 41.0|  323|\n",
      "|                  4.0|  250|\n",
      "|                 23.0|  270|\n",
      "|                 77.0|  375|\n",
      "|                 56.0|  250|\n",
      "|                 50.0|   41|\n",
      "|                 45.0|   32|\n",
      "|                 71.0|  192|\n",
      "|                 31.0|  303|\n",
      "|                 11.0|   70|\n",
      "|                 58.0|  102|\n",
      "|                 21.0|  282|\n",
      "|                 51.0|   54|\n",
      "|                 72.0|   42|\n",
      "|                 14.0|  179|\n",
      "|                 63.0|   67|\n",
      "|                 48.0|   50|\n",
      "|                 22.0|  798|\n",
      "|                 74.0|   21|\n",
      "|                 60.0|  130|\n",
      "|                 66.0|  174|\n",
      "|                 68.0|  170|\n",
      "|                  3.0|  515|\n",
      "|                 19.0|  223|\n",
      "|                 53.0|   80|\n",
      "|                 61.0|  154|\n",
      "|                 59.0|   61|\n",
      "|                 46.0|  105|\n",
      "|                 28.0| 1852|\n",
      "|                  2.0|  235|\n",
      "|                 57.0|   52|\n",
      "|                 17.0|   73|\n",
      "|                 38.0|  178|\n",
      "|                 27.0|  149|\n",
      "|                 10.0|   73|\n",
      "|                 73.0|  106|\n",
      "|                 30.0|  161|\n",
      "|                 40.0|  102|\n",
      "|                 13.0|   59|\n",
      "|                 33.0|  435|\n",
      "|                  6.0| 1498|\n",
      "|                 20.0|   86|\n",
      "|                 52.0|    8|\n",
      "|                 32.0| 1732|\n",
      "|                 24.0| 1397|\n",
      "|                 15.0|  165|\n",
      "|                  5.0|  261|\n",
      "|                 55.0|   11|\n",
      "|                 26.0|   96|\n",
      "|                  9.0|   24|\n",
      "|                 65.0|   81|\n",
      "|                 16.0|  265|\n",
      "|                 12.0|   45|\n",
      "|                 54.0|   27|\n",
      "|                 76.0|  735|\n",
      "|                 43.0|  311|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Pickup_Community_Area').count().show(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only showed top 10 as we have pretty large fall off.  now to replace nulls with actual values.  Will assume to used 99 for outside the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|Pickup_Community_Area|count|\n",
      "+---------------------+-----+\n",
      "|                  8.0| 3350|\n",
      "|                 28.0| 1852|\n",
      "|                 null| 1773|\n",
      "|                 32.0| 1732|\n",
      "|                  6.0| 1498|\n",
      "|                 24.0| 1397|\n",
      "|                  7.0| 1199|\n",
      "|                 22.0|  798|\n",
      "|                 76.0|  735|\n",
      "|                  3.0|  515|\n",
      "+---------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now lets sort the list, added desc to order the list from largest to smallest\n",
    "\n",
    "df2.groupby('Pickup_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just for reference\n",
    "\n",
    "Top Pickup Areas\n",
    "\n",
    "* 8 is the area of magnificiant mile, riverwalk, Gold Coast Neigherborhood, and Navy Pier (tourist trap)\n",
    "* 28 is the near west side, I think lots of condos and new resturants\n",
    "* null it outside the city\n",
    "* 32 is \"The Loop\" downtown busisness district, train EL hub\n",
    "* 6 is Lakeview, Near north neigherbood perdominatly white and Cubs Stadium\n",
    "* 24 West Town don't know as well, likely where Bulls play.  Need to verify\n",
    "* 7 Lincoln Park.  Like Lakeview but more expensive\n",
    "* 22 Logan Square.  Edge of transitioning neigherbood.  More afforable for new (white) owners\n",
    "* 76 O'Hare Airport\n",
    "* 3 Uptown, north of lakeview, like Logan Square less expensive, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Dropoff_Community_Area|count|\n",
      "+----------------------+-----+\n",
      "|                   8.0| 3241|\n",
      "|                  32.0| 1972|\n",
      "|                  28.0| 1942|\n",
      "|                  null| 1911|\n",
      "|                   6.0| 1449|\n",
      "|                  24.0| 1296|\n",
      "|                   7.0| 1213|\n",
      "|                  76.0|  829|\n",
      "|                  22.0|  814|\n",
      "|                   3.0|  514|\n",
      "+----------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do the same for the dropoffs\n",
    "df2.groupby('Dropoff_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just for reference\n",
    "\n",
    "Dropoff Areas\n",
    "\n",
    "* 8 is the area of magnificiant mile, riverwalk, Gold Coast Neigherborhood, and Navy Pier (tourist trap)\n",
    "* 32 is \"The Loop\" downtown busisness district, train EL hub\n",
    "* 28 is the near west side, I think lots of condos and new resturants\n",
    "* null it outside the city\n",
    "* 6 is Lakeview, Near north neigherbood perdominatly white and Cubs Stadium\n",
    "* 24 West Town don't know as well, likely where Bulls play.  Need to verify\n",
    "* 7 Lincoln Park.  Like Lakeview but more expensive\n",
    "* 76 O'Hare Airport\n",
    "* 22 Logan Square.  Edge of transitioning neigherbood.  More afforable for new (white) owners\n",
    "* 3 Uptown, north of lakeview, like Logan Square less expensive, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists are almost identical, just a few order changes.\n",
    "\n",
    "How much of the traffic comes from these heavy use neigherborhoods?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each field, compute missing percentage\n",
    "# from preprcessing example notebook\n",
    "\n",
    "df.agg(*[\n",
    "    (1 - F.count(c) / F.count('*')).alias(c + '_miss')\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might need to use this.  From the logistic regression example code\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# one line model build.  We may need to do multiple types\n",
    "model = LogisticRegressionWithSGD.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this example code is backwards.  we'd likely need to use Predictions and Labels as in the documentation\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "print(labelsAndPreds.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load the data file. Note this data is in sparse format.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and test accuracy.\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy = 1.0 * labelsAndPreds.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-naive-bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my personal favorite... trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Tree-Based Ensemble Methods**\n",
    "\n",
    "*Ensembles* combine multiple models together to produce a new model.  \n",
    "They may consist of models of the same type (e.g., all decision trees) or mixed type (e.g., decision tree + neural net + svm)  \n",
    "\n",
    "One of the fundamental results in machine learning is that multiple weak classifiers can be combined to produce a strong classifier.  \n",
    "\n",
    "Ensembles are useful in reducing overfitting, since predictions are based on several different trees  \n",
    "\n",
    "The two most popular tree-based ensemble methods are *Random Forests* and *Boosted Trees* (e.g. *Gradient-Boosted Trees*)  \n",
    "\n",
    "They are popular because they are often very competitive  \n",
    "\n",
    "The nice properties of decision trees carry over to ensembles of trees  \n",
    "\n",
    "This combining step can proceed using different methods, including:  \n",
    "\n",
    "- voting (for classification)\n",
    "- averaging (for regression) \n",
    "- running model predictions through another model (classification and regression)\n",
    "\n",
    "There are downsides to ensembles:  \n",
    "\n",
    "- Multiple models need to be trained, loaded, and maintained  \n",
    "- Model explanation is harder: no p-values like regression, several trees are feeding overall decision.  \n",
    "There are methods to provide feature importance information, such as partial dependence plots.\n",
    "\n",
    "**Random Forest**  \n",
    "Ensembles of decision trees  \n",
    "\n",
    "RFs inject two sources of randomness into modeling:  \n",
    "\n",
    "1. At each step, randomly select $p$ features out of $n$ total features for possible inclusion (random subspace method)\n",
    "2. Sample the original training set with replacement, up to the size of the original training set (bootstrapping of the training set)\n",
    "\n",
    "The number of features to randomly select $p$ is a parameter  \n",
    "The number of bootstrapped trees to grow $N$ is a parameter  \n",
    "\n",
    "Since the trees are grown independently, the training and prediction tasks are embarrassingly parallel and can be assigned to multiple workers.\n",
    "\n",
    "Classification prediction done by majority vote across trees\n",
    "\n",
    "**Random Forest Implementation**\n",
    "\n",
    "`from pyspark.mllib.tree import RandomForest`  \n",
    "\n",
    "Two most important parameters (which should be tuned using $k$-fold cross validation):  \n",
    "\n",
    "- `numTrees`: Number of trees in forest\n",
    "More trees will increase accuracy but also training time  \n",
    "\n",
    "- `maxDepth`: Maximum depth of each tree in forest\n",
    "Increasing depth can increase power of model, but will take longer to train and can overfit  \n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `subsamplingRate`: fraction of size of original training set (default=1.0 recommended)\n",
    "\n",
    "- `featureSubsetStrategy`: specified as fraction or function of total number of features\n",
    "\n",
    "**Random Forest Example: load data/train model/predict**  \n",
    "NOTE: Very similar to Decision Tree code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Gradient-Boosted Trees**  \n",
    "\n",
    "GBTs work by building a sequence of trees and combining their predictions at each iteration.  The trees constructed are generally *stumps* which use a single decision split.  A stump is an example of a weak learner.\n",
    "\n",
    "This is different from random forests, where each tree independently gives predictions on each training instance.\n",
    "\n",
    "\n",
    "\n",
    "A loss is specified and an optimization problem is solved whereby the objective is to minimize the loss of the model by adding weak learners using a gradient-descent-like procedure.\n",
    "\n",
    "The procedure follows a stage-wise additive model, meaning that one new weak learner is\n",
    "added at a time and existing weak learners are left unchanged.\n",
    "For the original work, see:\n",
    "\n",
    "*Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of Statistics (2001): 1189–1232.*\n",
    "\n",
    "\n",
    "**Gradient-Boosted Trees Implementation**  \n",
    "\n",
    "Since the trees are built in a sequential fashion, the algorithm can not be run in parallel.  \n",
    "However, shallow trees (e.g., stumps) can be used effectively; this saves time versus random forests, which use deeper trees.\n",
    "\n",
    "The loss function in classification problems is the log loss, equal to twice the binomial negative log likelihood.\n",
    "\n",
    "Important parameters:\n",
    "- `numIterations`:  equal to the number of trees in the ensemble.  More trees means longer runtime but also better performance up to a point.\n",
    "- `learningRate`:  how quickly the model adapts on each iteration. A smaller value may help the algo have better performance, but at the cost of additional runtime. The documentation recommends NOT tuning this param.\n",
    "\n",
    "The method `runWithValidation` can help mitigate overfitting.  It takes a training RDD and a validation RDD.\n",
    "\n",
    "The training is stopped when the improvement in the validation error is not more than a certain tolerance (supplied by the `validationTol` argument in `BoostingStrategy`).\n",
    "\n",
    "**GBT Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.json('https://data.cityofchicago.org/api/odata/v4/m6dm-c72p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(domain, dataset_id):\n",
    "    # for this exercise, we're not using an app token,\n",
    "    # but you *should* sign-up and register for an app_token if you want to use the Socrata API\n",
    "    client = Socrata(domain, app_token=None)\n",
    "    offset = None\n",
    "    data = []\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        records = client.get(dataset_id, offset=offset, limit=batch_size)\n",
    "        data.extend(records)\n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        offset = offset + batch_size if (offset) else batch_size\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def download_permits_dataset():\n",
    "    return seattle_permits_df if \"seattle_permits_df\" in globals() else download_dataset(\"data.seattle.gov\", \"k44w-2dcq\")\n",
    "\n",
    "# load Seattle permits data\n",
    "seattle_permits_df = download_permits_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
