{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DS5559 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this reads as an RDD\n",
    "#Need to update with our data file\n",
    "#data = sc.textFile('Trips50sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-646911e984ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#reads our data in as a DF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#better way?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#reads our data in as a DF\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv')\n",
    "\n",
    "#better way?\n",
    "#https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/\n",
    "\n",
    "#df = spark.read.format('csv').options(header='true').load('Trips50sample.csv')\n",
    "\n",
    "#https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n",
    "\n",
    "#df = spark.read.load('Trips50sample.csv', format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "#df = spark.read.load('Trips50sample.tar.gz', format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "#https://stackoverflow.com/questions/40377820/loading-compressed-gzipped-csv-file-in-spark-2-0\n",
    "#df = spark.read.option(\"header\", \"true\").csv(\"Trips50sample.tar.gz\")\n",
    "\n",
    "#df = spark.read.load('Trips50sample.csv', format=\"csv\", sep=\",\", inferSchema=\"false\", header=\"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+--------------------+--------------------+----+----+------------------+----------+--------------------+------------+\n",
      "|      _c0|                 _c1|                 _c2|         _c3|       _c4|                _c5|                 _c6|                 _c7|                 _c8| _c9|_c10|              _c11|      _c12|                _c13|        _c14|\n",
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+--------------------+--------------------+----+----+------------------+----------+--------------------+------------+\n",
      "|     null|Trip Start Timestamp|  Trip End Timestamp|Trip Seconds|Trip Miles|Pickup Census Tract|Dropoff Census Tract|Pickup Community ...|Dropoff Community...|Fare| Tip|Additional Charges|Trip Total|Shared Trip Autho...|Trips Pooled|\n",
      "| 11911213|02/10/2019 06:15:...|02/10/2019 06:30:...|       801.0|       1.9|      17031081201.0|       17031320100.0|                 8.0|                32.0| 5.0| 0.0|               0.0|       5.0|                True|           3|\n",
      "|110540837|12/27/2019 03:30:...|12/27/2019 04:15:...|      2723.0|      12.9|               null|                null|                null|                32.0|25.0|15.0|               3.0|      43.0|               False|           1|\n",
      "| 44935879|05/25/2019 05:00:...|05/25/2019 05:30:...|      2045.0|       8.1|      17031242600.0|       17031040402.0|                24.0|                 4.0|17.5| 0.0|              2.55|     20.05|                True|           3|\n",
      "| 61787628|07/19/2019 07:45:...|07/19/2019 08:00:...|      1189.0|       5.3|      17031040201.0|       17031242300.0|                 4.0|                24.0|12.5| 1.0|              2.55|     16.05|               False|           1|\n",
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+--------------------+--------------------+----+----+------------------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fields from test50\n",
    "\n",
    "\ttrip ID Trip Start Timestamp\tTrip End Timestamp\tTrip Seconds\tTrip Miles\tPickup Census Tract\tDropoff Census Tract\tPickup Community Area\tDropoff Community Area\tFare\tTip\tAdditional Charges\tTrip Total\tShared Trip Authorized\tTrips Pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data with a specific schema\n",
    "#https://stackoverflow.com/questions/39926411/provide-schema-while-reading-csv-file-as-a-dataframe\n",
    "#https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd\n",
    "\n",
    "#this schema only works with the fields in the test data, need to expand for the full dataset\n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField(\"trip_id\", StringType(), True),        \n",
    "    StructField(\"trip_start_time\", TimestampType(), True),\n",
    "    StructField(\"trip_end_time\", TimestampType(), True),\n",
    "    StructField(\"trip_seconds\", DoubleType(), True),\n",
    "    StructField(\"trip_miles\", DoubleType(), True),\n",
    "    StructField(\"trip_pickup_census\", DoubleType(), True),\n",
    "    StructField(\"trip_dropoff_census\", DoubleType(), True),\n",
    "    StructField(\"trip_pickup_ca\", DoubleType(), True),\n",
    "    StructField(\"trip_dropoff_ca\", DoubleType(), True),\n",
    "    StructField(\"trip_fare\", DoubleType(), True),\n",
    "    StructField(\"trip_tip\", DoubleType(), True),\n",
    "    StructField(\"trip_additional_charges\", DoubleType(), True),\n",
    "    StructField(\"trip_total\", DoubleType(), True),\n",
    "    StructField(\"trip_shared_trip_auth\", BooleanType(), True),\n",
    "    StructField(\"trip_trips_pooled\", IntegerType(), True)    \n",
    "])\n",
    "\n",
    "\n",
    "csv_2_df = spark.read.csv(\"Trips50sample.csv.gz\", header = 'false', schema=customSchema)\n",
    "# # #df = spark.read.load('Trips50sample.csv', format=\"csv\", header=\"false\", sep=',', schema=customSchema)\n",
    "\n",
    "# df = spark.read.load('Trips50sample.csv', format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "\n",
    "#testProduct.csv\n",
    "#ID|SEARCHNAME|PRICE\n",
    "#6607|EFKTON75LIN|890.88\n",
    "#6612|EFKTON100HEN|55.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "|trip_id|trip_start_time|trip_end_time|trip_seconds|trip_miles|trip_pickup_census|trip_dropoff_census|trip_pickup_ca|trip_dropoff_ca|trip_fare|trip_tip|trip_additional_charges|trip_total|trip_shared_trip_auth|trip_trips_pooled|\n",
      "+-------+---------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "|   null|           null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|           null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|           null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|           null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|           null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "+-------+---------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_2_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = StructType() \\\n",
    "#     .add(\"trip_ID\", StringType(), False)\\\n",
    "#     .add(\"Trip Start Timestamp\", TimestampType(), False)\\\n",
    "#     .add(\"trip_end_time\", TimestampType(), False)\\\n",
    "#     .add(\"trip_seconds\", DoubleType(), False)\\\n",
    "#     .add(\"trip_miles\", DoubleType(), False)\\\n",
    "#     .add(\"trip_pickup_census\", DoubleType(), True)\\\n",
    "#     .add(\"trip_dropoff_census\", DoubleType(), True)\\\n",
    "#     .add(\"trip_pickup_ca\", DoubleType(), True)\\\n",
    "#     .add(\"trip_dropoff_ca\", DoubleType(), True)\\\n",
    "#     .add(\"trip_fare\", DoubleType(), False)\\\n",
    "#     .add(\"trip_tip\", DoubleType(), False)\\\n",
    "#     .add(\"trip_additional_charges\", DoubleType(), False)\\\n",
    "#     .add(\"trip_total\", DoubleType(), False)\\\n",
    "#     .add(\"trip_shared_trip_auth\", BooleanType(), True)\\\n",
    "#     .add(\"trip_trips_pooled\", IntegerType(), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_with_schema = spark.read.format(\"csv\") \\\n",
    "#       .option(\"header\", False) \\\n",
    "#       .schema(schema) \\\n",
    "#       .load('Trips50sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: timestamp (nullable = true)\n",
      " |-- trip_end_time: timestamp (nullable = true)\n",
      " |-- trip_seconds: double (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_pickup_census: double (nullable = true)\n",
      " |-- trip_dropoff_census: double (nullable = true)\n",
      " |-- trip_pickup_ca: double (nullable = true)\n",
      " |-- trip_dropoff_ca: double (nullable = true)\n",
      " |-- trip_fare: double (nullable = true)\n",
      " |-- trip_tip: double (nullable = true)\n",
      " |-- trip_additional_charges: double (nullable = true)\n",
      " |-- trip_total: double (nullable = true)\n",
      " |-- trip_shared_trip_auth: boolean (nullable = true)\n",
      " |-- trip_trips_pooled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "|trip_ID|Trip Start Timestamp|trip_end_time|trip_seconds|trip_miles|trip_pickup_census|trip_dropoff_census|trip_pickup_ca|trip_dropoff_ca|trip_fare|trip_tip|trip_additional_charges|trip_total|trip_shared_trip_auth|trip_trips_pooled|\n",
      "+-------+--------------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "|   null|                null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|                null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|                null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|                null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "|   null|                null|         null|        null|      null|              null|               null|          null|           null|     null|    null|                   null|      null|                 null|             null|\n",
      "+-------+--------------------+-------------+------------+----------+------------------+-------------------+--------------+---------------+---------+--------+-----------------------+----------+---------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_with_schema.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+----+------------------+----------+----------------------+------------+\n",
      "|      _c0|Trip Start Timestamp|  Trip End Timestamp|Trip Seconds|Trip Miles|Pickup Census Tract|Dropoff Census Tract|Pickup Community Area|Dropoff Community Area|Fare| Tip|Additional Charges|Trip Total|Shared Trip Authorized|Trips Pooled|\n",
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+----+------------------+----------+----------------------+------------+\n",
      "| 11911213|02/10/2019 06:15:...|02/10/2019 06:30:...|       801.0|       1.9|    1.7031081201E10|       1.70313201E10|                  8.0|                  32.0| 5.0| 0.0|               0.0|       5.0|                  true|           3|\n",
      "|110540837|12/27/2019 03:30:...|12/27/2019 04:15:...|      2723.0|      12.9|               null|                null|                 null|                  32.0|25.0|15.0|               3.0|      43.0|                 false|           1|\n",
      "| 44935879|05/25/2019 05:00:...|05/25/2019 05:30:...|      2045.0|       8.1|      1.70312426E10|     1.7031040402E10|                 24.0|                   4.0|17.5| 0.0|              2.55|     20.05|                  true|           3|\n",
      "| 61787628|07/19/2019 07:45:...|07/19/2019 08:00:...|      1189.0|       5.3|    1.7031040201E10|       1.70312423E10|                  4.0|                  24.0|12.5| 1.0|              2.55|     16.05|                 false|           1|\n",
      "|110584188|12/27/2019 06:15:...|12/27/2019 06:15:...|       290.0|       1.0|    1.7031081402E10|       1.70310817E10|                  8.0|                   8.0| 5.0| 0.0|              2.55|      7.55|                 false|           1|\n",
      "+---------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+----+------------------+----------+----------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_c0\",\"trip_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is better, we've got our headers, ecept the ID\n",
    "\n",
    "with custom schema all the data is now nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: integer (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: double (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: double (nullable = true)\n",
      " |-- Dropoff Census Tract: double (nullable = true)\n",
      " |-- Pickup Community Area: double (nullable = true)\n",
      " |-- Dropoff Community Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional Charges: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Shared Trip Authorized: boolean (nullable = true)\n",
      " |-- Trips Pooled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.withColumn('_c0)',F.col('trip_id').cast(StringType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might need to use this.  From the logistic regression example code\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# one line model build.  We may need to do multiple types\n",
    "model = LogisticRegressionWithSGD.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this example code is backwards.  we'd likely need to use Predictions and Labels as in the documentation\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "print(labelsAndPreds.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load the data file. Note this data is in sparse format.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and test accuracy.\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy = 1.0 * labelsAndPreds.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-naive-bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my personal favorite... trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Tree-Based Ensemble Methods**\n",
    "\n",
    "*Ensembles* combine multiple models together to produce a new model.  \n",
    "They may consist of models of the same type (e.g., all decision trees) or mixed type (e.g., decision tree + neural net + svm)  \n",
    "\n",
    "One of the fundamental results in machine learning is that multiple weak classifiers can be combined to produce a strong classifier.  \n",
    "\n",
    "Ensembles are useful in reducing overfitting, since predictions are based on several different trees  \n",
    "\n",
    "The two most popular tree-based ensemble methods are *Random Forests* and *Boosted Trees* (e.g. *Gradient-Boosted Trees*)  \n",
    "\n",
    "They are popular because they are often very competitive  \n",
    "\n",
    "The nice properties of decision trees carry over to ensembles of trees  \n",
    "\n",
    "This combining step can proceed using different methods, including:  \n",
    "\n",
    "- voting (for classification)\n",
    "- averaging (for regression) \n",
    "- running model predictions through another model (classification and regression)\n",
    "\n",
    "There are downsides to ensembles:  \n",
    "\n",
    "- Multiple models need to be trained, loaded, and maintained  \n",
    "- Model explanation is harder: no p-values like regression, several trees are feeding overall decision.  \n",
    "There are methods to provide feature importance information, such as partial dependence plots.\n",
    "\n",
    "**Random Forest**  \n",
    "Ensembles of decision trees  \n",
    "\n",
    "RFs inject two sources of randomness into modeling:  \n",
    "\n",
    "1. At each step, randomly select $p$ features out of $n$ total features for possible inclusion (random subspace method)\n",
    "2. Sample the original training set with replacement, up to the size of the original training set (bootstrapping of the training set)\n",
    "\n",
    "The number of features to randomly select $p$ is a parameter  \n",
    "The number of bootstrapped trees to grow $N$ is a parameter  \n",
    "\n",
    "Since the trees are grown independently, the training and prediction tasks are embarrassingly parallel and can be assigned to multiple workers.\n",
    "\n",
    "Classification prediction done by majority vote across trees\n",
    "\n",
    "**Random Forest Implementation**\n",
    "\n",
    "`from pyspark.mllib.tree import RandomForest`  \n",
    "\n",
    "Two most important parameters (which should be tuned using $k$-fold cross validation):  \n",
    "\n",
    "- `numTrees`: Number of trees in forest\n",
    "More trees will increase accuracy but also training time  \n",
    "\n",
    "- `maxDepth`: Maximum depth of each tree in forest\n",
    "Increasing depth can increase power of model, but will take longer to train and can overfit  \n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `subsamplingRate`: fraction of size of original training set (default=1.0 recommended)\n",
    "\n",
    "- `featureSubsetStrategy`: specified as fraction or function of total number of features\n",
    "\n",
    "**Random Forest Example: load data/train model/predict**  \n",
    "NOTE: Very similar to Decision Tree code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Gradient-Boosted Trees**  \n",
    "\n",
    "GBTs work by building a sequence of trees and combining their predictions at each iteration.  The trees constructed are generally *stumps* which use a single decision split.  A stump is an example of a weak learner.\n",
    "\n",
    "This is different from random forests, where each tree independently gives predictions on each training instance.\n",
    "\n",
    "\n",
    "\n",
    "A loss is specified and an optimization problem is solved whereby the objective is to minimize the loss of the model by adding weak learners using a gradient-descent-like procedure.\n",
    "\n",
    "The procedure follows a stage-wise additive model, meaning that one new weak learner is\n",
    "added at a time and existing weak learners are left unchanged.\n",
    "For the original work, see:\n",
    "\n",
    "*Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of Statistics (2001): 1189–1232.*\n",
    "\n",
    "\n",
    "**Gradient-Boosted Trees Implementation**  \n",
    "\n",
    "Since the trees are built in a sequential fashion, the algorithm can not be run in parallel.  \n",
    "However, shallow trees (e.g., stumps) can be used effectively; this saves time versus random forests, which use deeper trees.\n",
    "\n",
    "The loss function in classification problems is the log loss, equal to twice the binomial negative log likelihood.\n",
    "\n",
    "Important parameters:\n",
    "- `numIterations`:  equal to the number of trees in the ensemble.  More trees means longer runtime but also better performance up to a point.\n",
    "- `learningRate`:  how quickly the model adapts on each iteration. A smaller value may help the algo have better performance, but at the cost of additional runtime. The documentation recommends NOT tuning this param.\n",
    "\n",
    "The method `runWithValidation` can help mitigate overfitting.  It takes a training RDD and a validation RDD.\n",
    "\n",
    "The training is stopped when the improvement in the validation error is not more than a certain tolerance (supplied by the `validationTol` argument in `BoostingStrategy`).\n",
    "\n",
    "**GBT Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.json('https://data.cityofchicago.org/api/odata/v4/m6dm-c72p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(domain, dataset_id):\n",
    "    # for this exercise, we're not using an app token,\n",
    "    # but you *should* sign-up and register for an app_token if you want to use the Socrata API\n",
    "    client = Socrata(domain, app_token=None)\n",
    "    offset = None\n",
    "    data = []\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        records = client.get(dataset_id, offset=offset, limit=batch_size)\n",
    "        data.extend(records)\n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        offset = offset + batch_size if (offset) else batch_size\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def download_permits_dataset():\n",
    "    return seattle_permits_df if \"seattle_permits_df\" in globals() else download_dataset(\"data.seattle.gov\", \"k44w-2dcq\")\n",
    "\n",
    "# load Seattle permits data\n",
    "seattle_permits_df = download_permits_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
