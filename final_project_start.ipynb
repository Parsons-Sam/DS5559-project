{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DS5559 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '21g') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.instances', '3') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear old df\n",
    "#del (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Schema.  \n",
    "This schema was been primarly determined by using a much smaller dataset and letting spark infer the schema.  We encountered an issue with spark reading in the ENTIRE dataset as NULL when there was a type mismatch.  Only the data we are likely to use later has been assigned to a specific type, otherwise it is left as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Census_Tract|Dropoff_Census_Tract|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Pickup_Centroid_Latitude|Pickup_Centroid_Longitude|Pickup_Centroid_Location|Dropoff_Centroid_Latitude|Dropoff_Centroid_Longitude|Dropoff_Centroid_Location|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...|12/01/2019 01:00:...|      2675.0|      32.5|        17031980000|                null|                 76.0|                  null|70.0|0.0|              9.06|     79.06|                 false|         1.0|           41.9790708201|           -87.9030396611|    POINT (-87.903039...|                     null|                      null|                     null|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...|12/01/2019 12:15:...|       550.0|       2.8|        17031242100|         17031081700|                 24.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8996701799|           -87.6698377982|    POINT (-87.669837...|            41.8920421365|            -87.6318639497|     POINT (-87.631863...|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...|12/01/2019 12:30:...|       922.0|       2.8|        17031080300|         17031281900|                  8.0|                  28.0|10.0|0.0|              3.11|     13.11|                 false|         1.0|           41.9074919303|           -87.6357600901|    POINT (-87.635760...|            41.8792550844|             -87.642648998|     POINT (-87.642648...|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...|12/01/2019 12:30:...|      1475.0|      12.5|        17031310600|         17031063302|                 31.0|                   6.0|17.5|4.0|              2.55|     24.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|            41.9347624564|            -87.6398538587|     POINT (-87.639853...|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...|12/01/2019 12:30:...|       594.0|       2.5|        17031310600|         17031330100|                 31.0|                  33.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|             41.859349715|            -87.6173580061|     POINT (-87.617358...|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a custom schema.  \n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField('Trip_ID', StringType(), True),        \n",
    "    StructField('Trip_Start_Timestamp', StringType(), True),\n",
    "    StructField('Trip_End_Timestamp', StringType(), True),\n",
    "    StructField('Trip_Seconds', DoubleType(), True),\n",
    "    StructField('Trip_Miles', DoubleType(), True),\n",
    "    StructField('Pickup_Census_Tract', StringType(), True),\n",
    "    StructField('Dropoff_Census_Tract', StringType(), True),\n",
    "    StructField('Pickup_Community_Area', DoubleType(), True),\n",
    "    StructField('Dropoff_Community_Area', DoubleType(), True),\n",
    "    StructField(\"Fare\", DoubleType(), True),\n",
    "    StructField(\"Tip\", DoubleType(), True),\n",
    "    StructField(\"Additional_Charges\", DoubleType(), True),\n",
    "    StructField(\"Trip_Total\", StringType(), True),\n",
    "    StructField(\"Shared_Trip_Authorized\", BooleanType(), True),\n",
    "    StructField(\"Trips_Pooled\", DoubleType(), True),\n",
    "    StructField('Pickup_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Location', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Location', StringType(), True)\n",
    "])\n",
    "\n",
    "#old readin.  Infer is slow for large dataset\n",
    "#df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, inferSchema=True)\n",
    "\n",
    "#read in the data to a dataframe\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, schema=customSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv')\n",
    "rdd = rdd.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53304688/spark-date-format-mmm-dd-yyyy-hhmmss-am-to-timestamp-in-df\n",
    "#https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "start_times = rdd.map(lambda x: (x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = start_times.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = start_times.withColumn(\"_2\",to_timestamp(col(\"_2\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = start_times.withColumn(\"_2\",to_timestamp(col(\"_2\"), \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable\n",
    "\n",
    "df = df.drop('Trip_End_Timestamp', \n",
    "             'Pickup_Census_Tract',\n",
    "             'Dropoff_Census_Tract',\n",
    "             'Pickup_Centroid_Latitude',\n",
    "             'Pickup_Centroid_Longitude', \n",
    "             'Pickup_Centroid_Location', \n",
    "             'Dropoff_Centroid_Latitude', \n",
    "             'Dropoff_Centroid_Longitude', \n",
    "             'Dropoff_Centroid_Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49108003"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a sampled dataframe for faster work while developing all the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(False, .0005, 1221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24414"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('Trip_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From reading the data dictionary it appears that there are multiple ways that pickup and drop off locations are being reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do histograms of the pickup and drop off areas?  To see if there are desinations that are popular (Airports, downtown, ball parks, museum row... etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare and Tip Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "heatmap_data = df2.groupby('Fare','Tip').count().sort('Fare','Tip').groupby('Fare').pivot('Tip').sum(\"count\").sort('Fare')\n",
    "heatmap_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data = np.array(heatmap_data.drop('Fare').toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://numpy.org/doc/stable/reference/generated/numpy.flipud.html#numpy.flipud\n",
    "hm_data = np.flipud(hm_data.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mungingdata.com/pyspark/column-to-list-collect-tolocaliterator/\n",
    "fares = list(heatmap_data.select('Fare').toPandas()['Fare'])\n",
    "tips = sorted([float(x) for x in heatmap_data.columns if x != 'Fare'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonpool.com/matplotlib-figsize/\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.axes().set_facecolor(\"#ffffff\")\n",
    "plt.axes().set_ylabel(\"Tip Amt\")\n",
    "plt.axes().set_xlabel(\"Fare Amt\")\n",
    "#https://www.pythonpool.com/matplotlib-heatmap/\n",
    "plt.xticks(ticks=np.arange(len(fares)),labels=fares,rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(tips)),labels=tips)\n",
    "hm = plt.imshow(hm_data,cmap='Blues',interpolation=\"none\")\n",
    "plt.colorbar(hm)\n",
    "plt.title(\"Tip Amounts by Fare Amounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = df2.groupby('Pickup_Community_Area','Dropoff_Community_Area').count().\\\n",
    "                    sort('Pickup_Community_Area','Dropoff_Community_Area').groupby('Pickup_Community_Area').pivot('Dropoff_Community_Area').sum(\"count\").sort('Pickup_Community_Area')\n",
    "#heatmap_data = heatmap_data.groupby('Pickup_Community_Area').pivot('Dropoff_Community_Area').count()\n",
    "heatmap_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data = np.array(heatmap_data.drop('Pickup_Community_Area').toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data = np.flipud(hm_data.transpose())\n",
    "#https://mungingdata.com/pyspark/column-to-list-collect-tolocaliterator/\n",
    "pickups = sorted(list(heatmap_data.select('Pickup_Community_Area').toPandas()['Pickup_Community_Area']))\n",
    "dropoffs = sorted([float(\"nan\") if x == \"null\" else float(x) for x in heatmap_data.columns if x != 'Pickup_Community_Area'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonpool.com/matplotlib-figsize/\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axes().set_facecolor(\"#ffffff\")\n",
    "plt.axes().set_ylabel(\"Pickup Areas\")\n",
    "plt.axes().set_xlabel(\"Dropoff Areas\")\n",
    "#https://www.pythonpool.com/matplotlib-heatmap/\n",
    "plt.xticks(ticks=np.arange(len(pickups)),labels=pickups,rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(dropoffs)),labels=dropoffs)\n",
    "hm = plt.imshow(hm_data,cmap='Blues',interpolation=\"none\")\n",
    "plt.colorbar(hm)\n",
    "plt.title(\"Tip Heatmap by Pickup and Dropoff Community Areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = df2.groupby('Pickup_Community_Area','Dropoff_Community_Area').count().\\\n",
    "                    sort('Pickup_Community_Area','Dropoff_Community_Area').groupby('Pickup_Community_Area').pivot('Dropoff_Community_Area').sum(\"count\").sort('Pickup_Community_Area')\n",
    "#heatmap_data = heatmap_data.groupby('Pickup_Community_Area').pivot('Dropoff_Community_Area').count()\n",
    "heatmap_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data = np.array(heatmap_data.drop('Pickup_Community_Area').toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_data = np.flipud(hm_data.transpose())\n",
    "#https://mungingdata.com/pyspark/column-to-list-collect-tolocaliterator/\n",
    "pickups = sorted(list(heatmap_data.select('Pickup_Community_Area').toPandas()['Pickup_Community_Area']))\n",
    "dropoffs = sorted([float(\"nan\") if x == \"null\" else float(x) for x in heatmap_data.columns if x != 'Pickup_Community_Area'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pythonpool.com/matplotlib-figsize/\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.axes().set_facecolor(\"#ffffff\")\n",
    "plt.axes().set_ylabel(\"Pickup Areas\")\n",
    "plt.axes().set_xlabel(\"Dropoff Areas\")\n",
    "#https://www.pythonpool.com/matplotlib-heatmap/\n",
    "plt.xticks(ticks=np.arange(len(pickups)),labels=pickups,rotation=90)\n",
    "plt.yticks(ticks=np.arange(len(dropoffs)),labels=dropoffs)\n",
    "hm = plt.imshow(hm_data,cmap='Blues',interpolation=\"none\")\n",
    "plt.colorbar(hm)\n",
    "plt.title(\"Tip Heatmap by Pickup and Dropoff Community Areas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickups_hist = df2.groupby('Pickup_Community_Area').count().withColumnRenamed(\"count\",\"area_count\").sort(\"area_count\")\n",
    "#https://stackoverflow.com/questions/38610559/convert-spark-dataframe-column-to-python-list\n",
    "areas = [str(x.Pickup_Community_Area) for x in pickups_hist.collect()]\n",
    "area_count = [x.area_count for x in pickups_hist.collect()]\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.bar(areas,area_count, color='lightblue')\n",
    "plt.title('Pickup Community Areas Histogram')\n",
    "plt.axes().set_facecolor(\"#ffffff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_hist = df2.groupby('Dropoff_Community_Area').count().withColumnRenamed(\"count\",\"area_count\").sort(\"area_count\")\n",
    "areas = [str(x.Dropoff_Community_Area) for x in dropoffs_hist.collect()]\n",
    "area_count = [x.area_count for x in dropoffs_hist.collect()]\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.bar(areas,area_count, color='lightblue')\n",
    "plt.title('Dropoff Community Areas Histogram')\n",
    "plt.axes().set_facecolor(\"#ffffff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only showing top 10 as we have pretty large fall off.  Now to replace nulls with actual values.  Will use 99 for areas outside the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|Pickup_Community_Area|count|\n",
      "+---------------------+-----+\n",
      "|                  8.0| 3350|\n",
      "|                 28.0| 1852|\n",
      "|                 null| 1773|\n",
      "|                 32.0| 1732|\n",
      "|                  6.0| 1498|\n",
      "|                 24.0| 1397|\n",
      "|                  7.0| 1199|\n",
      "|                 22.0|  798|\n",
      "|                 76.0|  735|\n",
      "|                  3.0|  515|\n",
      "+---------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now lets sort the list, added desc to order the list from largest to smallest\n",
    "\n",
    "df2.groupby('Pickup_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference https://www.google.com/maps/d/u/0/viewer?ll=41.91066822076546%2C-87.63676464181398&spn=0.340714%2C0.699692&msa=0&mid=1O-3Uot4mSetKW-M_govahruUjDc&z=12\n",
    "\n",
    "Top Pickup Areas\n",
    "\n",
    "* 8 is the area of Magnificiant Mile (high end shopping), the riverwalk (tourism, boat tours), Gold Coast Neigherborhood (very high income neigherhood), and Navy Pier (tourist trap)\n",
    "* 28 is the Near West Side neigherborhoods of West Loop, Greektown, and Fulton's Market.  Lots of new condos and resturants.  Also has University of Illinois Chicago, the Medical District (University and VA hospitals) and the United Center (home of the Bull and Blackhawks)\n",
    "* null is outside the city\n",
    "* 32 is \"The Loop\" downtown busisness district, train El hub\n",
    "* 6 is Lakeview, Near north neigherbood perdominatly white and has the Chciago Cubs Stadium\n",
    "* 24 West Town and has the neigherborhoods of Wicker Park, Ukranian Village, and River West.  These neigherbohoods have all been ungergroing gentrification.\n",
    "* 7 Lincoln Park.  Like Lakeview but more expensive.  Was one of the first neigherborhoods in Chicago to gentrify.  \n",
    "* 22 Logan Square.  Edge of transitioning neigherbood.  More afforable for new gentrifiying owners.\n",
    "* 76 O'Hare Airport\n",
    "* 3 Uptown, north of Lakeview, similar to Logan Square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Dropoff_Community_Area|count|\n",
      "+----------------------+-----+\n",
      "|                   8.0| 3241|\n",
      "|                  32.0| 1972|\n",
      "|                  28.0| 1942|\n",
      "|                  null| 1911|\n",
      "|                   6.0| 1449|\n",
      "|                  24.0| 1296|\n",
      "|                   7.0| 1213|\n",
      "|                  76.0|  829|\n",
      "|                  22.0|  814|\n",
      "|                   3.0|  514|\n",
      "+----------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do the same for the dropoffs\n",
    "df2.groupby('Dropoff_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Dropoff_Community_Area|count|\n",
      "+----------------------+-----+\n",
      "|                  77.0|  363|\n",
      "|                  76.0|  829|\n",
      "|                  75.0|   56|\n",
      "|                  74.0|   23|\n",
      "|                  73.0|   98|\n",
      "|                  72.0|   48|\n",
      "|                  71.0|  204|\n",
      "|                  70.0|   72|\n",
      "|                  69.0|  171|\n",
      "|                  68.0|  142|\n",
      "|                  67.0|  136|\n",
      "|                  66.0|  150|\n",
      "|                  65.0|   75|\n",
      "|                  64.0|   40|\n",
      "|                  63.0|   83|\n",
      "|                  62.0|   31|\n",
      "|                  61.0|  156|\n",
      "|                  60.0|  144|\n",
      "|                  59.0|   70|\n",
      "|                  58.0|   92|\n",
      "+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Dropoff_Community_Area').count().orderBy('Dropoff_Community_Area', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just for reference https://www.google.com/maps/d/u/0/viewer?ll=41.91066822076546%2C-87.63676464181398&spn=0.340714%2C0.699692&msa=0&mid=1O-3Uot4mSetKW-M_govahruUjDc&z=12\n",
    "\n",
    "Dropoff Areas\n",
    "\n",
    "* 8is the area of Magnificiant Mile (high end shopping), the riverwalk (tourism, boat tours), Gold Coast Neigherborhood (very high income neigherhood), and Navy Pier (tourist trap)\n",
    "* 32 is \"The Loop\" downtown busisness district, train El hub\n",
    "* 28 is the Near West Side neigherborhoods of West Loop, Greektown, and Fulton's Market.  Lots of new condos and resturants.  Also has University of Illinois Chicago, the Medical District (University and VA hospitals) and the United Center (home of the Bull and Blackhawks)\n",
    "* null is outside the city\n",
    "* 6 is Lakeview, Near north neigherbood perdominatly white and has the Chciago Cubs Stadium\n",
    "* 24 West Town and has the neigherborhoods of Wicker Park, Ukranian Village, and River West.  These neigherbohoods have all been ungergroing gentrification.\n",
    "* 7 Lincoln Park. Like Lakeview but more expensive.  Was one of the first neigherborhoods in Chicago to gentrify.  \n",
    "* 76 O'Hare Airport\n",
    "* 22 Logan Square.  Edge of transitioning neigherbood.  More afforable for new gentrifiying owners.\n",
    "* 3 Uptown, north of Lakeview, similar to Logan Square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists are almost identical, just a few order changes.\n",
    "\n",
    "How much of the traffic comes from these heavy use neigherborhoods?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace our null values in the pickup and dropoff locations\n",
    "# https://stackoverflow.com/questions/42312042/how-to-replace-all-null-values-of-a-dataframe-in-pyspark\n",
    "\n",
    "# choosing 78 as it is not a Chicago Community Area and using a larger number will result in a larger OHE matrix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.na.fill(value=78,subset=['Pickup_Community_Area', 'Dropoff_Community_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Dropoff_Community_Area|count|\n",
      "+----------------------+-----+\n",
      "|                   8.0| 3241|\n",
      "|                  32.0| 1972|\n",
      "|                  28.0| 1942|\n",
      "|                  78.0| 1911|\n",
      "|                   6.0| 1449|\n",
      "|                  24.0| 1296|\n",
      "|                   7.0| 1213|\n",
      "|                  76.0|  829|\n",
      "|                  22.0|  814|\n",
      "|                   3.0|  514|\n",
      "+----------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupby('Dropoff_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|Pickup_Community_Area|count|\n",
      "+---------------------+-----+\n",
      "|                  8.0| 3350|\n",
      "|                 28.0| 1852|\n",
      "|                 78.0| 1773|\n",
      "|                 32.0| 1732|\n",
      "|                  6.0| 1498|\n",
      "|                 24.0| 1397|\n",
      "|                  7.0| 1199|\n",
      "|                 22.0|  798|\n",
      "|                 76.0|  735|\n",
      "|                  3.0|  515|\n",
      "+---------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupby('Pickup_Community_Area').count().orderBy('count', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like it works to change the nulls to a 78 for the community area.\n",
    "\n",
    "Next lets add a colum with the tip or no tip as a binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+\n",
      "|923e2cd22e434fe90...|12/01/2019 12:15:...|       966.0|       4.6|                 28.0|                   6.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|\n",
      "|54fb0ae8d3d76bc94...|12/01/2019 12:45:...|       825.0|       3.3|                  7.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|\n",
      "|fc02ddb50acc2353e...|12/01/2019 01:00:...|       310.0|       1.2|                 31.0|                  28.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|\n",
      "|2a98ee39bbf866cd5...|12/01/2019 01:00:...|       592.0|       2.4|                 24.0|                  22.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|\n",
      "|5e4d0423955c50246...|12/01/2019 01:15:...|      1131.0|       9.3|                 56.0|                  78.0|15.0|0.0|              7.55|     22.55|                 false|         1.0|\n",
      "|6073293c44c2f2249...|12/01/2019 01:15:...|       846.0|       6.5|                  3.0|                   8.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|\n",
      "|e1f726de4ef78c018...|12/01/2019 01:30:...|       218.0|       0.5|                  8.0|                   8.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|\n",
      "|34e74cafda665d5bd...|12/01/2019 01:45:...|       601.0|       3.5|                  7.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|\n",
      "|413c31b05f2e8038b...|12/01/2019 02:00:...|       601.0|       2.4|                  6.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|\n",
      "|2bcd9c84129dd6032...|12/01/2019 02:15:...|       912.0|       5.9|                 22.0|                  78.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|\n",
      "|7e2263783d6d5b6d3...|12/01/2019 02:30:...|      1359.0|      13.0|                 76.0|                   6.0|20.0|0.0|              7.55|     27.55|                 false|         1.0|\n",
      "|f84fe60e5e0aeed8f...|12/01/2019 02:30:...|      1155.0|      11.4|                 31.0|                  77.0|17.5|0.0|              2.55|     20.05|                 false|         1.0|\n",
      "|24853652b7e088880...|12/01/2019 02:45:...|       534.0|       4.0|                 74.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|\n",
      "|ac0464215676fd503...|12/01/2019 02:45:...|      1267.0|      17.0|                 76.0|                  28.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|\n",
      "|d1c56fc291ec7dffc...|12/01/2019 03:00:...|       282.0|       1.2|                  6.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|\n",
      "|7da558cef4ff4a18d...|12/01/2019 03:30:...|       664.0|       4.5|                 10.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|\n",
      "|5833bfaff020b6217...|12/01/2019 04:00:...|      1432.0|       9.9|                  1.0|                   8.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|\n",
      "|fd334e93ef381cb95...|12/01/2019 04:15:...|       320.0|       1.4|                 17.0|                  17.0| 7.5|0.0|              2.85|     10.35|                 false|         1.0|\n",
      "|c85f20f37121342c2...|12/01/2019 06:00:...|      2578.0|      29.1|                 78.0|                  76.0|37.5|0.0|             10.12|     47.62|                 false|         1.0|\n",
      "|196999fccf50364b3...|12/01/2019 06:30:...|      1155.0|       6.6|                 28.0|                   6.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old code, Michaels worked better.  This should be deleted later\n",
    "\n",
    "#do we need this lit function?\n",
    "#https://hackersandslackers.com/transforming-pyspark-dataframes/\n",
    "#from pyspark.sql.functions import lit, when, col\n",
    "#df4 = df3.withColumn('testColumn', F.lit('this is a test'))\n",
    "# that worked\n",
    "#df4 = df3.withColumn('Tip_Bool', when((col(\"tip\") > 0), 1).otherwise(0))\n",
    "# df = df.withColumn([COLUMN_NAME]. F.when([CONDITIONAL], [COLUMN_VALUE]).otherwsie([COLUMN_VALUE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip_ID: string, Trip_Start_Timestamp: string, Trip_Seconds: double, Trip_Miles: double, Pickup_Community_Area: double, Dropoff_Community_Area: double, Fare: double, Tip: double, Additional_Charges: double, Trip_Total: string, Shared_Trip_Authorized: boolean, Trips_Pooled: double, binarized_tip: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the biniazier that Michael found\n",
    "\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer\n",
    "\n",
    "from pyspark.ml.feature import Binarizer\n",
    "#binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "binarizer = Binarizer(threshold=0, inputCol=\"Tip\", outputCol=\"binarized_tip\")\n",
    "binarizedTip = binarizer.transform(df3)\n",
    "df4=binarizedTip\n",
    "\n",
    "display(binarizedTip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|923e2cd22e434fe90...|12/01/2019 12:15:...|       966.0|       4.6|                 28.0|                   6.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|54fb0ae8d3d76bc94...|12/01/2019 12:45:...|       825.0|       3.3|                  7.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|fc02ddb50acc2353e...|12/01/2019 01:00:...|       310.0|       1.2|                 31.0|                  28.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|\n",
      "|2a98ee39bbf866cd5...|12/01/2019 01:00:...|       592.0|       2.4|                 24.0|                  22.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|5e4d0423955c50246...|12/01/2019 01:15:...|      1131.0|       9.3|                 56.0|                  78.0|15.0|0.0|              7.55|     22.55|                 false|         1.0|          0.0|\n",
      "|6073293c44c2f2249...|12/01/2019 01:15:...|       846.0|       6.5|                  3.0|                   8.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|e1f726de4ef78c018...|12/01/2019 01:30:...|       218.0|       0.5|                  8.0|                   8.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|          0.0|\n",
      "|34e74cafda665d5bd...|12/01/2019 01:45:...|       601.0|       3.5|                  7.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|413c31b05f2e8038b...|12/01/2019 02:00:...|       601.0|       2.4|                  6.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|2bcd9c84129dd6032...|12/01/2019 02:15:...|       912.0|       5.9|                 22.0|                  78.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|          1.0|\n",
      "|7e2263783d6d5b6d3...|12/01/2019 02:30:...|      1359.0|      13.0|                 76.0|                   6.0|20.0|0.0|              7.55|     27.55|                 false|         1.0|          0.0|\n",
      "|f84fe60e5e0aeed8f...|12/01/2019 02:30:...|      1155.0|      11.4|                 31.0|                  77.0|17.5|0.0|              2.55|     20.05|                 false|         1.0|          0.0|\n",
      "|24853652b7e088880...|12/01/2019 02:45:...|       534.0|       4.0|                 74.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|ac0464215676fd503...|12/01/2019 02:45:...|      1267.0|      17.0|                 76.0|                  28.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|          0.0|\n",
      "|d1c56fc291ec7dffc...|12/01/2019 03:00:...|       282.0|       1.2|                  6.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|\n",
      "|7da558cef4ff4a18d...|12/01/2019 03:30:...|       664.0|       4.5|                 10.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|5833bfaff020b6217...|12/01/2019 04:00:...|      1432.0|       9.9|                  1.0|                   8.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|\n",
      "|fd334e93ef381cb95...|12/01/2019 04:15:...|       320.0|       1.4|                 17.0|                  17.0| 7.5|0.0|              2.85|     10.35|                 false|         1.0|          0.0|\n",
      "|c85f20f37121342c2...|12/01/2019 06:00:...|      2578.0|      29.1|                 78.0|                  76.0|37.5|0.0|             10.12|     47.62|                 false|         1.0|          0.0|\n",
      "|196999fccf50364b3...|12/01/2019 06:30:...|      1155.0|       6.6|                 28.0|                   6.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|\n",
      "|737d6c6b076d8e431...|12/01/2019 06:45:...|       965.0|       5.2|                 78.0|                  76.0|10.0|0.0|              7.55|     17.55|                 false|         1.0|          0.0|\n",
      "|21c872628b565d7c3...|12/01/2019 07:15:...|       947.0|       4.8|                 11.0|                  25.0|10.0|1.0|              2.55|     13.55|                 false|         1.0|          1.0|\n",
      "|27b50e206afe9c653...|12/01/2019 07:15:...|      1344.0|      17.2|                 76.0|                   8.0|20.0|0.0|              7.55|     27.55|                 false|         1.0|          0.0|\n",
      "|1654dc5f4128d9ddc...|12/01/2019 07:30:...|       469.0|       3.3|                 71.0|                  61.0| 2.5|0.0|              2.55|      5.05|                  true|         3.0|          0.0|\n",
      "|8db3b6213397693e0...|12/01/2019 08:00:...|       579.0|       2.8|                 28.0|                  32.0| 7.5|1.0|              2.55|     11.05|                 false|         1.0|          1.0|\n",
      "|cdeb13f002259e403...|12/01/2019 09:45:...|      1659.0|      25.1|                 76.0|                  78.0|27.5|7.0|              8.79|     43.29|                 false|         1.0|          1.0|\n",
      "|78e8a1f137c8360ae...|12/01/2019 10:00:...|       426.0|       0.7|                  8.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|\n",
      "|e83230685c616705e...|12/01/2019 10:00:...|       566.0|       2.3|                  8.0|                  32.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|eabed151853510c8f...|12/01/2019 10:15:...|       832.0|       4.6|                  8.0|                  28.0| 5.0|2.0|               0.0|         7|                  true|         2.0|          1.0|\n",
      "|fc57ce518533645ff...|12/01/2019 10:15:...|      1578.0|      12.2|                 65.0|                  28.0|20.0|0.0|              2.55|     22.55|                 false|         1.0|          0.0|\n",
      "|87104da8003661cf1...|12/01/2019 10:30:...|      1599.0|      16.0|                 15.0|                  78.0|20.0|0.0|               0.0|        20|                  true|         1.0|          0.0|\n",
      "|a99a6bae426a55194...|12/01/2019 10:30:...|       186.0|       0.8|                 28.0|                  28.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|          0.0|\n",
      "|bd1d825c29f6cad90...|12/01/2019 10:45:...|       288.0|       1.3|                 74.0|                  78.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|\n",
      "|e63c16f7f89027713...|12/01/2019 11:00:...|      1009.0|       6.0|                 28.0|                  34.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|\n",
      "|6c4c8c97da85010ee...|12/01/2019 11:15:...|       562.0|       3.9|                  6.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|76229030ac9816c2a...|12/01/2019 11:15:...|      1911.0|      24.4|                 78.0|                  15.0|30.0|4.0|              2.55|     36.55|                 false|         1.0|          1.0|\n",
      "|f34bf33eb18ebb1e6...|12/01/2019 11:15:...|       404.0|       1.2|                 32.0|                  32.0| 5.0|1.0|              2.55|      8.55|                 false|         1.0|          1.0|\n",
      "|0812d78b5ab4cd74e...|12/01/2019 11:15:...|      1007.0|       7.0|                 32.0|                  22.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|26487c137ebd590c6...|12/01/2019 11:30:...|       806.0|       4.8|                  6.0|                   8.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|e7bfe4c4ca668b253...|12/01/2019 12:00:...|      1229.0|      17.0|                 69.0|                  78.0|17.5|0.0|              2.55|     20.05|                 false|         1.0|          0.0|\n",
      "|24f18d962adfb17e6...|12/01/2019 12:15:...|      1269.0|       7.0|                  6.0|                  28.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|\n",
      "|279d938100d60a37e...|12/01/2019 12:15:...|      2701.0|      29.6|                 76.0|                  78.0|37.5|8.0|               0.0|      45.5|                  true|         1.0|          1.0|\n",
      "|025f587d152ca9181...|12/01/2019 12:15:...|      2579.0|      16.5|                 76.0|                   6.0|27.5|0.0|              9.54|     37.04|                 false|         1.0|          0.0|\n",
      "|850f49d6a9b86b46e...|12/01/2019 12:15:...|       750.0|       5.2|                  8.0|                  33.0|10.0|0.0|              7.55|     17.55|                 false|         1.0|          0.0|\n",
      "|156ad97b3f1218f34...|12/01/2019 12:45:...|      2469.0|      17.3|                 56.0|                   6.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|          0.0|\n",
      "|8065cd19664586a6e...|12/01/2019 12:45:...|      1706.0|       9.7|                 17.0|                  78.0|17.5|0.0|               0.0|      17.5|                  true|         1.0|          0.0|\n",
      "|19fa4caa71dcd63a3...|12/01/2019 01:00:...|      2991.0|      19.0|                 78.0|                  76.0|30.0|0.0|               8.3|      38.3|                 false|         1.0|          0.0|\n",
      "|2f2e1013adba23025...|12/01/2019 01:30:...|       740.0|       2.8|                  6.0|                   7.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|8fdd0ec144c3f337c...|12/01/2019 01:30:...|       742.0|       2.8|                  7.0|                   6.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|7409bbe3104a9ef53...|12/01/2019 01:30:...|       581.0|       1.6|                 32.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+-----+\n",
      "|Dropoff_Community_Area|binarized_tip|count|\n",
      "+----------------------+-------------+-----+\n",
      "|                   8.0|          0.0| 2634|\n",
      "|                  28.0|          0.0| 1615|\n",
      "|                  78.0|          0.0| 1565|\n",
      "|                  32.0|          0.0| 1560|\n",
      "|                   6.0|          0.0| 1168|\n",
      "|                  24.0|          0.0| 1077|\n",
      "|                   7.0|          0.0| 1019|\n",
      "|                  22.0|          0.0|  668|\n",
      "|                   8.0|          1.0|  607|\n",
      "|                  76.0|          0.0|  571|\n",
      "|                   3.0|          0.0|  419|\n",
      "|                  32.0|          1.0|  412|\n",
      "|                  25.0|          0.0|  370|\n",
      "|                  78.0|          1.0|  346|\n",
      "|                  28.0|          1.0|  327|\n",
      "|                  33.0|          0.0|  316|\n",
      "|                  41.0|          0.0|  293|\n",
      "|                  43.0|          0.0|  287|\n",
      "|                   6.0|          1.0|  281|\n",
      "|                  77.0|          0.0|  277|\n",
      "|                  76.0|          1.0|  258|\n",
      "|                  31.0|          0.0|  245|\n",
      "|                   1.0|          0.0|  238|\n",
      "|                   5.0|          0.0|  227|\n",
      "|                  24.0|          1.0|  219|\n",
      "|                  21.0|          0.0|  216|\n",
      "|                  16.0|          0.0|  216|\n",
      "|                  23.0|          0.0|  210|\n",
      "|                  29.0|          0.0|  208|\n",
      "|                  71.0|          0.0|  195|\n",
      "|                   7.0|          1.0|  194|\n",
      "|                  56.0|          0.0|  191|\n",
      "|                  19.0|          0.0|  189|\n",
      "|                  44.0|          0.0|  183|\n",
      "|                   2.0|          0.0|  180|\n",
      "|                  38.0|          0.0|  170|\n",
      "|                   4.0|          0.0|  170|\n",
      "|                  42.0|          0.0|  164|\n",
      "|                  69.0|          0.0|  159|\n",
      "|                  49.0|          0.0|  158|\n",
      "|                  35.0|          0.0|  153|\n",
      "|                  15.0|          0.0|  151|\n",
      "|                  22.0|          1.0|  146|\n",
      "|                  66.0|          0.0|  142|\n",
      "|                  61.0|          0.0|  139|\n",
      "|                  30.0|          0.0|  138|\n",
      "|                  14.0|          0.0|  138|\n",
      "|                  68.0|          0.0|  134|\n",
      "|                  27.0|          0.0|  133|\n",
      "|                  67.0|          0.0|  129|\n",
      "+----------------------+-------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupby('Dropoff_Community_Area', 'binarized_tip').count().orderBy('count', ascending=False).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i guess now we should start doing more statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|binarized_tip|count|\n",
      "+-------------+-----+\n",
      "|          0.0|20376|\n",
      "|          1.0| 4038|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupby(\"binarized_tip\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can we do a logistic model with what we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+\n",
      "|923e2cd22e434fe90...|12/01/2019 12:15:...|       966.0|       4.6|                 28.0|                   6.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[6],[1.0])|\n",
      "|54fb0ae8d3d76bc94...|12/01/2019 12:45:...|       825.0|       3.3|                  7.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[7],[1.0])|            (78,[5],[1.0])|\n",
      "|fc02ddb50acc2353e...|12/01/2019 01:00:...|       310.0|       1.2|                 31.0|                  28.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|          (78,[31],[1.0])|           (78,[28],[1.0])|\n",
      "|2a98ee39bbf866cd5...|12/01/2019 01:00:...|       592.0|       2.4|                 24.0|                  22.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[24],[1.0])|           (78,[22],[1.0])|\n",
      "|5e4d0423955c50246...|12/01/2019 01:15:...|      1131.0|       9.3|                 56.0|                  78.0|15.0|0.0|              7.55|     22.55|                 false|         1.0|          0.0|          (78,[56],[1.0])|                (78,[],[])|\n",
      "|6073293c44c2f2249...|12/01/2019 01:15:...|       846.0|       6.5|                  3.0|                   8.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[3],[1.0])|            (78,[8],[1.0])|\n",
      "|e1f726de4ef78c018...|12/01/2019 01:30:...|       218.0|       0.5|                  8.0|                   8.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[8],[1.0])|\n",
      "|34e74cafda665d5bd...|12/01/2019 01:45:...|       601.0|       3.5|                  7.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|           (78,[7],[1.0])|            (78,[8],[1.0])|\n",
      "|413c31b05f2e8038b...|12/01/2019 02:00:...|       601.0|       2.4|                  6.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[5],[1.0])|\n",
      "|2bcd9c84129dd6032...|12/01/2019 02:15:...|       912.0|       5.9|                 22.0|                  78.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|          1.0|          (78,[22],[1.0])|                (78,[],[])|\n",
      "|7e2263783d6d5b6d3...|12/01/2019 02:30:...|      1359.0|      13.0|                 76.0|                   6.0|20.0|0.0|              7.55|     27.55|                 false|         1.0|          0.0|          (78,[76],[1.0])|            (78,[6],[1.0])|\n",
      "|f84fe60e5e0aeed8f...|12/01/2019 02:30:...|      1155.0|      11.4|                 31.0|                  77.0|17.5|0.0|              2.55|     20.05|                 false|         1.0|          0.0|          (78,[31],[1.0])|           (78,[77],[1.0])|\n",
      "|24853652b7e088880...|12/01/2019 02:45:...|       534.0|       4.0|                 74.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[74],[1.0])|                (78,[],[])|\n",
      "|ac0464215676fd503...|12/01/2019 02:45:...|      1267.0|      17.0|                 76.0|                  28.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|          0.0|          (78,[76],[1.0])|           (78,[28],[1.0])|\n",
      "|d1c56fc291ec7dffc...|12/01/2019 03:00:...|       282.0|       1.2|                  6.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[3],[1.0])|\n",
      "|7da558cef4ff4a18d...|12/01/2019 03:30:...|       664.0|       4.5|                 10.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[10],[1.0])|                (78,[],[])|\n",
      "|5833bfaff020b6217...|12/01/2019 04:00:...|      1432.0|       9.9|                  1.0|                   8.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|           (78,[1],[1.0])|            (78,[8],[1.0])|\n",
      "|fd334e93ef381cb95...|12/01/2019 04:15:...|       320.0|       1.4|                 17.0|                  17.0| 7.5|0.0|              2.85|     10.35|                 false|         1.0|          0.0|          (78,[17],[1.0])|           (78,[17],[1.0])|\n",
      "|c85f20f37121342c2...|12/01/2019 06:00:...|      2578.0|      29.1|                 78.0|                  76.0|37.5|0.0|             10.12|     47.62|                 false|         1.0|          0.0|               (78,[],[])|           (78,[76],[1.0])|\n",
      "|196999fccf50364b3...|12/01/2019 06:30:...|      1155.0|       6.6|                 28.0|                   6.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[6],[1.0])|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@nutanbhogendrasharma/role-of-onehotencoder-and-pipelines-in-pyspark-ml-feature-part-2-3275767e74f0\n",
    "# apparently the spark doc is wrong, we don't need fit, just transform.  Maybe thats a spark 3 thing?\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to pickup\n",
    "onehotencoder_pickup_vector = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "df4 = onehotencoder_pickup_vector.transform(df4)\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "onehotencoder_dropoff_vector = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "df4 = onehotencoder_dropoff_vector.transform(df4)\n",
    "\n",
    "df4.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = false)\n",
      " |-- Dropoff_Community_Area: double (nullable = false)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      " |-- binarized_tip: double (nullable = true)\n",
      " |-- Pickup_Community_Area_vec: vector (nullable = true)\n",
      " |-- Dropoff_Community_Area_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "# removed to see if works better\n",
    "#  'Pickup_Community_Area',\n",
    "#                         'Dropoff_Community_Area',\n",
    "\n",
    "# leaving out Trip_start_Timestamp for now as I don't know know to use the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+\n",
      "|923e2cd22e434fe90...|12/01/2019 12:15:...|       966.0|       4.6|                 28.0|                   6.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,3...|\n",
      "|54fb0ae8d3d76bc94...|12/01/2019 12:45:...|       825.0|       3.3|                  7.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[7],[1.0])|            (78,[5],[1.0])|(162,[0,1,2,3,5,1...|\n",
      "|fc02ddb50acc2353e...|12/01/2019 01:00:...|       310.0|       1.2|                 31.0|                  28.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|          (78,[31],[1.0])|           (78,[28],[1.0])|(162,[0,1,2,3,5,3...|\n",
      "|2a98ee39bbf866cd5...|12/01/2019 01:00:...|       592.0|       2.4|                 24.0|                  22.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[24],[1.0])|           (78,[22],[1.0])|(162,[0,1,2,3,5,3...|\n",
      "|5e4d0423955c50246...|12/01/2019 01:15:...|      1131.0|       9.3|                 56.0|                  78.0|15.0|0.0|              7.55|     22.55|                 false|         1.0|          0.0|          (78,[56],[1.0])|                (78,[],[])|(162,[0,1,2,3,5,6...|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from last weeks homework\n",
    "# use all of the fields as features\n",
    "\n",
    "#not sure if we should be scaling the OHE vectors but we'll try first\n",
    "\n",
    "assembler = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "transformed = assembler.transform(df4)\n",
    "df5 = transformed\n",
    "\n",
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|923e2cd22e434fe90...|12/01/2019 12:15:...|       966.0|       4.6|                 28.0|                   6.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "|54fb0ae8d3d76bc94...|12/01/2019 12:45:...|       825.0|       3.3|                  7.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[7],[1.0])|            (78,[5],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|fc02ddb50acc2353e...|12/01/2019 01:00:...|       310.0|       1.2|                 31.0|                  28.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|          (78,[31],[1.0])|           (78,[28],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "|2a98ee39bbf866cd5...|12/01/2019 01:00:...|       592.0|       2.4|                 24.0|                  22.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[24],[1.0])|           (78,[22],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "|5e4d0423955c50246...|12/01/2019 01:15:...|      1131.0|       9.3|                 56.0|                  78.0|15.0|0.0|              7.55|     22.55|                 false|         1.0|          0.0|          (78,[56],[1.0])|                (78,[],[])|(162,[0,1,2,3,5,6...|(162,[0,1,2,3,5,6...|\n",
      "|6073293c44c2f2249...|12/01/2019 01:15:...|       846.0|       6.5|                  3.0|                   8.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[3],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,9...|(162,[0,1,2,3,5,9...|\n",
      "|e1f726de4ef78c018...|12/01/2019 01:30:...|       218.0|       0.5|                  8.0|                   8.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|34e74cafda665d5bd...|12/01/2019 01:45:...|       601.0|       3.5|                  7.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|           (78,[7],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|413c31b05f2e8038b...|12/01/2019 02:00:...|       601.0|       2.4|                  6.0|                   5.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[5],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|2bcd9c84129dd6032...|12/01/2019 02:15:...|       912.0|       5.9|                 22.0|                  78.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|          1.0|          (78,[22],[1.0])|                (78,[],[])|(162,[0,1,2,3,5,2...|(162,[0,1,2,3,5,2...|\n",
      "|7e2263783d6d5b6d3...|12/01/2019 02:30:...|      1359.0|      13.0|                 76.0|                   6.0|20.0|0.0|              7.55|     27.55|                 false|         1.0|          0.0|          (78,[76],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,8...|(162,[0,1,2,3,5,8...|\n",
      "|f84fe60e5e0aeed8f...|12/01/2019 02:30:...|      1155.0|      11.4|                 31.0|                  77.0|17.5|0.0|              2.55|     20.05|                 false|         1.0|          0.0|          (78,[31],[1.0])|           (78,[77],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "|24853652b7e088880...|12/01/2019 02:45:...|       534.0|       4.0|                 74.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[74],[1.0])|                (78,[],[])|(162,[0,1,2,3,5,8...|(162,[0,1,2,3,5,8...|\n",
      "|ac0464215676fd503...|12/01/2019 02:45:...|      1267.0|      17.0|                 76.0|                  28.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|          0.0|          (78,[76],[1.0])|           (78,[28],[1.0])|(162,[0,1,2,3,5,8...|(162,[0,1,2,3,5,8...|\n",
      "|d1c56fc291ec7dffc...|12/01/2019 03:00:...|       282.0|       1.2|                  6.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[3],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|7da558cef4ff4a18d...|12/01/2019 03:30:...|       664.0|       4.5|                 10.0|                  78.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[10],[1.0])|                (78,[],[])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|5833bfaff020b6217...|12/01/2019 04:00:...|      1432.0|       9.9|                  1.0|                   8.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|           (78,[1],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,7...|(162,[0,1,2,3,5,7...|\n",
      "|fd334e93ef381cb95...|12/01/2019 04:15:...|       320.0|       1.4|                 17.0|                  17.0| 7.5|0.0|              2.85|     10.35|                 false|         1.0|          0.0|          (78,[17],[1.0])|           (78,[17],[1.0])|(162,[0,1,2,3,5,2...|(162,[0,1,2,3,5,2...|\n",
      "|c85f20f37121342c2...|12/01/2019 06:00:...|      2578.0|      29.1|                 78.0|                  76.0|37.5|0.0|             10.12|     47.62|                 false|         1.0|          0.0|               (78,[],[])|           (78,[76],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|196999fccf50364b3...|12/01/2019 06:30:...|      1155.0|       6.6|                 28.0|                   6.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling from our homework\n",
    "\n",
    "#from https://spark.apache.org/docs/latest/ml-features#standardscaler\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df5)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df5)\n",
    "\n",
    "\n",
    "df6 = scaledData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Trip_ID                                 |Trip_Start_Timestamp  |Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|features                                                 |scaledFeatures                                                                                                                                                |\n",
      "+----------------------------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|923e2cd22e434fe90dbdd716e737246b5ebf3732|12/01/2019 12:15:00 AM|966.0       |4.6       |28.0                 |6.0                   |10.0|0.0|2.55              |12.55     |false                 |1.0         |0.0          |(78,[28],[1.0])          |(78,[6],[1.0])            |(162,[0,1,2,3,5,34,90],[966.0,4.6,10.0,2.55,1.0,1.0,1.0])|(162,[0,1,2,3,5,34,90],[1.405086284218977,0.6478972965818061,1.0723416216023778,1.307140224788053,2.6993338487633283,3.776771907595458,4.232165358734312])    |\n",
      "|54fb0ae8d3d76bc94c61894d3cc7584f58ce8059|12/01/2019 12:45:00 AM|825.0       |3.3       |7.0                  |5.0                   |10.0|0.0|2.55              |12.55     |false                 |1.0         |0.0          |(78,[7],[1.0])           |(78,[5],[1.0])            |(162,[0,1,2,3,5,13,89],[825.0,3.3,10.0,2.55,1.0,1.0,1.0])|(162,[0,1,2,3,5,13,89],[1.1999960501870146,0.4647958866782522,1.0723416216023778,1.307140224788053,2.6993338487633283,4.627390068340773,9.391530555155365])   |\n",
      "|fc02ddb50acc2353e90562916615dd86a18ece1d|12/01/2019 01:00:00 AM|310.0       |1.2       |31.0                 |28.0                  |5.0 |0.0|2.55              |7.55      |false                 |1.0         |0.0          |(78,[31],[1.0])          |(78,[28],[1.0])           |(162,[0,1,2,3,5,37,112],[310.0,1.2,5.0,2.55,1.0,1.0,1.0])|(162,[0,1,2,3,5,37,112],[0.4509076067369388,0.16901668606481898,0.5361708108011889,1.307140224788053,2.6993338487633283,9.032357527193685,3.6955966929041795])|\n",
      "|2a98ee39bbf866cd55c177ebc3ea0aef325fee96|12/01/2019 01:00:00 AM|592.0       |2.4       |24.0                 |22.0                  |7.5 |0.0|2.55              |10.05     |false                 |1.0         |0.0          |(78,[24],[1.0])          |(78,[22],[1.0])           |(162,[0,1,2,3,5,30,106],[592.0,2.4,7.5,2.55,1.0,1.0,1.0])|(162,[0,1,2,3,5,30,106],[0.8610880748008637,0.33803337212963797,0.8042562162017834,1.307140224788053,2.6993338487633283,4.305340293941959,5.570085245662122]) |\n",
      "|5e4d0423955c50246c8a639665565e3411804906|12/01/2019 01:15:00 AM|1131.0      |9.3       |56.0                 |78.0                  |15.0|0.0|7.55              |22.55     |false                 |1.0         |0.0          |(78,[56],[1.0])          |(78,[],[])                |(162,[0,1,2,3,5,62],[1131.0,9.3,15.0,7.55,1.0,1.0])      |(162,[0,1,2,3,5,62],[1.64508549425638,1.3098793170023473,1.608512432403567,3.8701602733920786,2.6993338487633283,9.932890041943159])                          |\n",
      "+----------------------------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+---------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|016f5c09cc942f33e...|12/01/2019 07:00:...|       753.0|       2.4|                  6.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|025f587d152ca9181...|12/01/2019 12:15:...|      2579.0|      16.5|                 76.0|                   6.0|27.5|0.0|              9.54|     37.04|                 false|         1.0|          0.0|          (78,[76],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,8...|(162,[0,1,2,3,5,8...|\n",
      "|02643a50fbfd5cfd5...|12/02/2019 05:30:...|       678.0|       2.7|                 12.0|                  14.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[12],[1.0])|           (78,[14],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|0498d78606e900985...|12/01/2019 05:30:...|       404.0|       1.9|                  8.0|                   7.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[7],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|05692a43a10215ec3...|12/02/2019 07:00:...|       464.0|       3.9|                  8.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[3],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|032f5a1fb7abe78a9...|12/01/2019 04:15:...|       564.0|       1.4|                  8.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|0a63690199b5cfac9...|12/01/2019 10:00:...|      1121.0|      10.1|                  8.0|                  25.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|           (78,[8],[1.0])|           (78,[25],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|0e813934de44ca56c...|12/02/2019 09:30:...|      1012.0|       5.2|                 30.0|                  25.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|          (78,[30],[1.0])|           (78,[25],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|\n",
      "|19fa4caa71dcd63a3...|12/01/2019 01:00:...|      2991.0|      19.0|                 78.0|                  76.0|30.0|0.0|               8.3|      38.3|                 false|         1.0|          0.0|               (78,[],[])|           (78,[76],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|1bcdc2afc50c20307...|12/02/2019 09:45:...|      2266.0|      11.0|                 70.0|                  28.0|12.5|0.0|              2.55|     15.05|                  true|         3.0|          0.0|          (78,[70],[1.0])|           (78,[28],[1.0])|(162,[0,1,2,3,4,5...|(162,[0,1,2,3,4,5...|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\n",
    "train_inital, test = df6.randomSplit([0.8, 0.2], seed=2021)\n",
    "\n",
    "train_inital.show(5)\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampleing code sample\n",
    "# https://stackoverflow.com/questions/53273133/how-to-perform-up-sampling-using-sample-functionpy-spark\n",
    "\n",
    "df_a = train_inital.filter(train_inital['binarized_tip'] == 0)\n",
    "df_b = train_inital.filter(train_inital['binarized_tip'] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16427\n",
      "3191\n"
     ]
    }
   ],
   "source": [
    "a_count = df_a.count()\n",
    "b_count = df_b.count() \n",
    "print(a_count)\n",
    "print(b_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.147916013788781\n"
     ]
    }
   ],
   "source": [
    "ratio = a_count / b_count\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b_overampled = df_b.sample(withReplacement=True, fraction=ratio, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = df_a.unionAll(df_b_overampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|      scaledFeatures|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "|016f5c09cc942f33e...|12/01/2019 07:00:...|       753.0|       2.4|                  6.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|           (78,[6],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|025f587d152ca9181...|12/01/2019 12:15:...|      2579.0|      16.5|                 76.0|                   6.0|27.5|0.0|              9.54|     37.04|                 false|         1.0|          0.0|          (78,[76],[1.0])|            (78,[6],[1.0])|(162,[0,1,2,3,5,8...|(162,[0,1,2,3,5,8...|\n",
      "|02643a50fbfd5cfd5...|12/02/2019 05:30:...|       678.0|       2.7|                 12.0|                  14.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[12],[1.0])|           (78,[14],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|0498d78606e900985...|12/01/2019 05:30:...|       404.0|       1.9|                  8.0|                   7.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[7],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "|05692a43a10215ec3...|12/02/2019 07:00:...|       464.0|       3.9|                  8.0|                   3.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[3],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16427\n",
      "16420\n"
     ]
    }
   ],
   "source": [
    "df_af = train_final.filter(train_inital['binarized_tip'] == 0)\n",
    "df_bf = train_final.filter(train_inital['binarized_tip'] == 1)\n",
    "a_count = df_af.count()\n",
    "b_count = df_bf.count() \n",
    "print(a_count)\n",
    "print(b_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting... our training size has increased.  I'm not sure thats a good thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (162,[0,2,3,4,5,31,35,49,82,127,160],[1.2978883364670957e-05,0.001495483898928809,0.03573770412391918,-0.15149295002507418,-0.1282488537362025,-0.1587715968552557,-0.11991871657485817,-0.03200141299880642,0.004072288797686548,-0.0577089515419798,0.060321081486527074])\n",
      "Intercept: -0.014236846134381245\n"
     ]
    }
   ],
   "source": [
    "# from the docs https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\n",
    "\n",
    "# Load training data\n",
    "# training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "#elastic net was killing all of our predictors... maybe that was good but I've turned it off for now.  Was at 0.8\n",
    "\n",
    "lr = LogisticRegression(maxIter=3,\n",
    "                        regParam=0.1,\n",
    "                        elasticNetParam=0.3,\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"binarized_tip\")\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_final)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# not doing multinominal... this was from the example\n",
    "# We can also use the multinomial family for binary classification\n",
    "# mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "# mlrModel = mlr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "# print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "# print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|            features|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|032f5a1fb7abe78a9...|12/01/2019 04:15:...|       564.0|       1.4|                  8.0|                   8.0| 5.0|0.0|              2.55|      7.55|                 false|         1.0|          0.0|           (78,[8],[1.0])|            (78,[8],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|[0.03655704464227...|[0.50913824347418...|       0.0|\n",
      "|0a63690199b5cfac9...|12/01/2019 10:00:...|      1121.0|      10.1|                  8.0|                  25.0|12.5|0.0|              2.55|     15.05|                 false|         1.0|          0.0|           (78,[8],[1.0])|           (78,[25],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|[0.01811167736618...|[0.50452779557007...|       0.0|\n",
      "|0e813934de44ca56c...|12/02/2019 09:30:...|      1012.0|       5.2|                 30.0|                  25.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|          (78,[30],[1.0])|           (78,[25],[1.0])|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|[0.02326508540025...|[0.50581600901930...|       0.0|\n",
      "|19fa4caa71dcd63a3...|12/01/2019 01:00:...|      2991.0|      19.0|                 78.0|                  76.0|30.0|0.0|               8.3|      38.3|                 false|         1.0|          0.0|               (78,[],[])|           (78,[76],[1.0])|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|[-0.2981426829560...|[0.42601158243192...|       1.0|\n",
      "|1bcdc2afc50c20307...|12/02/2019 09:45:...|      2266.0|      11.0|                 70.0|                  28.0|12.5|0.0|              2.55|     15.05|                  true|         3.0|          0.0|          (78,[70],[1.0])|           (78,[28],[1.0])|(162,[0,1,2,3,4,5...|(162,[0,1,2,3,4,5...|[0.41124151341111...|[0.60138553308007...|       0.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = lrModel.transform(test)\n",
    "prediction.show(5)\n",
    "\n",
    "df7 = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4796"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classificiation evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can't find how to do this in a Dataframe.  Rip to an RDD I guess.  This feels stupid.\n",
    "\n",
    "pred_rdd= df7.select('prediction').rdd.flatMap(lambda x: x)\n",
    "label_rdd = df7.select('binarized_tip').rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4796"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4796"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like we can zip them together\n",
    "\n",
    "predictionAndLabels =  pred_rdd.zip(label_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (1.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics2 = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "cm1 = metrics2.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2430., 1519.],\n",
       "       [ 420.,  427.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tashman's sugestion of upsampleing worked.  We no longer have a useless classifier\n",
    "\n",
    "#well that doesn't look aweseome.  Looks like we are only predicting no tip.  Might have to do some weighting magic.  Despite confusing myself, the number of results are correct since only 20% was used for the test, and 80% used in the training\n",
    "\n",
    "#maybe we should check the sample to see if there are any tips in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@nutanbhogendrasharma/role-of-onehotencoder-and-pipelines-in-pyspark-ml-feature-part-2-3275767e74f0\n",
    "# apparently the spark doc is wrong, we don't need fit, just transform.  Maybe thats a spark 3 thing?\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#onehotencoder to pickup\n",
    "onehotencoder_pickup_vector = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "df4 = onehotencoder_pickup_vector.transform(df4)\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "onehotencoder_dropoff_vector = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "df4 = onehotencoder_dropoff_vector.transform(df4)\n",
    "\n",
    "df4.show(20)c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we do RF in DF?\n",
    "# from https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier\n",
    "\n",
    "#lets start with the earlier datafram before any of the LR transforms\n",
    "df8 =  df4 \n",
    "\n",
    "# we need to regroup the features\n",
    "\n",
    "# RF doesn't like the community areas as they are too big.  Drop them for now\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled']\n",
    "\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "assembler = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "transformed = assembler.transform(df8)\n",
    "df8 = transformed\n",
    "\n",
    "df8.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"binarized_tip\", outputCol=\"indexedLabel\").fit(df8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(labelIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "\n",
    "# Set max to 78 as we have 77 CA plus out of city.  Doesn't work, max is 32.  This may move us to OHE.  Code can't handle CAs as it is.  Need to try something else. \n",
    "# short term to get code to run, drop pickup, droppoff areas.\n",
    "\n",
    "# https://stackoverflow.com/questions/44959122/how-to-handle-categorical-features-for-decision-tree-random-forest-in-spark-ml\n",
    "# https://spark.apache.org/docs/latest/ml-features.html#onehotencoder\n",
    "# https://docs.databricks.com/_static/notebooks/binary-classification.html\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=10).fit(df8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(featureIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (20% held out for testing)\n",
    "# it won't be the same splits at this point, we'll have to fix that later once we have everything working\n",
    "\n",
    "(trainingData, testData) = df8.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "# these are all defalt settings\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "# more pipeline steps\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"binarized_tip\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_rdd= predictions.select('prediction').rdd.flatMap(lambda x: x)\n",
    "rf_label_rdd = predictions.select('binarized_tip').rdd.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictionAndLabels =  rf_pred_rdd.zip(rf_label_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "cm2 = rf_metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stuff coppied for other Tahsman Notebooks that might be useful.  Not developed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each field, compute missing percentage\n",
    "# from preprcessing example notebook\n",
    "\n",
    "df.agg(*[\n",
    "    (1 - F.count(c) / F.count('*')).alias(c + '_miss')\n",
    "    for c in df.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might need to use this.  From the logistic regression example code\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "# one line model build.  We may need to do multiple types\n",
    "model = LogisticRegressionWithSGD.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this example code is backwards.  we'd likely need to use Predictions and Labels as in the documentation\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "print(labelsAndPreds.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load the data file. Note this data is in sparse format.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and test accuracy.\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy = 1.0 * labelsAndPreds.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-naive-bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my personal favorite... trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-Based Ensemble Methods**\n",
    "\n",
    "*Ensembles* combine multiple models together to produce a new model.  \n",
    "They may consist of models of the same type (e.g., all decision trees) or mixed type (e.g., decision tree + neural net + svm)  \n",
    "\n",
    "One of the fundamental results in machine learning is that multiple weak classifiers can be combined to produce a strong classifier.  \n",
    "\n",
    "Ensembles are useful in reducing overfitting, since predictions are based on several different trees  \n",
    "\n",
    "The two most popular tree-based ensemble methods are *Random Forests* and *Boosted Trees* (e.g. *Gradient-Boosted Trees*)  \n",
    "\n",
    "They are popular because they are often very competitive  \n",
    "\n",
    "The nice properties of decision trees carry over to ensembles of trees  \n",
    "\n",
    "This combining step can proceed using different methods, including:  \n",
    "\n",
    "- voting (for classification)\n",
    "- averaging (for regression) \n",
    "- running model predictions through another model (classification and regression)\n",
    "\n",
    "There are downsides to ensembles:  \n",
    "\n",
    "- Multiple models need to be trained, loaded, and maintained  \n",
    "- Model explanation is harder: no p-values like regression, several trees are feeding overall decision.  \n",
    "There are methods to provide feature importance information, such as partial dependence plots.\n",
    "\n",
    "**Random Forest**  \n",
    "Ensembles of decision trees  \n",
    "\n",
    "RFs inject two sources of randomness into modeling:  \n",
    "\n",
    "1. At each step, randomly select $p$ features out of $n$ total features for possible inclusion (random subspace method)\n",
    "2. Sample the original training set with replacement, up to the size of the original training set (bootstrapping of the training set)\n",
    "\n",
    "The number of features to randomly select $p$ is a parameter  \n",
    "The number of bootstrapped trees to grow $N$ is a parameter  \n",
    "\n",
    "Since the trees are grown independently, the training and prediction tasks are embarrassingly parallel and can be assigned to multiple workers.\n",
    "\n",
    "Classification prediction done by majority vote across trees\n",
    "\n",
    "**Random Forest Implementation**\n",
    "\n",
    "`from pyspark.mllib.tree import RandomForest`  \n",
    "\n",
    "Two most important parameters (which should be tuned using $k$-fold cross validation):  \n",
    "\n",
    "- `numTrees`: Number of trees in forest\n",
    "More trees will increase accuracy but also training time  \n",
    "\n",
    "- `maxDepth`: Maximum depth of each tree in forest\n",
    "Increasing depth can increase power of model, but will take longer to train and can overfit  \n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `subsamplingRate`: fraction of size of original training set (default=1.0 recommended)\n",
    "\n",
    "- `featureSubsetStrategy`: specified as fraction or function of total number of features\n",
    "\n",
    "**Random Forest Example: load data/train model/predict**  \n",
    "NOTE: Very similar to Decision Tree code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient-Boosted Trees**  \n",
    "\n",
    "GBTs work by building a sequence of trees and combining their predictions at each iteration.  The trees constructed are generally *stumps* which use a single decision split.  A stump is an example of a weak learner.\n",
    "\n",
    "This is different from random forests, where each tree independently gives predictions on each training instance.\n",
    "\n",
    "\n",
    "\n",
    "A loss is specified and an optimization problem is solved whereby the objective is to minimize the loss of the model by adding weak learners using a gradient-descent-like procedure.\n",
    "\n",
    "The procedure follows a stage-wise additive model, meaning that one new weak learner is\n",
    "added at a time and existing weak learners are left unchanged.\n",
    "For the original work, see:\n",
    "\n",
    "*Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of Statistics (2001): 1189–1232.*\n",
    "\n",
    "\n",
    "**Gradient-Boosted Trees Implementation**  \n",
    "\n",
    "Since the trees are built in a sequential fashion, the algorithm can not be run in parallel.  \n",
    "However, shallow trees (e.g., stumps) can be used effectively; this saves time versus random forests, which use deeper trees.\n",
    "\n",
    "The loss function in classification problems is the log loss, equal to twice the binomial negative log likelihood.\n",
    "\n",
    "Important parameters:\n",
    "- `numIterations`:  equal to the number of trees in the ensemble.  More trees means longer runtime but also better performance up to a point.\n",
    "- `learningRate`:  how quickly the model adapts on each iteration. A smaller value may help the algo have better performance, but at the cost of additional runtime. The documentation recommends NOT tuning this param.\n",
    "\n",
    "The method `runWithValidation` can help mitigate overfitting.  It takes a training RDD and a validation RDD.\n",
    "\n",
    "The training is stopped when the improvement in the validation error is not more than a certain tolerance (supplied by the `validationTol` argument in `BoostingStrategy`).\n",
    "\n",
    "**GBT Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.json('https://data.cityofchicago.org/api/odata/v4/m6dm-c72p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(domain, dataset_id):\n",
    "    # for this exercise, we're not using an app token,\n",
    "    # but you *should* sign-up and register for an app_token if you want to use the Socrata API\n",
    "    client = Socrata(domain, app_token=None)\n",
    "    offset = None\n",
    "    data = []\n",
    "    batch_size = 1000\n",
    "\n",
    "    while True:\n",
    "        records = client.get(dataset_id, offset=offset, limit=batch_size)\n",
    "        data.extend(records)\n",
    "        if len(records) < batch_size:\n",
    "            break\n",
    "        offset = offset + batch_size if (offset) else batch_size\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def download_permits_dataset():\n",
    "    return seattle_permits_df if \"seattle_permits_df\" in globals() else download_dataset(\"data.seattle.gov\", \"k44w-2dcq\")\n",
    "\n",
    "# load Seattle permits data\n",
    "seattle_permits_df = download_permits_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
