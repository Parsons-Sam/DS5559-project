{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of DS5559 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Left Twix Members\n",
    "\n",
    "* Alice Wright - aew7j\n",
    "* Edward Thompson - ejt8b\n",
    "* Michael Davies -  mld9s\n",
    "* Sam Parsons - sp8hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAT 6021 members of our cohort looked at Transportation Network Company data sets to see if there was a potential relationship between tipping and other indicators, specifically with “transportation network providers” i.e. rideshares such as Uber, Lyft, etc.  At that point in our Data Science journey we did not have the skills or equipment to investigate this question in depth.  \n",
    "\n",
    "Utilizing machine learning skills from SYS 6018 and applying Spark to this dataset we hope to come up with a more robust set of answers and potentially a better predictor of tipping. With other classification algorithms such as random forest and the heavy-weight data processing of Spark, will we be able to create a more robust predictive model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Questions from the TNC Data:\n",
    "\n",
    "* Can it be predicted what fares are most likely to tip the driver?\n",
    "* Is there a relationship between time of the fare and tipping? (workday stat, bar close, weekday, weekend, etc)\n",
    "* Is there a relationship between start or end location of the ride and tipping? (downtown pickup, north shore, airport, etc)\n",
    "* Is there a relationship between length or cost of ride and tipping? (do longer rides result in tips)\n",
    "* Using this data would we be able to make recommendations to drivers to maximize likelihood of receiving a tip?\n",
    "* Is the likelihood of tipping changing over time?  Are more rides being tipped?\n",
    "* Are there re-identification abilities in this dataset? For instance, can we find records for a person who reliably takes a rideshare to/from work every day thereby linking a home address to a work address?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, joining in additional datasets may yield answers to questions about external factors such as:\n",
    "* How did news reporting/social media on rideshare companies correlate with tipping?\n",
    "* What relationship(s) does trip demand have with the stocks of these companies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "The best data source for this appears to be from the City of Chicago, as it is large (169M records and 21 columns), relatively clean, anonymized, and accessible via API.\n",
    "\n",
    "City of Chicago:\n",
    "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p\n",
    "\n",
    "So far we have only pulled the data down via a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Rubric\n",
    "\n",
    "* Data Import and PreProcessing | 2 pts\n",
    "\n",
    "* Data splitting/sampling | 1 pt\n",
    "\n",
    "* EDA (min two graphs) | 2 pts\n",
    "\n",
    "* Model construction (min 3 models) | 3 pts\n",
    "\n",
    "* Model evaluation | 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .config(\"spark.executor.memory\", '21g') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.instances', '3') \\\n",
    "        .config(\"spark.driver.memory\",'1g') \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# import data manipulation methods\n",
    "from pyspark.ml.feature import Binarizer\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                            Type            Data/Info\n",
      "-------------------------------------------------------------\n",
      "ArrayType                           type            <class 'pyspark.sql.types.ArrayType'>\n",
      "Binarizer                           type            <class 'pyspark.ml.feature.Binarizer'>\n",
      "BinaryType                          type            <class 'pyspark.sql.types.BinaryType'>\n",
      "BooleanType                         type            <class 'pyspark.sql.types.BooleanType'>\n",
      "BucketedRandomProjectionLSH         type            <class 'pyspark.ml.featur<...>etedRandomProjectionLSH'>\n",
      "BucketedRandomProjectionLSHModel    type            <class 'pyspark.ml.featur<...>andomProjectionLSHModel'>\n",
      "Bucketizer                          type            <class 'pyspark.ml.feature.Bucketizer'>\n",
      "ByteType                            type            <class 'pyspark.sql.types.ByteType'>\n",
      "ChiSqSelector                       type            <class 'pyspark.ml.feature.ChiSqSelector'>\n",
      "ChiSqSelectorModel                  type            <class 'pyspark.ml.feature.ChiSqSelectorModel'>\n",
      "CountVectorizer                     type            <class 'pyspark.ml.feature.CountVectorizer'>\n",
      "CountVectorizerModel                type            <class 'pyspark.ml.feature.CountVectorizerModel'>\n",
      "DCT                                 type            <class 'pyspark.ml.feature.DCT'>\n",
      "DataType                            type            <class 'pyspark.sql.types.DataType'>\n",
      "DateType                            type            <class 'pyspark.sql.types.DateType'>\n",
      "DecimalType                         type            <class 'pyspark.sql.types.DecimalType'>\n",
      "DenseVector                         type            <class 'pyspark.ml.linalg.DenseVector'>\n",
      "DoubleType                          type            <class 'pyspark.sql.types.DoubleType'>\n",
      "ElementwiseProduct                  type            <class 'pyspark.ml.feature.ElementwiseProduct'>\n",
      "F                                   module          <module 'pyspark.sql.func<...>yspark/sql/functions.py'>\n",
      "FeatureHasher                       type            <class 'pyspark.ml.feature.FeatureHasher'>\n",
      "FloatType                           type            <class 'pyspark.sql.types.FloatType'>\n",
      "GBTClassifier                       type            <class 'pyspark.ml.classification.GBTClassifier'>\n",
      "HashingTF                           type            <class 'pyspark.ml.feature.HashingTF'>\n",
      "IDF                                 type            <class 'pyspark.ml.feature.IDF'>\n",
      "IDFModel                            type            <class 'pyspark.ml.feature.IDFModel'>\n",
      "Imputer                             type            <class 'pyspark.ml.feature.Imputer'>\n",
      "ImputerModel                        type            <class 'pyspark.ml.feature.ImputerModel'>\n",
      "IndexToString                       type            <class 'pyspark.ml.feature.IndexToString'>\n",
      "IntegerType                         type            <class 'pyspark.sql.types.IntegerType'>\n",
      "LogisticRegression                  type            <class 'pyspark.ml.classi<...>tion.LogisticRegression'>\n",
      "LongType                            type            <class 'pyspark.sql.types.LongType'>\n",
      "MapType                             type            <class 'pyspark.sql.types.MapType'>\n",
      "MaxAbsScaler                        type            <class 'pyspark.ml.feature.MaxAbsScaler'>\n",
      "MaxAbsScalerModel                   type            <class 'pyspark.ml.feature.MaxAbsScalerModel'>\n",
      "MinHashLSH                          type            <class 'pyspark.ml.feature.MinHashLSH'>\n",
      "MinHashLSHModel                     type            <class 'pyspark.ml.feature.MinHashLSHModel'>\n",
      "MinMaxScaler                        type            <class 'pyspark.ml.feature.MinMaxScaler'>\n",
      "MinMaxScalerModel                   type            <class 'pyspark.ml.feature.MinMaxScalerModel'>\n",
      "MulticlassClassificationEvaluator   type            <class 'pyspark.ml.evalua<...>ClassificationEvaluator'>\n",
      "MulticlassMetrics                   type            <class 'pyspark.mllib.eva<...>ation.MulticlassMetrics'>\n",
      "NGram                               type            <class 'pyspark.ml.feature.NGram'>\n",
      "Normalizer                          type            <class 'pyspark.ml.feature.Normalizer'>\n",
      "NullType                            type            <class 'pyspark.sql.types.NullType'>\n",
      "OneHotEncoder                       type            <class 'pyspark.ml.feature.OneHotEncoder'>\n",
      "OneHotEncoderEstimator              type            <class 'pyspark.ml.featur<...>.OneHotEncoderEstimator'>\n",
      "OneHotEncoderModel                  type            <class 'pyspark.ml.feature.OneHotEncoderModel'>\n",
      "PCA                                 type            <class 'pyspark.ml.feature.PCA'>\n",
      "PCAModel                            type            <class 'pyspark.ml.feature.PCAModel'>\n",
      "Pipeline                            type            <class 'pyspark.ml.pipeline.Pipeline'>\n",
      "PolynomialExpansion                 type            <class 'pyspark.ml.feature.PolynomialExpansion'>\n",
      "QuantileDiscretizer                 type            <class 'pyspark.ml.feature.QuantileDiscretizer'>\n",
      "RFormula                            type            <class 'pyspark.ml.feature.RFormula'>\n",
      "RFormulaModel                       type            <class 'pyspark.ml.feature.RFormulaModel'>\n",
      "RandomForestClassifier              type            <class 'pyspark.ml.classi<...>.RandomForestClassifier'>\n",
      "RegexTokenizer                      type            <class 'pyspark.ml.feature.RegexTokenizer'>\n",
      "SQLTransformer                      type            <class 'pyspark.ml.feature.SQLTransformer'>\n",
      "ShortType                           type            <class 'pyspark.sql.types.ShortType'>\n",
      "SparkSession                        type            <class 'pyspark.sql.session.SparkSession'>\n",
      "StandardScaler                      type            <class 'pyspark.ml.feature.StandardScaler'>\n",
      "StandardScalerModel                 type            <class 'pyspark.ml.feature.StandardScalerModel'>\n",
      "StopWordsRemover                    type            <class 'pyspark.ml.feature.StopWordsRemover'>\n",
      "StringIndexer                       type            <class 'pyspark.ml.feature.StringIndexer'>\n",
      "StringIndexerModel                  type            <class 'pyspark.ml.feature.StringIndexerModel'>\n",
      "StringType                          type            <class 'pyspark.sql.types.StringType'>\n",
      "StructField                         type            <class 'pyspark.sql.types.StructField'>\n",
      "StructType                          type            <class 'pyspark.sql.types.StructType'>\n",
      "TimestampType                       type            <class 'pyspark.sql.types.TimestampType'>\n",
      "Tokenizer                           type            <class 'pyspark.ml.feature.Tokenizer'>\n",
      "VectorAssembler                     type            <class 'pyspark.ml.feature.VectorAssembler'>\n",
      "VectorIndexer                       type            <class 'pyspark.ml.feature.VectorIndexer'>\n",
      "VectorIndexerModel                  type            <class 'pyspark.ml.feature.VectorIndexerModel'>\n",
      "VectorSizeHint                      type            <class 'pyspark.ml.feature.VectorSizeHint'>\n",
      "VectorSlicer                        type            <class 'pyspark.ml.feature.VectorSlicer'>\n",
      "Vectors                             type            <class 'pyspark.mllib.linalg.Vectors'>\n",
      "Word2Vec                            type            <class 'pyspark.ml.feature.Word2Vec'>\n",
      "Word2VecModel                       type            <class 'pyspark.ml.feature.Word2VecModel'>\n",
      "os                                  module          <module 'os' from '/opt/c<...>nda/lib/python3.7/os.py'>\n",
      "sc                                  SparkContext    <SparkContext master=loca<...>appName=mllib_classifier>\n",
      "spark                               SparkSession    <pyspark.sql.session.Spar<...>object at 0x7f030a7c5d10>\n",
      "typ                                 module          <module 'pyspark.sql.type<...>on/pyspark/sql/types.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear old df\n",
    "#del (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Schema.  \n",
    "This schema was been primarly determined by using a much smaller dataset and letting spark infer the schema.  We encountered an issue with spark reading in the ENTIRE dataset as NULL when there was a type mismatch.  Only the data we are likely to use later has been assigned to a specific type, otherwise it is left as a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|  Trip_End_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Census_Tract|Dropoff_Census_Tract|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|Pickup_Centroid_Latitude|Pickup_Centroid_Longitude|Pickup_Centroid_Location|Dropoff_Centroid_Latitude|Dropoff_Centroid_Longitude|Dropoff_Centroid_Location|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "|bea79abbef050980e...|12/01/2019 12:15:...|12/01/2019 01:00:...|      2675.0|      32.5|        17031980000|                null|                 76.0|                  null|70.0|0.0|              9.06|     79.06|                 false|         1.0|           41.9790708201|           -87.9030396611|    POINT (-87.903039...|                     null|                      null|                     null|\n",
      "|00f26da5601bbcf98...|12/01/2019 12:15:...|12/01/2019 12:15:...|       550.0|       2.8|        17031242100|         17031081700|                 24.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8996701799|           -87.6698377982|    POINT (-87.669837...|            41.8920421365|            -87.6318639497|     POINT (-87.631863...|\n",
      "|02256ef89c5c4be82...|12/01/2019 12:15:...|12/01/2019 12:30:...|       922.0|       2.8|        17031080300|         17031281900|                  8.0|                  28.0|10.0|0.0|              3.11|     13.11|                 false|         1.0|           41.9074919303|           -87.6357600901|    POINT (-87.635760...|            41.8792550844|             -87.642648998|     POINT (-87.642648...|\n",
      "|072cb06b1a88042c4...|12/01/2019 12:15:...|12/01/2019 12:30:...|      1475.0|      12.5|        17031310600|         17031063302|                 31.0|                   6.0|17.5|4.0|              2.55|     24.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|            41.9347624564|            -87.6398538587|     POINT (-87.639853...|\n",
      "|099257be99c66c8b2...|12/01/2019 12:15:...|12/01/2019 12:30:...|       594.0|       2.5|        17031310600|         17031330100|                 31.0|                  33.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|           41.8563332167|           -87.6595642391|    POINT (-87.659564...|             41.859349715|            -87.6173580061|     POINT (-87.617358...|\n",
      "+--------------------+--------------------+--------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------+-------------------------+--------------------------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a custom schema.  \n",
    "\n",
    "customSchema = StructType([\n",
    "    StructField('Trip_ID', StringType(), True),        \n",
    "    StructField('Trip_Start_Timestamp', StringType(), True),\n",
    "    StructField('Trip_End_Timestamp', StringType(), True),\n",
    "    StructField('Trip_Seconds', DoubleType(), True),\n",
    "    StructField('Trip_Miles', DoubleType(), True),\n",
    "    StructField('Pickup_Census_Tract', StringType(), True),\n",
    "    StructField('Dropoff_Census_Tract', StringType(), True),\n",
    "    StructField('Pickup_Community_Area', DoubleType(), True),\n",
    "    StructField('Dropoff_Community_Area', DoubleType(), True),\n",
    "    StructField(\"Fare\", DoubleType(), True),\n",
    "    StructField(\"Tip\", DoubleType(), True),\n",
    "    StructField(\"Additional_Charges\", DoubleType(), True),\n",
    "    StructField(\"Trip_Total\", StringType(), True),\n",
    "    StructField(\"Shared_Trip_Authorized\", BooleanType(), True),\n",
    "    StructField(\"Trips_Pooled\", DoubleType(), True),\n",
    "    StructField('Pickup_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Pickup_Centroid_Location', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Latitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Longitude', StringType(), True),\n",
    "    StructField('Dropoff_Centroid_Location', StringType(), True)\n",
    "])\n",
    "\n",
    "#old readin.  Infer is slow for large dataset\n",
    "#df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, inferSchema=True)\n",
    "\n",
    "#read in the data to a dataframe\n",
    "df = spark.read.csv('/../../project/ds5559/Alice_Ed_Michael_Sam_project/BigTrips.csv', header = True, schema=customSchema)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't update if you don't resave the variable\n",
    "\n",
    "df = df.drop('Trip_End_Timestamp', \n",
    "             'Pickup_Census_Tract',\n",
    "             'Dropoff_Census_Tract',\n",
    "             'Pickup_Centroid_Latitude',\n",
    "             'Pickup_Centroid_Longitude', \n",
    "             'Pickup_Centroid_Location', \n",
    "             'Dropoff_Centroid_Latitude', \n",
    "             'Dropoff_Centroid_Longitude', \n",
    "             'Dropoff_Centroid_Location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip_ID: string (nullable = true)\n",
      " |-- Trip_Start_Timestamp: string (nullable = true)\n",
      " |-- Trip_Seconds: double (nullable = true)\n",
      " |-- Trip_Miles: double (nullable = true)\n",
      " |-- Pickup_Community_Area: double (nullable = true)\n",
      " |-- Dropoff_Community_Area: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: double (nullable = true)\n",
      " |-- Additional_Charges: double (nullable = true)\n",
      " |-- Trip_Total: string (nullable = true)\n",
      " |-- Shared_Trip_Authorized: boolean (nullable = true)\n",
      " |-- Trips_Pooled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sample(False, .005, seed = 2021) #increased our sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244762"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the big df for now\n",
    "del (df)\n",
    "\n",
    "#hopefully that will make things faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill our NA community areas\n",
    "\n",
    "df2 = df2.na.fill(value=78,subset=['Pickup_Community_Area', 'Dropoff_Community_Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a binary tip/no tip indicator\n",
    "# https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=0, inputCol=\"Tip\", outputCol=\"binarized_tip\")\n",
    "df2 = binarizer.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip_ID: string, Trip_Start_Timestamp: string, Trip_Seconds: double, Trip_Miles: double, Pickup_Community_Area: double, Dropoff_Community_Area: double, Fare: double, Tip: double, Additional_Charges: double, Trip_Total: string, Shared_Trip_Authorized: boolean, Trips_Pooled: double, binarized_tip: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original No Tip Count:  163097\n",
      "Original Tip Count   :  32843\n",
      "\n",
      "Final No Tip Count   :  163097\n",
      "Final Tip Count      :  163589\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "\n",
    "# our model didn't work on the standard test train split.  Prof. Tashman recomended upscalling the help with the imbalanced dataset.\n",
    "#from https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split\n",
    "\n",
    "train_inital, test = df2.randomSplit([0.8, 0.2], seed=2021)\n",
    "\n",
    "# cahce our test values for later speed\n",
    "test.cache()\n",
    "\n",
    "# oversampleing code sample\n",
    "# https://stackoverflow.com/questions/53273133/how-to-perform-up-sampling-using-sample-functionpy-spark\n",
    "\n",
    "df_a = train_inital.filter(train_inital['binarized_tip'] == 0)\n",
    "df_b = train_inital.filter(train_inital['binarized_tip'] == 1)\n",
    "\n",
    "org_a_count = df_a.count()\n",
    "org_b_count = df_b.count() \n",
    "\n",
    "\n",
    "ratio = df_a.count() / df_b.count()\n",
    "# print(ratio)\n",
    "\n",
    "df_b_overampled = df_b.sample(withReplacement=True, fraction=ratio, seed=2021)\n",
    "\n",
    "# cahce our train values for later speed\n",
    "train = df_a.unionAll(df_b_overampled).cache()\n",
    "\n",
    "df_af = train.filter(train_inital['binarized_tip'] == 0)\n",
    "df_bf = train.filter(train_inital['binarized_tip'] == 1)\n",
    "fin_a_count = df_af.count()\n",
    "fin_b_count = df_bf.count() \n",
    "\n",
    "print(\"Original No Tip Count: \", org_a_count)\n",
    "print(\"Original Tip Count   : \", org_b_count)\n",
    "print(\"\")\n",
    "print(\"Final No Tip Count   : \", fin_a_count)\n",
    "print(\"Final Tip Count      : \", fin_b_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|003ca5c025410fcc9...|12/01/2019 11:15:...|       739.0|       4.7|                  6.0|                  32.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|00bcab98504c1e0c9...|12/01/2019 08:00:...|      1414.0|       6.7|                  6.0|                  32.0|15.0|0.0|              2.55|     17.55|                 false|         1.0|          0.0|\n",
      "|016a4ffe2b9fa410e...|12/01/2019 03:45:...|       702.0|       1.6|                 28.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|02c02b9be416e1e57...|12/02/2019 10:15:...|       923.0|       2.7|                 28.0|                  24.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|02fe4c484e6e16e96...|12/02/2019 01:30:...|       997.0|       4.4|                 78.0|                  76.0|10.0|3.0|              2.55|     15.55|                 false|         1.0|          1.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "|000e879212e519b17...|12/02/2019 01:45:...|      2376.0|      14.6|                 24.0|                  56.0|25.0|0.0|              7.55|     32.55|                 false|         1.0|          0.0|\n",
      "|00340cc3ffecd7a5d...|12/01/2019 10:45:...|      3252.0|      39.9|                 31.0|                  78.0|37.5|0.0|               0.0|      37.5|                  true|         1.0|          0.0|\n",
      "|0046fbc141378b368...|12/02/2019 09:30:...|       697.0|       5.5|                 28.0|                  25.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|\n",
      "|006131af958b8a181...|12/01/2019 05:00:...|       677.0|       4.0|                  6.0|                   7.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|\n",
      "|008c500db458d3746...|12/01/2019 02:15:...|       101.0|       0.3|                 58.0|                  58.0| 2.5|0.0|              2.55|      5.05|                 false|         1.0|          0.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training size has increased.  This is to be expected in upscaling.\n",
    "\n",
    "Good reference: https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40467\n",
      "8355\n"
     ]
    }
   ],
   "source": [
    "#just as a reminder what was the truth in our test data?\n",
    "\n",
    "dft_a = test.filter(train_inital['binarized_tip'] == 0)\n",
    "dft_b = test.filter(train_inital['binarized_tip'] == 1)\n",
    "count_test_a = dft_a.count()\n",
    "count_test_b = dft_b.count()\n",
    "print(count_test_a)\n",
    "print(count_test_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmacc(pred):\n",
    "    # make a confusion matrix and return the accuracy\n",
    "    # select predictions and labels from prediction transform as rdd as there isn't a DF function for this\n",
    "    pred_rdd= pred.select('prediction').rdd.flatMap(lambda x: x)\n",
    "    label_rdd = pred.select('binarized_tip').rdd.flatMap(lambda x: x)\n",
    "    \n",
    "    #zip them together\n",
    "    predictionAndLabels =  pred_rdd.zip(label_rdd)\n",
    "    \n",
    "    #metrics transform\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    #make our confusion matrix\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    #calc accuracy from confusion matrix\n",
    "    \n",
    "    acc = (cm[0][0] + cm[1][1])/(cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0])\n",
    "    \n",
    "    print(cm)\n",
    "    print()\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline steps for Logistic Regression:\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "#assemble the vector or LR\n",
    "\n",
    "# our colulms for lr\n",
    "predictor_col_for_lr = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "lr_va = VectorAssembler(inputCols=predictor_col_for_lr, outputCol=\"features\") \n",
    "\n",
    "#scale our LR\n",
    "\n",
    "lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "lr = LogisticRegression(maxIter=10,\n",
    "                        regParam=0.1,\n",
    "                        elasticNetParam=0.3,\n",
    "                        featuresCol=\"features\",\n",
    "                        labelCol=\"binarized_tip\")\n",
    "\n",
    "# Build the pipeline\n",
    "lr_pipeline = Pipeline(stages=[ohe_pu, ohe_do, lr_va, lr_scaler, lr])\n",
    "\n",
    "# Fit the pipeline\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "lr_prediction = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29341. 11126.]\n",
      " [ 4918.  3437.]]\n",
      "\n",
      "0.671377657613371\n"
     ]
    }
   ],
   "source": [
    "cmacc(lr_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+\n",
      "|prediction|binarized_tip|            features|\n",
      "+----------+-------------+--------------------+\n",
      "|       1.0|          0.0|(162,[0,1,2,3,5,1...|\n",
      "|       1.0|          0.0|(162,[0,1,2,3,5,1...|\n",
      "|       1.0|          0.0|(162,[0,1,2,3,5,3...|\n",
      "|       1.0|          0.0|(162,[0,1,2,3,5,3...|\n",
      "|       1.0|          1.0|(162,[0,1,2,3,5,1...|\n",
      "+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.662099\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_5ea99b4e5245) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "# pipeline steps for Random Forest:\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"binarized_tip\", outputCol=\"indexedLabel\")#.fit(df2)\n",
    "\n",
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# our colulms for rf\n",
    "predictor_col_for_rf = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "rf_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)#.fit(df2)\n",
    "\n",
    "#skip scaling for now\n",
    "\n",
    "# lr_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "#                         withStd=True, withMean=False)\n",
    "\n",
    "#what do we want to do if we are doing a parameter search? make the parameters as variables and just do a loop?\n",
    "#we learned that this week.  May also need to add in cv step\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "                            featuresCol=\"indexedFeatures\", \n",
    "                            numTrees=10)\n",
    "\n",
    "# Build the pipeline\n",
    "rf_pipeline = Pipeline(stages=[ohe_pu, ohe_do, labelIndexer, rf_va, featureIndexer, rf])\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "\n",
    "# Make a prediction\n",
    "rf_prediction = rf_model.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "rf_prediction.select(\"prediction\", \"binarized_tip\",\"features\").show(5) \n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"binarized_tip\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(rf_prediction)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = rf_model.stages[5]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11876. 28591.]\n",
      " [ 3734.  4621.]]\n",
      "\n",
      "0.3379009462947032\n"
     ]
    }
   ],
   "source": [
    "cmacc(rf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try a GBT \n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|             Trip_ID|Trip_Start_Timestamp|Trip_Seconds|Trip_Miles|Pickup_Community_Area|Dropoff_Community_Area|Fare|Tip|Additional_Charges|Trip_Total|Shared_Trip_Authorized|Trips_Pooled|binarized_tip|Pickup_Community_Area_vec|Dropoff_Community_Area_vec|indexedLabel|            features|     indexedFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|003ca5c025410fcc9...|12/01/2019 11:15:...|       739.0|       4.7|                  6.0|                  32.0|10.0|0.0|              2.55|     12.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|           (78,[32],[1.0])|         1.0|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|[0.05973027994818...|[0.52982967390313...|       0.0|\n",
      "|00bcab98504c1e0c9...|12/01/2019 08:00:...|      1414.0|       6.7|                  6.0|                  32.0|15.0|0.0|              2.55|     17.55|                 false|         1.0|          0.0|           (78,[6],[1.0])|           (78,[32],[1.0])|         1.0|(162,[0,1,2,3,5,1...|(162,[0,1,2,3,5,1...|[0.05973027994818...|[0.52982967390313...|       0.0|\n",
      "|016a4ffe2b9fa410e...|12/01/2019 03:45:...|       702.0|       1.6|                 28.0|                   8.0| 7.5|0.0|              2.55|     10.05|                 false|         1.0|          0.0|          (78,[28],[1.0])|            (78,[8],[1.0])|         1.0|(162,[0,1,2,3,5,3...|(162,[0,1,2,3,5,3...|[0.05973027994818...|[0.52982967390313...|       0.0|\n",
      "+--------------------+--------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+-------------+-------------------------+--------------------------+------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-------------+--------------------+\n",
      "|prediction|binarized_tip|            features|\n",
      "+----------+-------------+--------------------+\n",
      "|       0.0|          0.0|(162,[0,1,2,3,5,1...|\n",
      "|       0.0|          0.0|(162,[0,1,2,3,5,1...|\n",
      "|       0.0|          0.0|(162,[0,1,2,3,5,3...|\n",
      "|       0.0|          0.0|(162,[0,1,2,3,5,3...|\n",
      "|       0.0|          1.0|(162,[0,1,2,3,5,1...|\n",
      "+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.627463\n",
      "GBTClassificationModel (uid=GBTClassifier_1163a44f61c0) with 5 trees\n"
     ]
    }
   ],
   "source": [
    "#onehotencoder to pickup\n",
    "ohe_pu = OneHotEncoder(inputCol=\"Pickup_Community_Area\", outputCol=\"Pickup_Community_Area_vec\")\n",
    "\n",
    "#onehotencoder to dropoff\n",
    "ohe_do = OneHotEncoder(inputCol=\"Dropoff_Community_Area\", outputCol=\"Dropoff_Community_Area_vec\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"binarized_tip\", outputCol=\"indexedLabel\")#.fit(data)\n",
    "\n",
    "#assemble the vector or GBT\n",
    "\n",
    "# our colulms for gbt\n",
    "predictor_col_for_gbt = ['Trip_Seconds',\n",
    "                        'Trip_Miles',\n",
    "                        'Fare',\n",
    "                        'Additional_Charges',\n",
    "                        'Shared_Trip_Authorized',\n",
    "                        'Trips_Pooled',\n",
    "                        'Pickup_Community_Area_vec',\n",
    "                        'Dropoff_Community_Area_vec']\n",
    "\n",
    "gbt_va = VectorAssembler(inputCols=predictor_col_for_rf, outputCol=\"features\") \n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4)#.fit(data)\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=5)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "gbt_pipeline = Pipeline(stages=[ohe_pu, ohe_do, labelIndexer, gbt_va, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "gbt_model = gbt_pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "gbt_prediction = gbt_model.transform(test)\n",
    "\n",
    "gbt_prediction.show(3)\n",
    "\n",
    "# Select example rows to display.\n",
    "gbt_prediction.select(\"prediction\", \"binarized_tip\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"binarized_tip\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(gbt_prediction)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "gbtModel = gbt_model.stages[5]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14237. 26230.]\n",
      " [ 4404.  3951.]]\n",
      "\n",
      "0.37253697103764694\n"
     ]
    }
   ],
   "source": [
    "cmacc(gbt_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
